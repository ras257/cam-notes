\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Fundamentals of Probability}
\section{Probability Spaces}
\subsection{Definitions}
\begin{definition}[Subset Complement]
  For any subset $A \subseteq \Omega$, the \textit{complement} of $A$ is defined as $A^{\comp} = \Omega \setminus A$.
\end{definition}
\begin{definition}[$\sigma$-algebra]
  Suppose $\Omega$ is a set and $\salg$ is a collection of subsets of $\Omega$.

  We call $\salg$ a \textit{$\sigma$-algebra} if:
  \begin{enumerate}
    \item $\Omega \in \salg$.
    \item If $A \in \salg$, then $A^{\comp} \in \salg$.
    \item If $(A_n)_{n \in \N}$ is a countable collection of sets in $\salg$ (i.e. $A_n \in \salg\ \forall n$), then their union $\bigcup_{n \in \N} A_n \in \salg$.
  \end{enumerate}
\end{definition}
\begin{definition}[Probability Measure]
  Consider a $\sigma$-algebra $\salg$ on $\Omega$.
  A function $\P: \salg \to [0, 1]$ is called a \textit{probability measure} if:
  \begin{enumerate}
    \item $\P(\Omega) = 1$
    \item For any countable disjoint collection $(A_n)_{n \in \N}$ in $\salg$:
      \[
        \P\left(\bigcup_{n} A_n\right) = \sum_{n} \P(A_n)
      \]
      This is known as \textit{countable additivity}
  \end{enumerate}
  $\P(A)$ is then called the \textit{probability} of $A \in \salg$.
\end{definition}
\begin{definition}[Probability Space]
  For a set $\Omega$, $\sigma$-algebra $\salg$ and probability measure $\P$, we call the triplet $(\Omega, \salg, \P)$ a \textit{probability space}.
\end{definition}
\begin{definition}[Outcomes and Events]
  The elements of $\Omega$ are called \textit{outcomes} and the elements of $\salg$ are called \textit{events}.
\end{definition}
We talk about probabilities of events and \textbf{not} probabilities of outcomes.
\begin{remark}
  When $\Omega$ is countable, we take $\salg$ to be all subsets of $\Omega$, that is, $\salg = \powerset{\Omega}$.
\end{remark}
\subsection{Properties of Probability Measures}
We can derive some basic properties about probability measures directly from the definitions:
\begin{enumerate}
  \item $\P(A^{\comp}) = 1 - \P(A)$ as $\P(A \cup A^{\comp}) = \P(\Omega) = 1 = \P(A) + \P(A^{\comp})$.
  \item $\P(\emptyset) = 0$ as the probability of an empty union is an empty sum.
  \item If $A \subseteq B$, then $\P(B \setminus A) = \P(B) - \P(A)$ as $\P((B \setminus A) \cup A) = \P(B \setminus A) + \P(A) = \P(B)$.
  \item If $A \subseteq B$, then $\P(A) \leq \P(B)$ as $\P(B \setminus A) \geq 0$ above.
  \item $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$ as:
    \begin{align*}
      \P(A \cup B) &= \P((A \setminus (A \cap B)) \cup (B \setminus (A \cap B)) \cup (A \cap B)) \\
                   &= \P(A \setminus (A \cap B)) + \P(B \setminus (A \cap B)) + \P(A \cap B) \\
                   &= \P(A) + \P(B) - \P(A \cap B)
    \end{align*}
\end{enumerate}
\begin{proposition}[Countable Subadditivity]
  If $(A_n)$ is a countable collection of events in $\salg$, then:
  \[
    \P\left(\bigcup_{n \in \N} A_n\right) \leq \sum_{n \in \N} \P(A_n)
  \]
  \label{countableSub}
\end{proposition}
\begin{proof}
  Define $B_1 = A_1$ and for $n \geq 2$ define:
  \[
    B_n = A_n \setminus (A_1 \cup \cdots \cup A_{n - 1})
  \]
  Consider $B_i$ and $B_j$ for $i \neq j$.
  WLOG, assume that $i < j$.
  $B_i \subseteq A_i$ as $B_i = A_i \setminus (\cdots)$ and $B_j \cap A_i = \emptyset$ as $i < j$ so $B_j$ has had all elements of $A_i$ removed from it.
  Thus $B_i \cap B_j = \emptyset$ for $i \neq j$ so $(B_n)$ is a disjoint collection of sets in $\salg$.
  Considering the union, we have:
  \begin{align*}
    \bigcup_{k = 1}^{n} B_k &= A_1 \cup (A_2 \setminus A_1) \cup (A_3 \setminus (A_1 \cup A_2)) \cup \cdots \cup( A_n \setminus (A_1 \cup \cdots \cup A_{n - 1})) \\
                            &= (A_1 \cup A_2) \cup (A_3 \setminus (A_1 \cup A_2)) \cup \cdots \cup (A_n \setminus (A_1 \cup \cdots \cup A_{n - 1})) \\
                            &\;\;\vdots \\
                            &= A_1 \cup A_2 \cup \cdots \cup A_n = \bigcup_{k = 1}^{n} A_k
  \end{align*}
  Thus $\bigcup\limits_{n \in \N} B_n = \bigcup\limits_{n \in \N} A_n$:
  \[
    \P\left(\bigcup_{n \in \N} A_n\right) = \P\left(\bigcup_{n \in \N} B_n\right) = \sum_{n \in \N} \P(B_n) \leq \sum_{n \in \N} \P(A_n)
  \]
  using countable additivity and $\P(B_n) \leq \P(A_n)$ as $B_n \subseteq A_n\ \forall n$.
\end{proof}
\subsection{Continuity of Probability Measures}
\begin{definition}[Increasing and Decreasing Sequences in $\salg$]
  A sequence of events $(A_n)_{n \in \N}$ in $\mathcal{F}$ is called:
  \begin{itemize}
    \item An \textit{increasing sequence of events} if $A_n \subseteq A_{n + 1}\ \forall n$
    \item A \textit{decreasing sequence of events} if $A_{n + 1} \subseteq A_n\ \forall n$
  \end{itemize}
\end{definition}
For an increasing sequence of events, $\P(A_n)$ is an increasing function of $n$ as $\P(A_n) \leq \P(A_{n + 1})\ \forall n$.
Similarly, for a decreasing sequence of events, $\P(A_n)$ is decreasing function of $n$ as $\P(A_{n + 1}) \leq \P(A_n)\ \forall n$.

We have a specific definition for continuity when considering probability measures:
\begin{proposition}[Continuity of Probability Measures]
  If $(A_n)$ is an increasing sequence of events in $\salg$, then:
  \[
    \P(A_n) \to \P\left(\bigcup_{k = 1}^{\infty} A_k\right) \text{ as $n \to \infty$}
  \]
\end{proposition}
\begin{proof}
  Similarly to in the proof of \cref{countableSub}, define $B_1 = A_1$, and, for $n \geq 2$ define:
  \[
    B_n = A_n \setminus (A_1 \cup \cdots \cup \cdots A_{n - 1})
  \]
  Then $(B_n)$ is disjoint collection of events and $\bigcup_{k = 1}^{n} A_k = \bigcup_{k = 1}^{n} B_k$.
  Since $A_n \subseteq A_{n + 1}\ \forall n$, $\bigcup_{k = 1}^{n} A_k = A_n$ so:
  \[
    \P(A_n) = \P\left(\bigcup_{k = 1}^{n} B_k\right) = \sum_{k = 1}^{n} \P(B_k) \to \sum_{k = 1}^{\infty} \P(B_k) \text{ as $n \to \infty$}
  \]
  We can then rewrite this infinite sum using countable additivity:
  \[
    \sum_{k = 1}^{\infty} \P(B_k) = \P\left(\bigcup_{k = 1}^{\infty} B_k\right)
  \]
  and since $\bigcup_{k = 1}^{\infty} B_k = \bigcup_{k = 1}^{\infty} A_k$:
  \[
    \P(A_n) \to \P\left(\bigcup_{k = 1}^{\infty} A_k\right)
  \]
  which is the desired result.
\end{proof}
\begin{corollary}
  If $(A_n)$ is a decreasing sequence of events in $\salg$, then:
  \[
    \P(A_n) \to \P\left(\bigcap_{k = 1}^{\infty} A_k\right) \text{ as $n \to \infty$}
  \]
\end{corollary}
\begin{proof}
  $(A_n)$ is a decreasing sequence of events so $A_{n + 1} \subseteq A_{n}\ \forall n \iff  A^{\comp}_{n} \subseteq A^{\comp}_{n + 1}\  \forall n$.
  Thus $(A^{\comp}_n)$ is an increasing sequence of events so we can apply the previous result.
  \begin{align*}
    \P(A^{\comp}_n) &\to \P\left(\bigcup_{k = 1}^{\infty} A^{\comp}_k\right) \\
    \implies 1 - \P(A_n) &\to 1 - \P\left(\left[\bigcup_{k = 1}^{\infty} A^{\comp}_k\right]^{\comp}\right) \\
    \implies \P(A_n) &\to \P\left(\left[\bigcup_{k = 1}^{\infty} A^{\comp}_k\right]^{\comp}\right)
  \end{align*}
  We can rewrite the complement of the union as the intersection of the complements:
  \begin{align*}
    \left[\bigcup_{k = 1}^{\infty} A^{\comp}_k\right]^{\comp} &= \Omega \setminus (A^{\comp}_1 \cup A^{\comp}_2 \cup \cdots) \\
                                                   &= (\Omega \setminus A^{\comp}_1) \cap (\Omega \setminus A^{\comp}_2) \cap \cdots \\
                                                   &= A_1 \cap A_2 \cap \cdots \\
                                                   &= \bigcap_{k = 1}^{\infty} A_k
  \end{align*}
  which is the desired result.
\end{proof}
\subsection{Examples of Probability Spaces}
We will now look at some basic examples of probability spaces.
\begin{example}[6-Sided Die]
  The set of outcomes is $\Omega = \{1, 2, 3, 4, 5, 6\}$, and as it is countable, $\salg = \powerset{\Omega}$.
  We then have $\P(\{\omega\}) = 1/6$ for $\omega \in \Omega$ as each singleton event is equally likely.
  Furthermore, for any $A \in \salg$, we have $\P(A) = |A|/6$.
\end{example}
\begin{example}[Picking Elements]
  For a set $\Omega$ with $n$ elements, $\Omega = \{\omega_1, \ldots, \omega_n\}$, $\salg = \powerset{\Omega}$.
  We can model the experiment of picking a random element of $\Omega$ by setting $\P(A) = |A|/|\Omega|$.
  We then have:
  \[
    \P(\{\omega\}) = 1/|\Omega|\ \forall \omega \in \Omega
  \]
\end{example}
\begin{example}[Picking Balls]
  Suppose we have $n$ balls labelled $\{1, \ldots, n\}$ that are indistinguishable by touch.
  If we wish to model picking $k \leq n$ balls \textbf{at random} without replacement then:
  \[
    \Omega = \{A \subseteq \{1, \ldots, n\}: |A| = k\},\ |\Omega| = \binom{n}{k},\ \P(\{\omega\}) = \frac{1}{\binom{n}{k}}
  \]
\end{example}
\begin{example}[Deck of Cards]
  Take a standard deck of 52 cards that is \textbf{well-shuffled} (all permutations equally likely).
  The set of outcomes $\Omega$ is then:
  \[
    \Omega = \{\text{all permutations of 52 cards}\},\ |\Omega| = 52!
  \]
  We can then find the probability that the top two cards are aces:
  \[
    \P(\text{top 2 cards are aces}) = \frac{4 \times 3 \times 50!}{52!}
  \]
  As there are 4 ways to pick the top ace, 3 ways to pick the second ace, and $50!$ ways to pick the remaining cards.
\end{example}
\begin{example}[Largest Digit]
  Consider a string of $n$ random digits from $\{0, \ldots, 9\}$.
  The set of outcomes is then:
  \[
    \Omega = \{0, \ldots, 9\}^n,\ |\Omega| = 10^n
  \]
  We can then define the even $A_k$ to be:
  \[
    A_k = \{\text{no digit exceeds $k$}\}
  \]
  then
  \[
    \P(A_k) = \frac{(k + 1)^n}{10^n}
  \]
  as we pick from $k + 1$ possible digits $n$ times.

  Furthermore, if $B_k = \{\text{largest digit is $k$}\}$, we have:
  \[
    \P(B_k) = \P(A_k \setminus A_{k - 1}) = \frac{(k + 1)^n - k^n}{10^n}
  \]
  as $A_{k - 1} \subseteq A_k$.
\end{example}
\begin{example}[Birthday Problem]
  Suppose we have $n$ people, what is the probability that at least two of them share the same birthday?
  We first assume each birthday is equally likely to be one of $\{1, \ldots, 365\}$ (i.e. no-one was born on the 29th of February).

  The set of outcomes and $\sigma$-algebra is:
  \[
    \Omega = \{1, 2, \ldots, 365\}^n,\ \salg = \powerset{\Omega}
  \]
  Since each birthday is equally likely, the probability of a singleton event is:
  \[
    \P(\{\omega\}) = \frac{1}{365^{n}},\ \omega \in \Omega
  \]
  We then define the event $A = \{\text{at least two people have same birthday}\}$.
  It is often more convenient to work out $\P(A^{\comp})$ and then use this to find $\P(A)$.
  The compliment of $A$ is the event that no-one shares a birthday:
  \[
    A^{\comp} = \{\text{all $n$ birthdays are distinct}\}
  \]
  We can then calculate $\P(A^{\comp})$ as:
  \[
    \P(A^{\comp}) = \frac{365 \times 364 \times \cdots \times (365 - n + 1)}{365^n}
  \]
  so
  \[
    \P(A) = 1 - \P(A^{\comp}) = 1 - \frac{365 \times 364 \times \cdots \times (365 - n + 1)}{365^n}
  \]

  Surprisingly, for $n = 22$, $\P(A) \approx 0.476$, and for $n = 23$, $\P(A) \approx 0.507$.
  This means that we only need 23 people for the probability of two people sharing a birthday to be greater than $1/2$.
\end{example}
\subsection{Principle of Inclusion Exclusion}
We saw that for two sets $A, B$, $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$.
We can apply this result three times to extend it to three sets $A, B, C$:
\begin{align*}
  \P(A \cup B \cup C) &= \P(A \cup B) + \P(C) - \P((A \cup B) \cap C) \\
                      &= \P(A) + \P(B) + \P(C) - \P(A \cap B) - \P((A \cap C) \cup (B \cap C)) \\
                      &= \P(A) + \P(B) + \P(C) - \P(A \cap B) - \P(A \cap C) - \P (B \cap C) \\
                      &\quad + \P((A \cap C) \cap (B \cap C)) \\
                      &= \P(A) + \P(B) + \P(C) - \P(A \cap B) - \P(A \cap C) - \P(B \cap C) + \P(A \cap B \cap C)
\end{align*}
We can also use induction to extend this to the \textit{principle of inclusion exclusion}.
\begin{proposition}[Inclusion-Exclusion Formula]
  For $A_1, \ldots, A_n \in \salg$:
  \[
    \P\left(\bigcup_{i = 1}^{n} A_i\right) = \sum_{k = 1}^{n} (-1)^{k + 1} \sum_{1 \leq i_1 < \ldots < i_k \leq n} \P(A_{i_1} \cap \cdots \cap A_{i_k})
  \]
  \label{inclusionExclusion}
\end{proposition}
\begin{proof}
  \induction
  {$n = 2$}{
    $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$ as shown previously.
  }
  {$n - 1$ events}{}
  {$n$ events}{
    \begin{align*}
      \P\left(\bigcup_{i = 1}^{n}  A_i\right) &= \P\left(\left[\bigcup_{i = 1}^{n - 1} A_i\right] \cup A_n\right) \\
                                   &= \P\left(\bigcup_{i = 1}^{n - 1} A_i\right) + \P(A_n) - \P\left(\bigcup_{i = 1}^{n - 1} \underbrace{(A_i \cap A_n)}_{B_i}\right) \tag{$\dag$} \label{eqnDag}
    \end{align*}
    Let $B_i = A_i \cap A_n$.
    Applying the induction hypothesis:
    \[
      \P\left(\bigcup_{i = 1}^{n - 1} A_i\right) = \sum_{k = 1}^{n - 1} (-1)^{k + 1} \sum_{1 \leq i_1 < \ldots < i_k \leq n - 1} \P(A_{i_1} \cap \cdots \cap A_{i_k}) \tag{$\star$} \label{eqnStar}
    \]
    \begin{align*}
      \P\left(\bigcup_{i = 1}^{n - 1} B_i\right) &= \sum_{k = 1}^{n - 1} (-1)^{k + 1} \sum_{1 \leq i_1 < \cdots <i_k \leq n - 1} \P(B_{i_1}\cap \cdots \cap B_{i_k}) \\
                                                 &= \sum_{k = 1}^{n - 1} (-1)^{k + 1} \sum_{1 \leq i_1 < \cdots <i_k \leq n - 1} \P(A_{i_1}\cap \cdots \cap A_{i_k} \cap A_n) \tag{$\ast$} \label{eqnAst}
    \end{align*}
    Combining the above into \cref{eqnDag}, the $\P(A_n)$ accounts for the additional single intersection when $k = 1$ in \cref{eqnStar} and \cref{eqnAst} accounts for all of the intersections that include the new event $A_n$.
    The additional minus sign accounts for the fact that the terms of \cref{eqnAst} have an additional intersection so their signs need to be flipped to achieve the final result.
  }
\end{proof}
If we have the probability space $(\Omega, \salg, \P)$ for finite $\Omega$ and define $\P(A) = |A|/|\Omega|$, then we can derive the set-theoretic version of inclusion exclusion that we saw in IA Numbers and Sets:
\[
  |A_1 \cup \cdots \cup A_n| = \sum_{k = 1}^{n} (-1)^{k + 1} \sum_{1 \leq i_1 < \cdots < i_k \leq n} |A_{i_1} \cap \cdots \cap A_{i_k}|
\]
\subsubsection{Boferroni Inequalities}
Using the principle of inclusion exclusion in the cases of two and three sets, we see that:
\begin{align*}
  \P(A \cup B) &\leq \P(A) + \P(B) \\
  \P(A \cup B \cup C) &\geq \P(A) + \P(B) + \P(C) - \P(A \cap B) - \P(A \cap C) - \P(B \cap C) \\
\end{align*}
The \textit{Bofferoni Inequalities} tell us if the truncation of the inclusion exclusion formula will be an overestimate or an underestimate based on if it finishes on an even or odd term.
\begin{proposition}[Bofferoni Inequalities]
  \[
    \P\left(\bigcup_{i = 1}^{n} A_i\right) \begin{cases}
    \leq \sum\limits_{k = 1}^{r} (-1)^{k + 1} \sum\limits_{1 \leq i_1 < \cdots < i_k \leq n} \P(A_i \cap \cdots \cap A_{i_k}) & \text{ if $r$ is odd} \\

    \geq \sum\limits_{k = 1}^{r} (-1)^{k + 1} \sum\limits_{1 \leq i_1 < \cdots < i_k \leq n} \P(A_i \cap \cdots \cap A_{i_k}) & \text{ if $r$ is even} \\
    \end{cases}
  \]
\end{proposition}
\begin{proof}
  \induction
  {$n = 2$}{
    For $r = 1$, $\P(A) + \P(B) \geq \P(A) + \P(B) - \P(A \cap B) = \P(A \cup B)$.\par
    For $r = 2$, $\P(A) + \P(B) - \P(A \cap B) \leq \P(A \cup B)$, which are the correct inequalities.
  }
  {$n - 1$ events}{}
  {$n$ events}{
    Assume $r$ is odd.
    \begin{align*}
      \P\left(\bigcup_{i = 1}^{n} A_i\right) &= \P\left(\left[\bigcup_{i = 1}^{n - 1} A_i\right] \cup A_n\right) \\
                                             &= \P\left(\bigcup_{i = 1}^{n - 1} A_i\right) + \P(A_n) - \P\left(\bigcup_{i = 1}^{n - 1} B_i\right) \tag{$\star$} \label{eqnStar2}
    \end{align*}
    where $B_i = A_i \cap A_n$.
    By the induction hypothesis, since $r$ is odd:
    \[
      \P\left(\bigcup_{i = 1}^{n - 1} A_i\right) \leq \sum_{k = 1}^{r} (-1)^{k + 1} \sum_{1 \leq i_1 < \cdots < i_k \leq n - 1} \P(A_i \cap \cdots \cap A_{i_k})
    \]
    Since $r$ is odd $r - 1$ is even, using the induction hypothesis again:
    \begin{align*}
      \P\left(\bigcup_{i = 1}^{n - 1} B_i\right) &\geq \sum_{k = 1}^{r - 1} (-1)^{k + 1} \sum_{1 \leq i_1 < \cdots < i_k \leq n - 1} \P(B_{i_n} \cap \cdots \cap B_{i_k}) \\
                                                 &= \sum_{k = 1}^{r - 1} (-1)^{k + 1} \sum_{1 \leq i_1 < \cdots < i_k \leq n - 1} \P(A_{i_n} \cap \cdots \cap A_{i_k} \cap A_n)
    \end{align*}
    Substituting back into \cref{eqnStar2} yields the desired result.
    The proof for even $r$ is identical apart from the two inequalities above are flipped so the final inequality will also be flipped, giving the result for even $r$.
  }
\end{proof}
\section{Combinatorial Analysis}
\subsection{Multinomial Coefficients}
\label{multinomialCoefficients}
Suppose $\Omega$ is a finite set with $|\Omega| = n$.
We want to partition $\Omega$ into $k$ disjoint subsets $\Omega_1, \ldots, \Omega_k$ with $|\Omega_i| = n_i$ and $n_1 + \cdots + n_k = n$.
How many ways are there to do this?
Let $M$ be the number of ways:
\begin{align*}
  M &= \binom{n}{n_1} \times \binom{n - n_1}{n_2} \times \cdots \times \binom{n - (n_1 + \cdots n_{k - 1})}{n_k} \\
    &= \frac{n!}{n_1! n_2! \cdots n_k!}
\end{align*}
As we pick $n_1$ from $n$, $n_2$ from the remaining $n - n_1$, and so on.
\begin{definition}[Multinomial Coefficient]
  For $n_1, \ldots, n_k$ such that $n_1 + \cdots + n_k = n$, we define the multinomial coefficient to be:
  \[
    \binom{n}{n_1, n_2, \ldots, n_k} = \frac{n!}{n_1! n_2! \cdots n_k!}
  \]
\end{definition}
\subsection{Increasing Functions}
\begin{definition}[Increasing and Strictly Increasing Functions]
  A function $f$ is:
  \begin{itemize}
    \item \textit{Increasing} if $x < y \implies f(x) \leq f(y)$
    \item \textit{Strictly increasing} if $x < y \implies f(x) < f(y)$
  \end{itemize}
\end{definition}
\begin{proposition}
  For $k \leq n$, the number of \textbf{strictly increasing} $f: \{1, \ldots, k\} \to \{1, \ldots, n\}$ is $\binom{n}{k}$
  \label{strictlyIncreasingF}
\end{proposition}
\begin{proof}
  If we pick a subset of size $k$ from $\{1, \ldots, n\}$ to be $\im f$, then this uniquely determines $f$ as it must be order preserving.
  Therefore, the number of strictly increasing $f$ is the number of such subsets which is $\binom{n}{k}$.
\end{proof}
\begin{proposition}
  For $k \leq n$, the number of \textbf{increasing} $f: \{1, \ldots, k\} \to \{1, \ldots, n\}$ is $\binom{n + k - 1}{k}$.
\end{proposition}
\begin{proof}
  We can define a bijection between:
  \begin{align*}
    &\{f: \{1, \ldots, k\} \to \{1, \ldots, n\} \text{ increasing}\} \\
    \text{ and }&\{g: \{1, \ldots, k\} \to \{1, \ldots, n + k - 1\} \text{ strictly increasing}\}
  \end{align*}
  We do this by mapping each increasing $f: \{1, \ldots, k\} \to \{1, \ldots, n\}$ to $g(i) = f(i) + i - 1$.
  Such $g$ are strictly increasing as, if $i < j$, then $f(i) - f(j) \leq 0$ and $i - j < 0$ so:
  \[
    g(i) - g(j) = (f(i) - f(j)) + (i - j) < 0 \implies g(i) < g(j)
  \]
  This construction ensures that $g(1) = f(1)$ and then always adds 1 each time the input increases to ensure that $g$ never stays at a constant value.

  This mapping is a bijection as:
  \[
    g_1 = g_2 \implies f_1(i) + i - 1 = f_2(i) + i - 1\ \forall i \implies f_1(i) = f_2(i)\ \forall i \implies f_1 = f_2
  \]
  Thus, the number of increasing $f$ is the same as the number of strictly increasing $g: \{1, \ldots, k\} \to \{1, \ldots, n + k - 1\}$, which is $\binom{n + k - 1}{k}$ by \cref{strictlyIncreasingF}.
\end{proof}
\subsection{Surjective Functions}
\begin{proposition}
  For $n \geq m$, the number of surjective $f: \{1, \ldots, n\} \to \{1, \ldots, m\}$ is:
  \[
    |A| = \sum_{k = 0}^{m} (-1)^{k} \binom{m}{k} (m - k)^{n}
  \]
\end{proposition}
\begin{proof}
  Consider $\Omega = \{f: \{1, \ldots, n\} \to \{1, \ldots, m\}\}$ and $A = \{f \in \Omega: f \text{ is a surjection}\}$.
  Define:
  \[
    A_i = \{f \in \Omega: i \notin \{f(1), \ldots, f(n)\}\} \text{ for } i \in \{1, \ldots, m\}
  \]
  We can then derive the following chain of equivalence:
  \begin{align*}
    f \in A &\iff f \text{ is surjective} \\
            &\iff \forall i \in \{1, \ldots, m\}, \exists j \in \{1, \ldots, n\} \text{ s.t. } f(j) = i \\
            &\iff \centernot\exists i \in \{1, \ldots, m\} \text{ s.t. } \forall j \in \{1, \ldots, n\},\ f(j) \neq i \\
            &\iff f \notin A_i\ \forall i \in \{1, \ldots, m\} \\
            &\iff f \in A^{\comp}_i\ \forall i \in \{1, \ldots, m\} \\
            &\iff f \in A^{\comp}_1 \cap \cdots \cap A^{\comp}_m
  \end{align*}
  Thus $A = A^{\comp}_1 \cap \cdots \cap A^{\comp}_m = (A_1 \cup \cdots \cup A_m)^{\comp}$ so $|A| = |\Omega| - |A_1 \cup \cdots \cup A_m|$.
  $|\Omega| = m^{n}$ as this is the total number of functions $f: \{1, \ldots, n\} \to \{1, \ldots, m\}$.
  Using inclusion exclusion (\cref{inclusionExclusion}), we have:
  \[
    |A_1 \cup \cdots \cup A_m| = \sum_{k = 1}^{m} (-1)^{k + 1} \sum_{1 \leq i_1 < \cdots < i_k \leq n} |A_{i_1} \cap \cdots \cap A_{i_k}|
  \]
  $A_{i_1} \cap \cdots \cap A_{i_k}$ are all the functions in $\Omega$ that do not contain $i_1, \ldots, i_k$ in their image.
  There are $(m - k)^{n}$ such functions as we are essentially counting the number of $g: \{1, \ldots, n\} \to \{1, \ldots, m - k\}$.
  As there are $\binom{m}{k}$ ways to choose a subset $\{i_1, \ldots, i_k\}$ from $\{1, \ldots, m\}$:
  \[
    |A_1 \cup \cdots \cup A_m| = \sum_{k = 1}^{m} (-1)^{k + 1} \binom{m}{k}(m - k)^{n}
  \]
  Therefore:
  \begin{align*}
    |A| &= m^{n} - \sum_{k = 1}^{m} (-1)^{k + 1} \binom{m}{k}(m - k)^{n} \\
        &= \sum_{k = 0}^{m} (-1)^{k} \binom{m}{k} (m - k)^{n}
  \end{align*}
\end{proof}
\subsection{Derangements}
\begin{definition}[Derangement]
  A \textit{derangement} is a permutation with no fixed points.
\end{definition}
\begin{proposition}
  Given a random permutation of $\{1, \ldots, n\}$, the probability that it is a derangement is:
  \[
    \sum_{k = 0}^{n} \frac{(-1)^{k}}{k!}
  \]
\end{proposition}
\begin{proof}
  Consider $\Omega = \{\text{permutations of } \{1, \ldots, n\}\}$ and $\P(\{\omega\}) = \frac{1}{n!}$.
  Define the events:
  \begin{align*}
    A &= \{\text{derangements}\} = \{f \in \Omega: f(i) \neq i\ \forall i \in \{1, \ldots, n\}\} \\
    A_i &= \{f \in \Omega: f(i) = i\} \text{ for $i \in \{1, \ldots, n\}$}
  \end{align*}
  We can rewrite $A$ as:
  \[
    A = A^{\comp}_1 \cap \cdots \cap A^{\comp}_n = \left(\bigcup_{i = 1}^{n} A_i\right)^{\comp}
  \]
  Since it is exactly the set of $f$ that are in none of the $A_i$.
  So $\P(A) = 1 - \P\left(\bigcup_{i = 1}^{n} A_i\right)$.
  By inclusion-exclusion (\cref{inclusionExclusion}):
  \[
    \P\left(\bigcup_{i = 1}^{n} A_i\right) = \sum_{k = 1}^{n} (-1)^{k + 1} \sum_{1 \leq i_1 < \cdots < i_k \leq n} \P(A_{i_1} \cap \cdots \cap A_{i_k})
  \]
  $\P(A_{i_1} \cap \cdots \cap A_{i_k})$ is the probability that a random permutation fixes $i_1, \ldots, i_k$.
  If a permutation fixes $i_1, \ldots, i_k$, there are $(n - k)!$ ways to arrange the remaining elements so:
  \[
    \P(A_{i_1} \cap \cdots \cap A_{i_k}) = \frac{(n - k)!}{n!}
  \]

  Since there are $\binom{n}{k}$ ways to choose a subset $\{i_1, \ldots, i_k\}$ from $\{1, \ldots, n\}$:
  \begin{align*}
    \P\left(\bigcup_{i = 1}^{n} A_i\right) &= \sum_{k = 1}^{n} (-1)^{k + 1} \binom{n}{k} \frac{(n - k)!}{n!} \\
                                           &= \sum_{k = 1}^{n} (-1)^{k + 1} \frac{1}{k!}
  \end{align*}
  So:
  \[
    \P(A) = 1 - \sum_{k = 1}^{n} (-1)^{k + 1}\frac{1}{k!} = \sum_{k = 0}^{n} \frac{(-1)^{k}}{k!}
  \]
  which is the desired result.
\end{proof}
\begin{remark}
  As $n \to \infty$, $\P(A) \to \sum_{k = 0}^{\infty} \frac{(-1)^{k}}{k!} = e^{-1} \approx 0.368$.
  So for large $n$, the probability of a random permutation being a derangement is approximately 37\%.
\end{remark}
\subsection{Stirling's Formula}
\begin{remark}[Asymptotic Notation]
  If $(a_n)$ and $(b_n)$ are sequences then we write $a_n \sim b_n$ as $n \to \infty$ if $\frac{a_n}{b_n} \to 1$ as $n \to \infty$.
\end{remark}
Stirling's formula provides an asymptotic expression for $n!$ as $n \to \infty$.
\begin{theorem}[Stirling's Formula]
  \[
    n! \sim \sqrt{2\pi n} \left(\frac{n}{e}\right)^n \text{ as $n \to \infty$}
  \]
  \label{stirling}
\end{theorem}
We will first show the weaker statement $\log n! \sim n \log n$.
\begin{proof}[Weaker Statment]
  Define $\ell_n = \log n! = \log 2 + \cdots + \log n$ and for $x \in \R$, we write $\floor{x}$ to mean the integer part of $x$.
  \[
    \log \floor{x} \leq \log x \leq \log \floor{x + 1}
  \]
  If we integrate this from $1$ to $n$ we get:
  \begin{align*}
    \sum_{k = 1}^{n - 1} \log k &\leq \int_{1}^{n} \log x \d{x} \leq \sum_{k = 1}^{n} \log k \\
    \ell_{n - 1} &\leq n \log n - n + 1 \leq \ell_{n}
\end{align*}
So we can bound $\ell_n$ by:
\[
  n \log n - n + 1 \leq \ell_n \leq (n + 1) \log(n + 1) - (n + 1) + 1
\]
we can then divide through by $n \log n$ to get:
\[
  1 - \cancelto{0}{\frac{n - 1}{n \log n}} \leq \frac{\ell_n}{n \log n} \leq \cancelto{1}{\frac{\log (n + 1)}{\log n}} + \cancelto{0}{\frac{\log(n + 1)}{n \log n}} - \cancelto{0}{\frac{1}{\log n}}
\]
as $n \to \infty$, so by squeeze theorem:
\[
  \frac{\ell_n}{n \log n} \to 1 \text{ as $n \to \infty$}
\]
and thus $\ell_n = \log n! \sim n \log n$.
\end{proof}
\nonexaminable
To prove the stronger Stirling's formula, we first need to prove an integral identity.
\begin{lemma}
  For all twice differentiable $f : \R \to \R$ and $a < b$ we have:
  \label{integralIdentity}
  \[
    \int_{a}^{b} f(x)\d{x} = \frac{f(a) + f(b)}{2}(b - a) - \frac{1}{2}\int_{a}^{b} (x - a)(b - x)f''(x) \d{x}
  \]
\end{lemma}
\begin{proof}
  Integrating the final integral by parts twice yields:
  \begin{align*}
    \int_{a}^{b} (x - a)(b - x) f''(x) \d{x} &= \eval{(-x^2 + ax + bx - ab)f'(x)}{a}{b} + \int_{a}^{b} (2x - a - b)f'(x) \d{x} \\
                                             &= \int_{a}^{b} (2x - a - b)f'(x) \d{x} \\
                                             &= \eval{(2x - a - b)f(x)}{a}{b} - 2 \int_{a}^{b} f(x) \d{x} \\
                                             &= (b - a)(f(b) + f(a)) - 2 \int_{a}^{b} f(x) \d{x}
  \end{align*}
  from which the identity follows.
\end{proof}
We can now prove the stronger asymptotic expression:
\begin{proof}[Stirling's Formula -- \Cref{stirling}]\par
  \textbf{Showing $n! \sim n^n e^{-n}\sqrt{n} \cdot A$}\par
  Taking $f(x) = \log x$, $a = k$, $b = k + 1$ for $k \in \N$ in \cref{integralIdentity} yields:
  \[
    \int_{k}^{k + 1} \log x \d{x} = \frac{\log k + \log(k + 1)}{2} + \frac{1}{2}\int_{k}^{k + 1} \frac{(x - k)(k + 1 - x)}{x^2} \d{x}
  \]
  Summing both sides from $k = 1$ to $n - 1$, we have:
  \[
    \int_{1}^{n} \log x \d{x} = \frac{\log (n - 1)! + \log n!}{2} + \sum_{k = 1}^{n - 1} a_k
  \]
  where
  \[
    a_k = \frac{1}{2} \int_{k}^{k + 1} \frac{(x - k)(k + 1 - x)}{x^2} \d{x} \stackrel{x \mapsto x + k}= \frac{1}{2} \int_{0}^{1} \frac{x(1 - x)}{(x + k)^2} \d{x}
  \]
  We can also rewrite:
  \[
    \frac{\log (n - 1)! + \log n!}{2} = \frac{\log n! - \log n + \log n!}{2} = \log n! - \frac{\log n}{2}
  \]
  and so, after rearranging:
  \begin{align*}
    \log n! &= n \log n - n + \frac{\log n}{2} + 1 - \sum_{k = 1}^{n - 1} a_k \\
    \implies n! &= n^n e^{-n} \sqrt{n} \exp\left[1 - \sum_{k = 1}^{n - 1} a_k\right]
  \end{align*}
  We now wish to bound $a_k$.
  Since $x + k \geq k$ for $x \geq 0$ we have:
  \begin{align*}
    a_k = \frac{1}{2} \int_{0}^{1} \frac{x(1 - x)}{(x + k)^2} \d{x} &\leq \frac{1}{2k^2} \int_{0}^{1} x(1 - x) \d{x} \\
                                                                    &= \frac{1}{12k^2}
  \end{align*}
  This means that $\sum_{k = 1}^{\infty} a_k$ converges by comparison to $\sum_{k = 1}^{\infty} \frac{1}{k^2}$.
  Let
  \[
    A = \exp\left[1 - \sum_{k = 1}^{\infty} a_k\right]
  \]
  So $n! = n^n e^{-n} \sqrt{n} \cdot A \exp\left[\sum_{k = n}^{\infty} a_k\right]$.
  As $n \to \infty$, $\sum_{k = n}^{\infty} a_k \to 0$ so $\exp\left[\sum_{k = n}^{\infty} a_k\right] \to 1$.
  Thus:
  \[
    \frac{n!}{n^{n}e^{-n}\sqrt{n}} \to A \text{ as $n \to \infty$}
  \]
  We now know that $n! \sim n^n e^{-n}\sqrt{n} \cdot A$ for some constant $A$.

  \textbf{Showing $A = \sqrt{2 \pi}$}\par
  To show that $A = \sqrt{2 \pi}$, we will find an asymptotic expression for $2^{-2n} \binom{2n}{n}$ in two different ways.

  Firstly, using the asymptotic expression we just derived, we have:
  \[
    2^{(-2n)}\binom{2n}{n} = 2^{-2n} \frac{(2n)!}{(n!)^2} \sim \frac{2^{-2n} (2n)^{2n} e^{-2n} \sqrt{2n} \cdot A}{(n^{n} e^{-n}\sqrt{n} \cdot A)(n^{n} e^{-n}\sqrt{n} \cdot A)} = \frac{\sqrt{2}}{A\sqrt{n}}
  \]

  Secondly, using a different method, we will show that $2^{-2n}\binom{2n}{n} \sim \frac{1}{\sqrt{\pi n}}$ as $n \to \infty$, which will then force $A = \sqrt{2\pi}$.

  \textbf{Showing $2^{-2n} \binom{2n}{n} \sim \frac{1}{\sqrt{\pi n}}$}\par
  Consider the integrals:
  \[
    I_n = \int_{0}^{\pi/2} (\cos \theta)^{n} \d{\theta},\ I_0 = \frac{\pi}{2},\  I_1 = 1
  \]
  Integrating by parts:
  \begin{align*}
    I_n &= \eval{\cos^{n - 1} \theta \sin \theta}{0}{\pi/2} + (n - 1)\int_{0}^{\pi/2} \cos^{n - 2} \theta \sin^2 \theta \d{\theta} \\
        &= (n - 1) \int_{0}^{\pi/2} \cos^{n - 2} \theta (1 - \cos^2 \theta) \d{\theta} \\
        &= (n - 1)(I_{n - 2} - I_n)
  \end{align*}
  Rearranging, we obtain the recurrence $I_n = \frac{n - 1}{n} I_{n - 2}$.
  Thus:
  \begin{align*}
    I_{2n} = \frac{2n - 1}{2n} I_{2n - 2} &= \frac{(2n - 1) \cdot (2n - 3) \cdots 3 \cdot 1}{2n \cdot (2n - 2) \cdots 2} I_0 \\
                                          &= \frac{2n \cdot (2n - 1) \cdot (2n - 2) \cdots 3 \cdot 2\cdot 1}{2n \cdot 2n \cdot (2n - 2) \cdot (2n - 2) \cdots 2 \cdot 2} I_0 \\
                                          &= \frac{(2n)!}{2^{2n} (n!)^2} I_0  \\
                                          &= 2^{-2n} \binom{2n}{n} \frac{\pi}{2}
  \end{align*}
  Similarly:
  \[
    I_{2n + 1} = \frac{(2n) \cdots 4 \cdot 2}{(2n + 1) \cdots 3\cdot 1} I_1 = \frac{1}{2n + 1} \left(2^{-2n} \binom{2n}{n}\right)^{-1}
  \]
  If we know that $\frac{I_{2n}}{I_{2n + 1}} \to 1$ as $n \to \infty$ then:
  \[
    \frac{\frac{\pi}{2} \cdot 2^{-2n} \cdot \binom{2n}{n}}{\frac{1}{2n + 1} \cdot \left(2^{-2n} \cdot \binom{2n}{n}\right)^{-1}} \to 1 \implies \left(2^{-2n} \cdot \binom{2n}{n}\right)^{2} \sim \frac{1}{\pi n} \text{ as $n \to \infty$}
  \]
  which is the desired result.

  Consider $\frac{I_n}{I_{n - 2}}$:
  \[
    \frac{I_n}{I_{n - 2}} = \frac{n - 1}{n} \to 1 \text{ as $n \to \infty$}
  \]
  $I_n$ is decreasing in $n$ as the integrand is $0 \leq \cos \theta \leq 1$ over the integration bounds, so:
  \[
    \frac{I_{2n}}{I_{2n + 1}} \leq \frac{I_{2n - 1}}{I_{2n + 1}} \to 1 \text{ and } \frac{I_{2n}}{I_{2n + 1}} \geq \frac{I_{2n}}{I_{2n - 2}} \to 1
  \]
  So $\frac{I_{2n}}{I_{2n + 1}} \to 1$ by squeeze theorem as $n \to \infty$.

  \textbf{Conclusion}\par
  This forces $A = \sqrt{2\pi}$ and so $n! \sim \sqrt{2\pi n} \left(\frac{n}{e}\right)^{n}$.
\end{proof}
\section{Independence of Events}
\subsection{Independence}
\begin{definition}[Independent Events]
  For $A, B \in \salg$, we say that $A$ is \textit{independent} of $B$ and write $A \perp B$ if:
  \label{independentEvents}
  \[
    \P(A \cap B) = \P(A) \P(B)
  \]
  A countable collection of events $(A_n)_{n \in \N}$ is said to be independent if for any distinct $i_1, \ldots, i_k$:
  \[
    \P\left(\bigcap_{j = 1}^{k} A_{i_j}\right) = \prod_{j = 1}^{k} \P(A_{i_j})
  \]
\end{definition}
\begin{remark}[Warning]
  Pairwise independence of events is \textbf{not sufficient} to conclude that a collection of events is independent.
\end{remark}
\begin{example}
  Consider tossing a fair coin twice, associating heads with 0 and tails with 1.
  This has probability space:
  \[
    \Omega = \{(0, 0), (0, 1), (1, 0), (1, 1)\}, \salg = \powerset{(\Omega)},\ \P(\{\omega\}) = \frac{1}{4}
  \]
  Define the events $A = \{(0, 0), (0, 1)\}$, $B = \{(0, 0), (1, 0)\}$, and $C = \{(1, 0), (0, 1)\}$.

  We see that $\P(A) = \P(B) = \P(C) = \frac{1}{2}$.
  Since $\P(A \cap B) = \frac{1}{4} = \frac{1}{2} \cdot \frac{1}{2} = \P(A)\P(B)$, $A \perp B$.
  Similarly, $A \perp C$ and $B \perp C$, so $A, B, C$ are pairwise independent.
  However:
  \[
    \P(A \cap B \cap C) = \P(\emptyset) = 0 \neq \P(A)\P(B)\P(C)
  \]
  so $A, B, C$ is not an independent collection of events even though they are pairwise independent.
\end{example}
\begin{lemma}
  If $A$ is independent of $B$, then $A \perp B^{\comp}$.
\end{lemma}
\begin{proof}
  We wish to show that $\P(A \cap B^{\comp}) = \P(A)\P(B^{\comp})$.
  \begin{align*}
    \P(A \cap B^{\comp}) &= \P(A \setminus (A \cap B)) \\
                         &= \P(A) - \P(A \cap B) \text{ as $A \cap B \subseteq A$} \\
                         &= \P(A) - \P(A) \P(B) \text{ as $A \perp B$}\\
                         &= \P(A)(1 - \P(B)) \\
                         &= \P(A)\P(B^{\comp})
  \end{align*}
  Thus $A \perp B^{\comp}$.
\end{proof}
\subsection{Conditional Probability}
For the following section, consider a probability space $(\Omega, \salg, \P)$.
\begin{definition}[Conditional Probability]
  For $A, B \in \salg$ with $\P(B) > 0$, we define the \textit{conditional probability of $A$ given $B$} as:
  \[
    \P(A \vert B) = \frac{\P(A \cap B)}{\P(B)}
  \]
\end{definition}
\begin{lemma}
  For $A, B \in \salg$ with $\P(B) > 0$, $A$ and $B$ are independent if and only if $\P(A \vert B) = \P(A)$
\end{lemma}
\begin{proof}
\[
  A \perp B \iff \P(A \cap B) = \P(A)\P(B) \iff \P(A) = \frac{\P(A \cap B)}{\P(B)} = \P(A \vert B)
\]
\end{proof}
Intuitively this makes sense since if the event $B$ occurs, it does not change the probability of the event $A$ occurring as $A$ and $B$ are independent.
\begin{proposition}[Countable Additivity for Conditional Probabilities]
  Suppose $(A_n)_{n \in \N}$ is a disjoint sequence of events in $\salg$, then:
  \label{conditionalAdditivity}
  \[
    \P\left(\left. \bigcup\nolimits_{n \in \N} A_n \middle\vert B \right.\right) = \sum_{n \in \N} \P(A_n \vert B)
  \]
  That is, countable additivity holds for conditional probabilities.
\end{proposition}
\begin{proof}
  \begin{align*}
    \P\left(\left. \bigcup\nolimits_{n \in \N} A_n \middle\vert B \right.\right) &= \frac{\P([\bigcup_{n \in \N} A_n] \cap B)}{\P(B)} \\
                                                                                 &= \frac{\P(\bigcup_{n \in \N} (A_n \cap B))}{\P(B)}
  \end{align*}
  The sequence $(A_n)$ is disjoint, so for $i \neq j$, $A_i \cap A_j = \emptyset \implies (A_i \cap B) \cap (A_j \cap B) = \emptyset$.
  So $(A_n \cap B)$ is a disjoint sequence, therefore, we can apply the countable additivity property of the normal probability measure:
  \begin{align*}
    \P\left(\left. \bigcup\nolimits_{n \in \N} A_n \middle\vert B \right.\right) &= \sum_{n \in \N} \frac{\P(A_n \cap B)}{\P(B)} \\
                                                                                 &= \sum_{n \in \N} \P(A_n \vert B)
  \end{align*}
\end{proof}
\begin{remark}
  The above combined with $\P(B \vert B) = 1$ mean that $\P(\ \cdot\ \vert B)$ is a new probability measure that is related to the original probability measure by the definition of conditional probability.
\end{remark}
\begin{proposition}[Law of Total Probability]
  If $(B_n)_{n \in \N}$ is a disjoint collection of events in $\salg$ such that $\bigcup_{n \in \N} B_n = \Omega$ and $\P(B_n) > 0\ \forall n$, then for any $A \in \salg$:
  \[
    \P(A) = \sum_{n \in \N} \P(A \vert B_n)\P(B_n)
  \]
  \label{lawOfTotalProb}
\end{proposition}
\begin{proof}
  Since $A \in \salg$, $A \subseteq \Omega$ so $A \cap \Omega = A$.
  Therefore, we have:
  \begin{align*}
    \P(A) = \P(A \cap \Omega) &= \P\left(A \cap \left[\bigcup_{n \in \N} B_n\right]\right) \\
                              &= \P\left(\bigcup_{n \in \N} (B_n \cap A)\right) \\
                              &= \sum_{n \in \N} \P(B_n \cap A) \text{ as $(B_n \cap A)$ are disjoint}\\
                              &= \sum_{n \in \N} \P(A \vert B_n) \P(B_n) \text{ as $\P(A \vert B_n) = \frac{\P(B_n \cap A)}{\P(B_n)}$}
  \end{align*}
\end{proof}
\subsection{Bayes' Formula}
\begin{proposition}[Bayes' Formula]
  If $(B_n)_{n \in \N}$ is a disjoint collection of events in $\salg$ such that $\bigcup_{n \in \N} B_n = \Omega$ and $\P(B_n) > 0\ \forall n$, then for any $A \in \salg$ with $\P(A) > 0$:
  \[
    \P(B_k \vert A) = \frac{\P(A \vert B_k)\P(B_k)}{\sum_{i \in \N} \P(A \vert B_i) \P(B_i)}
  \]
\end{proposition}
\begin{proof}
  We can write:
  \[
    \P(B_k \vert A) = \frac{\P(B_k \cap A)}{\P(A)} = \frac{\P(A \vert B_k)\P(B_k)}{\P(A)}
  \]
  and then use the law of total probability (\cref{lawOfTotalProb}) to rewrite $\P(A)$.
\end{proof}
This formula is the basis for Bayesian statistics.
We call $(B_n)$ the \textit{parameters} and $A$ the \textit{evidence}.
If we know the probabilities of each of the parameters, $\P(B_k)$, and we have a model which gives us $\P(A \vert B_k)$ (the probability of the evidence occurring given each of the parameters), then with this formula we can find the \textit{posterior probabilities}.
These are the probabilities of a particular parameter $B_n$ occurring given the evidence $A$ occurs.
\subsection{Examples}
\begin{example}[False Positives]
  Suppose that condition $A$ affects 0.1\% of the population.
  We have a test which is positive for 98\% of the affected population and 1\% of those unaffected.
  Pick an individual at random.
  What is the probability that they suffer from A given that they tested positive?

  Define $A = \{\text{individual suffers from A}\}$, $P =\{\text{individual tests positive}\}$.
  Our parameters are $A$ and $A^{\comp}$ and our evidence is $P$.
  We want the probability $\P(A \vert P)$ and so need:
  \[
    \P(A) = 0.001,\ \P(A^{\comp}) = 0.999,\ \P(P \vert A) = 0.98,\ \P(P \vert A^{\comp}) = 0.01
  \]
  Using Bayes' formula:
  \[
    \P(A \vert P) = \frac{\P(P \vert A)\P(A)}{\P(P \vert A)\P(A) + \P(P \vert A^{\comp})\P(A^{\comp})} = 0.089 \approx 0.09
  \]

  This result seems counter-intuitive, however it can be more easily explained if we rewrite $P(A \vert P)$ as:
  \[
    \P(A \vert P) = \frac{1}{1 + \frac{\P(P \vert A^{\comp})\P(A^{\comp})}{\P(P \vert A)\P(A)}}
  \]
  $\P(A^{\comp})$ and $\P(P \vert A)$ are very close to 1 so the relevant ratio governing the size of $\P(A \vert P)$ is $\frac{\P(P \vert A^{\comp})}{\P(A)}$.

  The probability of a false positive, is much higher than the probability of having the disease, that is, $\P(P \vert A^{\comp}) \gg \P(A)$.
  This makes $\frac{\P(P \vert A^{\comp})}{\P(A)}$ large and therefore makes $\P(A \vert P)$ very small.

  \textbf{Example on a population of size 1000}\par
  Suppose we have a population of 1000 people with 1 suffering from A.
  Amongst the 999 of those not suffering, about 10 people will test positive (i.e. 10 false positives).
  In total there will be 11 people testing positive, but with only 1 of them actually suffering from A.
  So if we pick one of the positive results at random, the probability that they are suffering from A is $\frac{1}{11} \approx 0.09$.
\end{example}
\begin{example}
  \textbf{What is the probability someone has 2 boys given that they have 2 children, one of whom is a boy?}\par
  Since no further information is given, we take all outcomes to be equally likely, that is, a child is equally as likely to be either a boy or a girl.
  We define the 4 disjoint events:
  \begin{align*}
    BG &= \{\text{elder child is a boy, younger is a girl}\} \\
    GB &= \{\text{elder child is a girl, younger is a boy}\} \\
    BB &= \{\text{both are boys}\} \\
    GG &= \{\text{both are girls}\}
  \end{align*}
  One child is a boy is the event $BB \cup BG \cup GB$ and children are boys is the event $BB$.
  Therefore we want $\P(BB \vert BB \cup BG \cup GB)$:
  \begin{align*}
    \P(BB \vert BB \cup BG \cup GB) &= \frac{\P(BB \cap (BB \cup BG \cup GB))}{\P(BB \cup BG \cup GB)} \\
                                    &= \frac{\P(BB)}{\P(BB \cup BG \cup GB)} = \frac{1/4}{3/4} = \frac{1}{3}
  \end{align*}

  \textbf{What is the probability that they have two boys given the eldest child is a boy?}\par
  $BB \cup BG$ is the event the eldest child is a boy so we want:
  \[
    \P(BB \vert BB \cup BG) = \frac{1}{2}
  \]

  \textbf{What is the probability we have two boys given that one of them is a boy born on a Thursday?}\par
  We define the 5 disjoint events:
  \begin{align*}
    TG &= \{\text{eldest is a boy born on a Thursday, youngest is a girl}\} \\
    GT &= \{\text{eldest is a girl, youngest is a boy born on Thursday}\} \\
    TN &= \{\text{eldest is a boy born on a Thursday, youngest is a boy not born on a Thursday}\} \\
    NT &= \{\text{eldest is a boy not born on a Thursday, youngest is a boy born on a Thursday}\} \\
    TT &= \{\text{both are boys born on a Thursday}\}
  \end{align*}
  We assume, similarly to before, that there is an equal chance for a child to be born on a each of the days of the week.
  We wish to find:
  \begin{align*}
    &\P(TT \cup TN \cup NT \vert TT \cup TG \cup GT \cup TN \cup NT) \\
                                   &=\frac{\P((TT \cup TN \cup NT) \cap (TT \cup TG \cup GT \cup TN \cup NT))}{\P(TT \cup TG \cup GT \cup TN \cup NT)} \\
                                   &= \frac{\P(TT \cup TN \cup NT)}{\P(TT \cup TG \cup GT \cup TN \cup NT)} \\
                                   &= \frac{\frac{1}{2} \cdot \frac{1}{7}\cdot \frac{1}{2} \cdot \frac{1}{7} + \frac{1}{2} \cdot \frac{1}{7} \cdot \frac{1}{2} \cdot \frac{6}{7} + \frac{1}{2} \cdot \frac{6}{7} \cdot \frac{1}{2} \cdot \frac{1}{7}}{\frac{1}{2} \cdot \frac{1}{7} \cdot \frac{1}{2} \cdot \frac{1}{7} + \frac{1}{2} \cdot \frac{1}{7} \cdot \frac{1}{2} + \frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{7} + \frac{1}{2} \cdot \frac{1}{7} \cdot \frac{1}{2} \cdot \frac{6}{7} \cdot 2} \\
                                   &= \frac{13}{27} > \frac{1}{3}
  \end{align*}

  \textbf{Summary}\par
  When conditioning, when we specified which one we know is a boy is we got $1/2$.
  When did not specify which one was a boy we get $1/3$, and when we specify that one is a boy born on a Thursday (a relatively unlikely event), our probability moves back towards $1/2$.

  In this case, if we condition on an event with probability $\alpha$, as $\alpha$ gets smaller and smaller, the conditional probability gets closer to $1/2$.
\end{example}
\begin{example}[Simpson's Paradox]
  Consider the following example admissions data:
  \begin{center}
    \begin{tabular}{c|c|c|c}
      \hline\hline
      All Applicants & Admitted & Rejected & \% Admitted \\
      \hline
      State & 25 & 25 & 50\% \\
      Independent & 28 & 22 & 56\% \\
      \hline\hline
      London Schools & Admitted & Rejected & \% Admitted \\
      \hline
      State & 15 & 22 & 41\% \\
      Independent & 5 & 8 & 38\% \\
      \hline\hline
      Cambridge Schools & Admitted & Rejected & \% Admitted \\
      \hline
      State & 10 & 3 & 77\% \\
      Independent & 23 & 14 & 62\% \\
    \end{tabular}
  \end{center}
  Notice how when we look at all applicants, the acceptance rate is higher from independent schools than it is from state schools.
  However, when we partition the data into two subcategories, we see that the acceptance rate from state schools is higher in both cases.

  This phenomenon is called \textit{confounding} in statistics.
  It arises when we aggregate data from two disparate populations.

  Define the following events for a random individual:
  \begin{align*}
    A &= \{\text{individual is admitted}\} \\
    B &= \{\text{individual is from London}\},\ B^{\comp} = \{\text{individual is from Cambridge}\} \\
    C &= \{\text{individual is from a state school}\},\ C^{\comp} = \{\text{individual is from an independent school}\}
  \end{align*}
  For this set of data, we see from the second table that:
  \[
    \P(A \vert B \cap C) > \P(A \vert B \cap C^\comp)
  \]
  that is, the probability that a student is admitted given they are from a London state school is higher than the probability they are admitted given that they are from a London independent school.

  We see from the third table that a similar thing can be said about admitted students from Cambridge:
  \[
    \P(A \vert B^{\comp} \cap C) > \P(A \vert B^{\comp} \cap C^\comp)
  \]
  However, as discussed, from the first table, we see that:
  \[
    \P(A \vert C) < \P(A \vert C^\comp)
  \]
  so when aggregated, the acceptance rate is higher from independent schools.

  We can also get the inequality to go in the other direction.
  This can be seen if we rewrite $\P(A \vert C)$ as follows:
  \begin{align*}
    \P(A \vert C) &= \P((A \cap B) \cup (A \cap B^{\comp}) \vert C)\\
                  &= \P(A \cap B \vert C) + \P(A \cap B^{\comp} \vert C) \text{ using \cref{conditionalAdditivity}} \\
                  &= \frac{\P(A \cap B \cap C)}{\P(C)} + \frac{\P(A \cap B^{\comp} \cap C)}{\P(C)} \text{ by definition} \\
                  &= \frac{\P(A \vert B \cap C)\P(B \cap C)}{\P(C)} + \frac{\P(A \vert B^{\comp} \cap C)\P(B^{\comp} \cap C)}{\P(C)} \\
                  &= \P(A \vert B \cap C)\P(B \vert C) + \P(A \vert B^{\comp} \cap C)\P(B^{\comp} \vert C) \\
                  &> \P(A \vert B \cap C^\comp)\P(B \vert C) + \P(A \vert B^{\comp} \cap C^\comp)\P(B^{\comp} \vert C) \text{ by assumption}
  \end{align*}
  So if $\P(B \vert C) = \P(B \vert C^\comp)$ (which it is not here), then we would get that:
  \begin{align*}
    \P(A \vert C) &> \P(A \vert B \cap C^\comp)\P(B \vert C^\comp) + \P(A \vert B^{\comp} \cap C^\comp)\P(B^{\comp} \vert C^\comp) \\
                  &= \P(A \vert C^\comp) \text{ by following the above backwards with $C \mapsto C^\comp$}
  \end{align*}
  so the inequality between $\P(A \vert C)$ and $\P(A \vert C^\comp)$ can go in both directions.

  This boils down to that if we consider natural numbers $a_1, a_2, b_1, b_2, c_1, c_2, d_1, d_2$ where we know the ratios satisfy:
  \[
    \frac{a_1}{a_2} > \frac{b_1}{b_2} \text{ and } \frac{c_1}{c_2} > \frac{d_1}{d_2}
  \]
  then we do not know the relation between:
  \[
    \frac{a_1 + c_1}{a_2 + c_2}\ ? \ \frac{b_1 + d_1}{b_2 + d_2}
  \]
  unless we know the specific values of the numbers.

  In the above example:
  \[
    \frac{a_1}{a_2} = \frac{15}{37} = 41\%,\ \frac{b_1}{b_2} = \frac{5}{13} = 38\%,\ \frac{c_1}{c_2} = \frac{10}{13} = 77\%,\ \frac{d_1}{d_2} = \frac{23}{37} = 62\%
  \]
  and so:
  \[
    50\% = \frac{15 + 10}{37 + 13} < \frac{5 + 23}{37 + 13} = 66\%
  \]
\end{example}
\end{document}
