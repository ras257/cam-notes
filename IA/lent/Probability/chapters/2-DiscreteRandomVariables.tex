\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Discrete Random Variables}
\section{Discrete Probability Distributions}
\subsection{Definition}
\begin{definition}[Discrete Probability Distribution]
  Suppose we have a probability space $(\Omega, \salg, \P)$ where $\Omega$ is \textbf{finite or countable}:
  \[
    \Omega = \{\omega_1, \omega_2, \ldots\},\ \salg = \powerset{\Omega}
  \]
  If we know $\P(\{\omega_i\})$ for all $i$, then for any $A \in \salg$:
  \[
    \P(A) = \P\left(\bigcup_{\omega_i \in A} \{\omega_i\}\right) = \sum_{\omega_i \in A} \P(\{\omega_i\})
  \]
  We then call $(\P(\{\omega_i\}))_i$ a \textit{discrete probability distribution} and we write $p_i = \P(\{\omega_i\})$.
\end{definition}
Since the $p_i$ are all probabilities, we must have $p_i \geq 0\ \forall i$.
Furthermore, since $\P(\Omega) = 1$, we must also have $\sum_{i} p_i = 1$
\begin{remark}[Summary]
   If we have a probability space $(\Omega, \salg, \P)$ where the $\Omega$ is at most countably infinite, then the \textit{discrete probability distribution} refers to the probabilities that $\P$ assigns to each singleton $\{\omega_i\}$ of $\Omega$.
\end{remark}
We will now cover some important discrete probability distributions.
\subsection{Bernoulli Distribution}
\label{bernoulliDist}
\begin{definition}[Bernoulli Distribution]
  The \textit{Bernoulli distribution} with parameter $p \in [0, 1]$, models a probability space with two outcomes where:
  \[
    \Omega = \{0, 1\},\ p_0 = \P(\{0\}) = 1 - p,\ p_1 = \P(\{1\}) = p
  \]
\end{definition}
This means Bernoulli distributions model the outcome of a single trial with two outcomes, for example the toss of a biased coin.
If we associate tails with 0 and heads with 1 then:
\[
  \P(\text{Heads}) = p_1 = p,\ \P(\text{Tails}) = p_0 = 1 - p
\]
\begin{remark}
  A coin with a probability $p$ of heads is sometimes called a $p$-coin.
\end{remark}
\subsection{Binomial Distribution}
\label{binomialDist}
\begin{definition}[Binomial Distribution]
  The \textit{binomial distribution} with parameters $n \in \N$ and $p \in [0, 1]$ is denoted $\binomial(n, p)$.

  It models number of successes in $n$ independent trials, each with an outcome of either success, with probability $p$, or failure, with probability $1 - p$.

  $\Omega$ all the possible numbers of trials where the outcome was success, i.e. $\Omega = \{1, \ldots, n\}$ as we can have anywhere from 1 to $n$ successful trials.

  The probability $p_k = \P(\{k\})$ is:
  \[
    p_k = \P(\text{$k$ successes}) = \binom{n}{k}p^{k}(1 - p)^{n - k} \text{ for $k \in \Omega$}
  \]
  as there are $\binom{n}{k}$ choices for when the successes should be.
\end{definition}
\begin{remark}[Intuition]
  We can think of the binomial distribution as modelling the number of heads we get if we toss a $p$-coin $n$ times.
\end{remark}
\subsection{Multinomial Distribution}
\begin{definition}[Multinomial Distribution]
  The \textit{multinomial distribution} with parameters $n \in \N$ and $p_1, \ldots, p_k \in [0, 1]$ such that $\sum_{i = 1}^{k} p_i = 1$, is denoted $\multinomial(n, p_1, \ldots, p_k)$.

  It models the outcome of $n$ independent trials, each with $k$ outcomes of probability $p_i$ for $1 \leq i \leq k$.

  $\Omega$ is the set of all tuples whose entries are the number of times $n_i$ each of the $k$ outcomes occurred:
  \[
    \Omega = \left\{(n_1, \ldots, n_k) \in \N^{k} : \sum_{i = 1}^{k} n_i = n\right\}
  \]
  We require their sum to be $n$ as we did $n$ trials in total.

  The probability of a particular tuple, $p_{n_1, \ldots, n_k} = \P(\{(n_1, \ldots, n_k)\})$ is:
  \begin{align*}
    p_{n_1, \ldots, n_k} &= \binom{n}{n_1}p^{n_1}_{1} \cdot \binom{n - n_1}{n_2}p^{n_2}_{2} \cdots \binom{n - n_1 - \cdots - n_{k - 1}}{n_k} p^{n_k}_{k} \\
                         &= \binom{n}{n_1, \ldots, n_k} \cdot p^{n_1}_{1} \cdots p^{n_k}_{n}
  \end{align*}
  This uses the \textit{multinomial coefficient} that we saw in \cref{multinomialCoefficients}.
\end{definition}
\begin{remark}[Intuition]
  We can think of the multinomial distribution as modelling the results of tossing $n$ balls independently into $k$ boxes where each ball goes into box $i$ with probability $p_i$.
  Therefore:
  \[
    \P(n_1 \text{ balls in box 1}, \ldots, n_k \text{ balls in box $k$}) = p_{n_1, \ldots, n_k}
  \]
\end{remark}
\subsection{Geometric Distribution}
\begin{definition}[Geometric Distribution]
  The \textit{geometric distribution} with parameter $p \in [0, 1]$ is denoted $\geometric(p)$.
  There are two ways to define the geometric distribution:

  \textbf{Starting from 1}\par
    When starting from 1, it models the number of \textit{Bernoulli trials} (success with probability $p$, failure with probability $1 - p$) \textbf{needed to get the first success}.

    In this case $\Omega$ is the possible numbers of trials needed before a success first occurs so $\Omega = \N$.

    The probability $p_k = \P(\{k\})$ is then:
    \[
      p_k = \P(\text{$k$ trials needed for the first success}) = (1-p)^{k - 1}p \text{ for } k \in \Omega
    \]
  \textbf{Starting from 0}\par
    Starting from 0 is defined similarly, however, it now models the \textbf{number of failures before the first success}.
    So $\Omega$ is instead $\N_0$ and $p_k = \P(\{k\})$ is:
    \[
      p_k = \P(\text{$k$ failures before the first success}) = (1-p)^{k}p \text{ for } k \in \Omega
    \]
\end{definition}
\begin{remark}[Intuition]
  Consider tossing a $p$-coin until you get heads.
  We can think of the geometric distribution as modelling the number of tosses needed before the first head (if starting from 1), or the number of tails you would see before the first head (if starting from 0).
\end{remark}
\subsection{Poisson Distribution}
\begin{definition}[Poisson Distibution]
  The \textit{Poisson distribution} with parameter $\lambda > 0$ is denoted $\poisson(\lambda)$.

  It models the probability of a given number of events occurring in a fixed interval of time if these events occur with a mean rate $\lambda$ per time interval and independently of the time since the last event.

  $\Omega$ is all the possible numbers of times an event could occur in the interval, i.e., $\Omega = \N_0$.

  The probability $p_k = \P(\{k\})$ is:
  \[
    p_k = \P(\text{$k$ events in one interval}) = \frac{\lambda^{k} e^{-\lambda}}{k!}
  \]
\end{definition}
\begin{remark}
  Although we defined the Poisson distribution using an interval of time, it can also be used with over intervals such as areas or volumes.
\end{remark}
\begin{remark}[Intuition]
  Suppose customers arrive at a shop at a constant average rate of $\lambda$ customers per hour.
  We can think of the Poisson distribution as modelling the number of customers that arrive in a given hour, assuming that they arrive independently of each other and that customers cannot arrive at exactly the same instant.
\end{remark}
\subsubsection{Limit of a binomial distribution}
The Poisson distribution can be thought of as the limit of a binomial distribution (\cref{binomialDist}).

Suppose that the events occur in the interval $[0, 1]$.
We can discretise $[0, 1]$ as $[\frac{i - 1}{N}, \frac{i}{N}]$ for $i = 1, \ldots, N$ for some $N \in \N$.

In each interval $[\frac{i - 1}{N}, \frac{i}{N}]$, suppose the event occurs with probability $p$ and does not occur with probability $1 - p$ independently for different intervals.
\[
  \widetilde{p}_k = \P(\text{event occurred $k$ times}) = \binom{N}{k} p^{k} (1 - p)^{N - k},\ k = 0, \ldots, N
\]
as we pick a subset of $k$ intervals for the event to occur in.
This is $\binomial(N, p)$.

As $\lambda$ is the rate of the event occurring over $[0, 1]$, the probability of it occurring in one of the $N$ intervals is approximately $p = \frac{\lambda}{N}$.
Substituting this into $p_k$, we have:
\[
  \widetilde{p}_k = \frac{N!}{k!(N - k)!} \left(\frac{\lambda}{N}\right)^{k} \left(1-\frac{\lambda}{N}\right)^{N - k}
\]
Keeping $k$ fixed and taking the limit as $N \to \infty$ so that the intervals become infinitesimal and our approximation becomes exact:
\[
  \widetilde{p}_k = \frac{\lambda^{k}}{k!} \cancelto{1}{\frac{N(N - 1) \cdots (N - k + 1)}{N^{k}}} \cancelto{e^{-\lambda}}{\left(1 - \frac{\lambda}{N}\right)^{N - k}}
\]
So we see that $\widetilde{p}_k \to \frac{\lambda^{k} e^{-\lambda}}{k!} = p_k$ as $N \to \infty$.
\section{Random Variables}
\subsection{Definition}
Consider a probability space $(\Omega, \salg, \P)$:
\begin{definition}[Random Variable]
  A \textit{random variable} is a function $X: \Omega \to \R$ with the property that for $\forall x \in \R$:
  \[
    \{\omega \in \Omega : X(\omega) \leq x\} \in \salg
  \]
\end{definition}
We impose the restriction that $\{\omega \in \Omega : X(\Omega) \leq x\} \in \salg$ so that we are able to talk about the probability of it occurring as an event.
\begin{remark}[Notation]
  We write $\{X \leq x\} \equiv \{\omega \in \Omega: X(\omega) \leq x\}$ and for any $A \subseteq \R$, we write $\{X \in A\} \equiv \{\omega \in \Omega: X(\omega) \in A\}$.
\end{remark}
\begin{example}[Indicator Functions]
  Let $A \in \salg$.
  Define the \textit{indicator} of $A$ as:
  \[
    1_A: \Omega \to \{0, 1\},\ 1_A(\omega) = \begin{cases}
      1 & \text{ if } \omega \in A \\
      0 & \text{ if } \omega \in A^{\comp}
    \end{cases}
  \]
  $1_A$ is a random variable as for $x \geq 1$, $\{\omega \in \Omega: 1_A(\omega) \leq x\} = \Omega \in \salg$, and for $x < 1$, $\{\omega \in \Omega: 1_A(\omega) \leq x\} = A^{\comp} \in \salg$ which is true as $A \in \salg$.
\end{example}
\begin{definition}[Probability Distribution Function]
  For a random variable $X$, we define the \textit{probability distribution function of $X$} to be:
  \[
    F_X: \R \to [0, 1],\  F_X(x) = \P(X \leq x)
  \]
\end{definition}
\begin{definition}[$n$-dimensional Random Variable]
  $X = (X_1, \ldots, X_n)$ is called a \textit{random variable in $\R^{n}$} if $(X_1, \ldots, X_n): \Omega \to \R^{n}$ such that $\forall x_1, \ldots, x_n \in \R$:
  \[
    \{\omega: X(\omega) \leq x_1, \ldots, X_n(\omega) \leq x_n\} \in \salg
  \]
  Equivalently, we require that all $X_i$ are real random variables so that:
  \[
    \{X_1 \leq x_1, \ldots, X_n \leq x_n\} = \{X_1 \leq x_1\} \cap \cdots \cap \{X_n \leq x_n\} \in \salg
  \]
  as $\{X_i \leq x_i\} \in \salg\ \forall i \in \{1, \ldots, n\}$ as all the $X_i$ are random variables.
\end{definition}
\begin{remark}[Notation]
  We write $\{X_1 \leq x_1, \ldots, X_n \leq x_n\} \equiv \{\omega: X(\omega) \leq x_1, \ldots, X_n(\omega) \leq x_n\}$.
\end{remark}
\subsection{Discrete Random Variables}
\begin{definition}[Discrete Random Variable]
  A random variable $X$ is called \textit{discrete} if it takes values in a countable set.

  Suppose it takes values in a countable set $S$, then for every $x \in S$, we write:
  We call $(p_x)_{x \in S}$ the \textit{probability mass function} or \textit{pmf} of $X$ or the \textit{distribution} of $X$.

\end{definition}
\begin{definition}[Probability Mass Function]
  For a discrete random variable $X$ taking values in a countable set $S$, the \textit{probability mass function}, \textit{pmf}, or \textit{distribution} of $X$ is defined to be:
  \[
    p_X: X \to S,\ x \mapsto \P(X = x) = \P(\{\omega \in \Omega: X(\omega) = x\})\
  \]
\end{definition}
If $A \subseteq S$, then $\P(X \in A) = \sum_{x \in A} p_X(x)$.

If the $p_X(x)$ are the Bernoulli distribution, then we say that $X$ is a \textit{Bernoulli r.v. (random variable)}, or we say that \textit{$X$ is Bernoulli distributed}.
For other discrete distributions, we use a similar convention, for example, we might say that $X$ is binomially distributed or that $X$ follows a Poisson distribution.
\begin{remark}[Notation]
  We use the notation $X \sim \binomial(n, p)$ to mean ``$X$ is binomially distributed'', and similarly for other distributions.
\end{remark}
\begin{remark}[Recap]
Recall from \cref{independentEvents} that two events $A, B \in \salg$ are independent if $\P(A \cap B) = \P(A)\P(B)$.
\end{remark}
\begin{definition}[Independent Discrete Random Variables]
  If $X_1, \ldots, X_n$ are discrete random variables with values in $S_1, \ldots, S_n$, then they are called \textit{independent random variables} if $\forall x_1 \in S_1, \ldots, x_n \in S_n$:
  \[
    \P(X_1 = x_1, \ldots, X_n = x_n) = \P(X_1 = x_1) \cdots \P(X_n = x_n)
  \]
\end{definition}
This builds on our definition for independence of events.
For discrete random variables $X_1, X_2$, we using the events $\{X_1 = x_1\}$ and $\{X_2 = x_2\}$ but require that these events are independent \textbf{for all} possible $x_1 \in S_1$ and $x_2 \in S_2$.
If this is the case then:
\begin{align*}
  \P(X_1 = x_1, X_2 = x_2) &= \P(\{X_1 = x_1\} \cap \{X_2 = x_2\}) \\
                           &= \P(\{X_1 = x_1\})\P(\{X_2 = x_2\}) \text{ by independence of events} \\
                           &= \P(X_1 = x_1)\P(X_2 = x_2)
\end{align*}
for all $x_1 \in S_1$ and $x_2 \in S_2$.
\begin{example}
  \label{binomialAsBernSum}
  Consider tossing a $p$-coin $N$ times independently, as usual, we associate tails with 0 and heads with 1 so that $\Omega =\{0, 1\}^{N},\ \salg = \powerset{\Omega}$.
  For some $\omega \in \Omega$, label the elements of $\omega$ as $(\omega_1, \ldots, \omega_n)$ where $\omega_i$ is the outcome of the $i$-th toss.

  We calculate the probability of a singleton event $\{\omega\}$ as:
  \[
    p_\omega = \P(\{\omega\}) = \prod_{k = 1}^{N} p^{\omega_k} (1 - p)^{1 - \omega_k}
  \]
  The terms of the product give the correct probability for each toss as:
  \[
     p^{\omega_k} (1-p)^{1 - \omega_k}= \begin{cases}
     p & \text{ if } \omega_k = 1 \\
     1 - p & \text{ if } \omega_k = 0
    \end{cases}
  \]

  For all $\forall k = 1, \ldots, N$, define the random variables $X_k: \Omega \to \{0, 1\}$, $X_k(\omega) = \omega_k$.
  These are all random variable as for all $x \in \R$, $\{\omega \in \Omega: X_k(\omega) \leq x\} \in \powerset{\omega} = \salg$ and are all discrete r.v.s as they take values in the countable set $\{0, 1\}$.

  As these are independent tosses, for any $k$ (i.e. the $k$-th toss):
  \[
    \P(X_k = 1) = \P(\omega_k = 1) = p \text{ and } \P(X_k = 0) = \P(\omega_k = 0) = 1 - p
  \]
  So $X_k$ is Bernoulli r.v. (see \cref{bernoulliDist}) with parameter $p$.

  We can also show that $X_1, \ldots, X_n$ are independent r.v.s.
  For $\forall x_1, \ldots, x_N \in \{0, 1\}$:
  \begin{align*}
    \P(X_1 = x_1, \ldots, X_n = x_n) &= \P(\{(x_1, \ldots, x_n)\})\\
                                     &= \prod_{k = 1}^{N} p^{x_k}(1 - p)^{1 - x_k} \\
                                     &= \prod_{k = 1}^{N} \P(X_k = x_k)
  \end{align*}
  so they are independent.

  We can also define a new discrete random variable $S_N: \Omega \to \N_0$:
  \[
    S_N(\omega) = X_1(\omega) + \cdots + X_N(\omega) = \text{\#(heads in $n$ tosses)}
  \]
  this again has to be a random variable as $\salg = \powerset{\Omega}$.
  We then have:
  \[
    \P(S_N = k) = \binom{N}{k}p^{k}(1-p)^{N - k}
  \]
  which is $\binomial(N, p)$ so $S_N$ is a binomial r.v.
  So a binomial r.v. with parameters $N$ and $p$ can also be though of as the sum of $N$ Bernoulli r.v.s with parameter $p$.
\end{example}
\subsection{Expectation}
\begin{definition}[Non-negative Random Variable]
  Let $X: \Omega \to \R$ be a random variable.
  We say that $X$ is \textit{non-negative} if $X(\omega) \geq 0\ \forall \omega \in \Omega$.
\end{definition}
\begin{definition}[Expectation for non-negative discrete random variables]
  For a \textbf{non-negative discrete random variable} $X$, we define the \textit{expectation} of $X$ to be:
  \label{nonNegativeExpectation}
  \[
    \E[X] = \sum_{\omega \in \Omega} \P(\{\omega\})X(\omega)
  \]
  since $X$ is a discrete random variable, this is a finite or countable sum.
\end{definition}
Define $\Omega_X = \{X(\omega): \omega \in \Omega\}$, that is, the image of $\Omega$ under $X$.
This allows us to partition $\Omega$ into the disjoint sets $\{\omega \in \Omega: X(\omega) = x\} = \{X = x\}$ for each $x \in \Omega_x$.
We can therefore write $\Omega = \bigcup_{x \in \Omega_X} \{X = x\}$.
Since they are disjoint, we can rewrite $\E[X]$ as follows:
\begin{align*}
  \E[X] &= \sum_{\omega \in \Omega} X(\omega) \P(\{\omega\}) \\
        &= \sum_{x \in \Omega_X} \sum_{\omega \in \{X = x\}} X(\omega)\P(\{\omega\}) \\
        &= \sum_{x \in \Omega_X} \sum_{\omega \in \{X = x\}} x \cdot \P(\{\omega\}) \text{ as $X(\omega) = x$ for $\omega \in \{X = x\}$} \\
        &= \sum_{x \in \Omega_X} x \sum_{\omega \in \{X = x\}} \P(\{\omega\}) \\
        &= \sum_{x \in \Omega_X} x \cdot \P(X = x) \text{ as $P(A) = \sum_{\omega \in A} \P(\{\omega\})$ for finite/countable $A$}
\end{align*}
So, the expectation of $X$ is a weighted average of the values taken by $X$ with weights given by $\P(X = x)$.
This formula for expectation is used more often than the one in the definition.
\begin{example}[Expectation of Binomial Distributed Variable]
  \label{binomialExpectation}
  Suppose $X \sim \binomial(N, p)$.
  This means that:
  \[
    \P(X = k) = \binom{N}{k}p^{k}(1-p)^{N-k}\ \forall k \in \{0, \ldots, N\}
  \]
  To find the expected value, $\Omega_X = \{0, \ldots, N\}$ so:
  \begin{align*}
    \E[X] &= \sum_{k = 0}^{N} k \cdot \P(X = k) \\
          &= \sum_{k = 1}^{N} k \cdot \frac{N!}{(N - k)!k!} p^{k}(1 - p)^{N - k}  \text{ discard first term as 0} \\
          &= Np \cdot \sum_{k = 1}^{N} \frac{(N - 1)!}{(k - 1)!((N - 1) - (k - 1))!}p^{k - 1}(1 - p)^{(N - 1) - (k - 1)} \\
          &= Np \cdot \sum_{k = 1}^{N} \binom{N-1}{k - 1}p^{k - 1}(1 - p)^{(N - 1) - (k - 1)} \\
          &= Np \cdot \sum_{k = 0}^{N - 1} \binom{N - 1}{k}p^{k}(1 - p)^{(N - 1) - k} \text{ re-indexing $k \mapsto k + 1$}\\
          &= Np (p + 1 - p)^{N - 1} = Np
  \end{align*}
  So the expected value of a random variable that is binomially distributed is the product of its parameters.
\end{example}
\begin{example}[Expectation of Poisson Distributed Variable]
  Suppose $X \sim \poisson(\lambda)$, $\lambda > 0$.
  This means that:
  \[
    \P(X = k) = \frac{e^{-\lambda} \lambda^{k}}{k!}\ \forall k \in \N_0
  \]
  To find the expected value, $\Omega_X = \N_0$ so:
  \begin{align*}
    \E[X] &= \sum_{k = 0}^{\infty} k \cdot \P(X = k) \\
          &= \sum_{k = 1}^{\infty} k \cdot e^{-\lambda} \frac{\lambda^{k}}{k!} \text{ discard first terms as 0} \\
          &= e^{-\lambda} \lambda \sum_{k = 1}^{\infty} \frac{\lambda^{k - 1}}{(k - 1)!} \\
          &= e^{-\lambda} \lambda \sum_{k = 0}^{\infty} \frac{\lambda^{k}}{k!} \text{ re-indexing $k \mapsto k - 1$} \\
          &= e^{-\lambda} \lambda e^{\lambda} = \lambda
  \end{align*}
  So the expected value of a random variable that follows a Poisson distribution is just the value of the parameter $\lambda$.
\end{example}
\subsubsection{Expectation for possibly negative discrete random variables}
So far we have only defined expectation for \textbf{non-negative} discrete random variables in \cref{nonNegativeExpectation}.
However we would like this to extend this to r.v.s which can take negative values.
\begin{remark}[Notation]
  For any random variable $X$, define the random variables $X^{+} = \max\{X, 0\}$ and $X^{-} = \max\{-X, 0\}$.

  We can then write $X = X^{+} - X^{-}$ and $|X| = X^{+} + X^{-}$.
\end{remark}
Suppose $X$ is a discrete random variable.
We can find $\E[X^{+}]$ and $\E[X^{-}]$, since they are non-negative.
\begin{definition}[Expectation of a possibly negative discrete random variable]
  If at least one of $\E[X^{+}]$ and $\E[X^{-}]$ is finite, then we define the expectation of $X$ to be:
  \[
    \E[X] = \E[X^{+}] - \E[X^{-}]
  \]
  If both $\E[X^{+}]$ and $\E[X^{-}]$ are infinite, then $\E[X]$ is not defined.
  Whenever we write $\E[X]$, we imply that it is defined.
\end{definition}
When $\E[X]$ is well-defined, then we have the same formula as before:
\[
  \E[X] = \sum_{x \in \Omega_X} x \cdot \P(X = x)
\]
\begin{definition}[Integrable random variable]
  If $\E[|X|] < \infty$, then we say that $X$ is \textit{integrable}.
\end{definition}
\subsubsection{Properties of Expectation}
\begin{proposition}[Basic Properties of Expectation]
\begin{enumerate}
  \item If $X \geq 0$, then $\E[X] \geq 0$
  \item If $X \geq 0$ and $\E[X] = 0$, then $\P(X = 0) = 1$.
  \item If $c \in \R$, then $\E[cX] = c\E[X]$ and $\E[c + X] = c + \E[X]$.
  \item If $X$ and $Y$ are integrable random variables, then:
    \[
      \E(X + Y) = \E(X) + \E(Y)
    \]
  \item Let $c_1, \ldots, c_n \in \R$ and $X_1, \ldots, X_n$ integrable random variables, then:
    \[
      \E\left[\sum_{i = 1}^{n} c_i X_i\right] = \sum_{i = 1}^{n} c_i \E[X_i]
    \]
    by combining \textbf{iii} and \textbf{iv}.
    We require that they are integrable so that there is not issues with adding infinities.
\end{enumerate}
\end{proposition}
\begin{proof}
  \begin{enumerate}
    \item The sum only consists of non-negative terms as $\P: \salg \to [0, 1]$ and $X \geq 0$.
    \item $\sum_{x \in \Omega_X} x \cdot \P(X = x) = 0 \implies x \cdot \P(X = x) = 0\ \forall x \implies \P(X = x) = 0\ \forall x \neq 0$.
      So $\P(\Omega) = \sum_{x \in \Omega_x} \P(X = x) = \P(X = 0) \implies \P(X = 0) = 1$.
    \item Let $Y(\omega) = cX(\omega)$.
      \begin{align*}
        \sum_{y \in \Omega_Y} y \cdot \P(Y = y) &= \sum_{x \in \Omega_X} cx \cdot \P(Y = cx) \\
                                                &= c \cdot \sum_{x \in \Omega_X} \P(X = x) = \E[X]
      \end{align*}
      as $\{Y = cx\} = \{\omega \in \Omega: cX(\omega) = cx\} = \{X = x\}$.

      Similarly, let $Z(\omega) = X(\Omega) + c$.
      \begin{align*}
        \sum_{z \in \Omega_Z} z \cdot \P(Z = z) &= \sum_{x \in \Omega_X} (x + c) \cdot \P(Z = x + c) \\
                                                &= \sum_{x \in \Omega_X} (x + c) \cdot \P(X = x) \text{ as $\{Z = x + c\} = \{X = x\}$} \\
                                                &= \sum_{x \in \Omega_X} x \cdot \P(X = x) + c\sum_{x \in \Omega_X} \P(X = x) \\
                                                &= \E(X) + c \P(\Omega) = \E(X) + c
      \end{align*}
    \item Let $Z(\omega) = X(\omega) + Y(\omega)$.
      \begin{align*}
        \E(Z) &= \sum_{x \in \Omega_X} \sum_{y \in \Omega_y} (x + y) \P(X = x, y = Y) \\
              &= \sum_{x \in \Omega_X} x \cdot \sum_{y \in \Omega_X} \P(X = x, y = Y) + \sum_{y \in \Omega_Y} y \cdot \sum_{x \in \Omega_X} \P(X = x, y = Y) \\
              &= \sum_{x \in \Omega_X} x \cdot \P(X = x) + \sum_{y \in \Omega_y} y \cdot \P(Y = y) \\
              &= \E[X] + \E[Y]
      \end{align*}
    \item Follows from using \textbf{iii} and induction with \textbf{iv}.
  \end{enumerate}
\end{proof}
\begin{remark}
  Property \textbf{iv} above leads to an alternative proof for \cref{binomialExpectation}.

  We saw in \cref{binomialAsBernSum} that we can express $X$ as $X = B_1 + \cdots B_N$ where $B_i$ are all independent Bernoulli r.v.s with parameter $p$.
  We find that the expectation of a Bernoulli r.v. is:
  \[
    \E[B_i] = 0 \cdot \P(B_i = 0) + 1 \cdot \P(B_i = 1) = p
  \]
  since $\Omega_{B_i} = \{0, 1\}$.
  The expectation of $X$ is then the sum of these expectations:
  \[
    \E[X] = \E\left[\sum_{i = 1}^{N} B_i\right] = \sum_{i = 1}^{N} \E[B_i] = \sum_{i = 1}^{N} p = Np
  \]
\end{remark}
\begin{lemma}[Countably Infinite Expectation Sum]
  If $X_1, X_2, \ldots$ are non-negative discrete r.v.s, then:
  \label{sumExpectationSwap}
  \[
    \E\left[\sum_{n = 1}^{\infty} X_n\right] = \sum_{n = 1}^{\infty} \E[X_n]
  \]
\end{lemma}
\begin{proof}
  $\Omega$ is countable as they are discrete random variables.
  \begin{align*}
    \E\left[\sum_{n = 1}^{\infty} X_n\right] &= \sum_{\omega \in \Omega} \left(\sum_{n = 1}^{\infty} X_n(\omega)\right)\P(\{\omega\}) \\
                                             &= \sum_{n = 1}^{\infty} \sum_{\omega \in \Omega} X_n(\omega)\P(\{\omega\}) \\
                                             &= \sum_{n = 1}^{\infty} \E[X_n]
  \end{align*}
  We are able to swap the two sums without issues as the terms are non-negative.
\end{proof}
\begin{example}[Expectation of Indicator Function]
  \label{expOfIndicator}
  Let $A \in \salg$ and $X(\omega) = 1_A(\omega)$.
  Calculating the expected value is easy as $\Omega_X = \{0, 1\}$ so:
  \begin{align*}
    \E[X] &= 0 \cdot \P(X = 0) + 1 \cdot \P(X = 1) \\
          &= \P(X = 1) \\
          &= \P(\{\omega \in \Omega: X(\omega) = 1_A(\omega) = 1\}) \\
          &= \P(A)
  \end{align*}
  So $\E[X] = \P(A)$.
\end{example}
\begin{proposition}[Expectation of a function of a r.v.]
  Let $g: \R \to \R$ and consider the discrete random variable $g(X)$ defined by $g(X)(\omega) = g(X(\omega))$.
  \[
    \E[g(X)] = \sum_{x \in \Omega_x} g(x) \cdot \P(X = x)
  \]
\end{proposition}
\begin{proof}
  Set $Y = g(X)$.
  \[
    \E[Y] = \sum_{y \in \Omega_Y} y \cdot \P(Y = y) \\
  \]
  We can rewrite the event $\{Y = y\}$ as:
  \begin{align*}
    \{Y = y\} &= \{\omega \in \Omega: Y(\omega) = y\} \\
              &= \{\omega \in \Omega: g(X(\omega)) = y\} \\
              &= \{\omega \in \Omega: X(\omega) \in g^{-1}(\{y\})\} \\
              &= \{X \in g^{-1}(\{y\})\}
  \end{align*}
  So:
  \begin{align*}
    \E[Y] &= \sum_{y \in \Omega_y} y \cdot \P(X \in g^{-1}(\{y\})) \\
          &= \sum_{y \in \Omega_y} y \cdot \sum_{x \in g^{-1}(\{y\})} \P(X = x) \\
          &= \sum_{y \in \Omega_y} \sum_{x \in g^{-1}(\{y\})} g(x) \cdot \P(X = x) \text{ as $g(x) = y\ \forall x \in g^{-1}(\{y\})$}
  \end{align*}
  The sets $g^{-1}(\{y\})$ for all $y \in \Omega_Y$ just define a partition of $\Omega_x$ so summing over all elements of all partition is equivalent to summing over $\Omega_X$.
  Thus:
  \[
    \E[Y] = \E[g(X)] = \sum_{x \in \Omega_X} g(x) \cdot \P(X = x)
  \]
\end{proof}
\begin{lemma}[Expectation of a discrete r.v. in $\N$]
  If $X$ is a discrete random variable that only takes values in $\N$, then:
  \[
    \E[X] = \sum_{k = 1}^{\infty} \P(X \geq k) = \sum_{k=0}^{\infty} \P(X > k)
  \]
\end{lemma}
\begin{proof}
  For any $n \in \N$:
  \[
    n = \sum_{k = 1}^{\infty} 1(n \geq k) = \sum_{k = 0}^{\infty} 1(n > k)
  \]
  As there are exactly $n$:
  \begin{itemize}
    \item $k \in \N$ such that $n \geq k$
    \item  $k \in \N_0$ such that $n > k$
  \end{itemize}
  Therefore, since $X(\omega) \geq 0$ and $X(\omega) \in \N$ for all $\omega$, we have the following:
  \[
    X(\omega) = \sum_{k = 1}^{\infty} 1(X(\omega) \geq k) = \sum_{k = 0}^{\infty} 1(X(\omega) > k)
  \]
  for all $\omega$.

  Taking the expectation, as they are non-negative, we can apply \cref{sumExpectationSwap}:
  \[
    \E[X] = \E\left[\sum_{k = 1}^{\infty} 1(X \geq k)\right] = \sum_{k = 1}^{\infty} \E[1(X \geq k)] = \sum_{k = 1}^{\infty} \P(X \geq k)
  \]
  using $\E[1_A] = \P(A)$ from \cref{expOfIndicator} and similarly for the sum starting from 0.
\end{proof}
\end{document}
