\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Differentiation}
\section{Basics}
\subsection{Definition of Differentiability}
What is the derivative of a function at a point?
\begin{definition}[Differentiable]
  Let $f: X \subseteq \C \to \C$ and let $a \in X$.
  We say that $f$ is \textit{differentiable} at $a$ if the limit:
  \[
    \lim_{x \to a} \frac{f(x) - f(a)}{x - a} = \lim_{h \to 0} \frac{f(a + h) - f(a)}{h}
  \]
  exists.

  The value of this limit is then called the \textit{derivative} of $f$ at $a$ and is denoted $f'(a)$ or $\deriv{f}{x}(a)$.
\end{definition}
\begin{remark}[Note]
  Here both $x \in X$ and $a + h \in X$, we cannot approach it from outside of the domain.
\end{remark}
\begin{remark}
  We can't make sense of the derivative at an isolated point of the domain.
  We \textit{could} define it to have a particular value but it would not lead to any interesting behaviour.
\end{remark}
For accumulation points, we can distinguish how we approach $a$:
\begin{itemize}
  \item For interior points, we can approach in any direction.
    If the limits from different direction disagree, then the limit does not exist.
  \item For non-interior points, the domain restricts how we can approach $a$.
\end{itemize}
We need to be careful about non-interior points however most of the theory for this course will build on the first case.
\begin{example}
  \begin{enumerate}
    \item Consider $f(z) = z$.
      This is differentiable at all points as:
      \[
        f'(z) = \lim_{h \to 0} \frac{f(z + h) - f(z)}{h} = \lim_{h \to 0} 1 =1
      \]
    \item Consider $f(z) = \overline{z}$.
      This is not differentiable at any point.

      If we approach it along the real line by setting $h = \lambda \in \R$:
      \[
        \lim_{\lambda \to 0} \frac{f(z + \lambda) - f(z)}{\lambda} = \lim_{\lambda \to 0} \frac{\lambda}{\lambda} = 1
      \]
      But if we approach it along the imaginary axis by setting $h = i\lambda$:
      \[
        \lim_{\lambda \to 0} \frac{f(z + i\lambda) - f(z)}{i\lambda} = \lim_{\lambda \to 0} \frac{-i\lambda}{\lambda} = -1
      \]
      So the limit from different directions disagrees and so does not exist.
    \item Consider $f(x) = \sin x$. This is differentiable at all points on $\R$:
      \begin{align*}
        f'(x) &= \lim_{h \to 0} \frac{\sin(x + h) - \sin(x)}{h} \\
              &= \lim_{h \to 0} \frac{\sin h\cos x}{h} + \lim_{h \to 0} \frac{\sin x (\cos h - 1)}{h} \\
              &= \cos x \cancelto{1}{\lim_{h \to 0} \frac{\sin h}{h}} + \sin x \cancelto{0}{\lim_{h \to 0} \frac{\cos h - 1}{h}} \\
              &= \cos x
      \end{align*}
  \end{enumerate}
\end{example}
\subsection{Rules for Differentiation}
We can derive some properties of derivatives from the properties of limits (\cref{limitLaws}).
\begin{lemma}[Differentiation Rules]
  Let $f, g: X \subseteq \C \to \C$ be differentiable at $a \in X$, then so are:
  \begin{enumerate}
    \item $f + g$ with $(f + g)' = f' + g'$
    \item $fg$ with $(fg)' = fg' + f'g$ (\textit{Product Rule})
    \item $\frac{1}{f}$ provided $f(z) \neq 0\ \forall z \in X$ and $\left(\frac{1}{f}\right)' = - \frac{f'}{f^2}$ (\textit{Reciprocal Rule})
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}
    \item Since we know that $f$ and $g$ are differentiable at $a$, we can split up the limit as follows:
      \begin{align*}
        (f + g)'(a) &= \lim_{h \to 0} \frac{1}{h} [f(a + h) + g(a + h) - f(a) - g(a)] \\
                    &= \lim_{h \to 0} \frac{1}{h} [f(a + h) - f(a)] + \lim_{h \to 0} \frac{1}{h} [g(a + h) - g(a)] \\
                    &= f'(a) + g'(a)
      \end{align*}
    \item
      To show that it is differentiable, we can show that the limit exists at $a$ by finding its value:
      \begin{align*}
        (fg)'(a) &= \lim_{h \to 0} \frac{1}{h}[f(a + h)g(a + h) - f(a)g(a)] \\
                 &= \lim_{h \to 0} \frac{1}{h}[(f(a + h) - f(a))g(a + h) + (g(a + h) - g(a))f(a))] \\
                 &= \lim_{h \to 0} \frac{1}{h}[(f(a + h) - f(a))g(a + h)] + f(a) \lim_{h \to 0} \frac{1}{h}[g(a + h) - g(a)] \\
                 &= \lim_{h \to 0} g(a + h) \lim_{h \to 0} \frac{1}{h}[f(a + h) - f(a)] + f(a)g'(a) \\
                 &= g(a) f'(a) + f(a)g'(a) \text{ since $g$ is continuous at $a$}
      \end{align*}
      We know that $g$ is continuous at $a$ since it is differentiable there, a proof of which will be given later in \cref{diffCont}.
    \item
      Proceeding similarly to above, we have:
      \begin{align*}
        \left(\frac{1}{f}\right)'(a) &= \lim_{h \to 0} \frac{1}{h} \left[\frac{1}{f(a + h)} - \frac{1}{f(a)}\right] \\
                                     &= \lim_{h \to 0} \frac{1}{h} \left[\frac{f(a) - f(a + h)}{f(a + h)f(a)}\right] \\
                                     &= -\lim_{h \to 0} \frac{1}{h} [f(a + h) - f(a)] \lim_{h \to 0} \frac{1}{f(a + h)f(a)} \\
                                     &= -\frac{f'(a)}{(f(a))^2} \text{since $f$ is continuous at $a$}
      \end{align*}
  \end{enumerate}
\end{proof}
\begin{example}
  \label{powerRule}
  \begin{enumerate}
    \item Using induction and the product rule, we can show that $f(z) = z^{n}$ is differentiable with $f'(z) = nz^{n - 1}$.
      Combining this with the addition of derivatives, this means that polynomials are always differentiable.
    \item $f(z) = \frac{1}{z}$ is differentiable on $\C \setminus \{0\}$ with $f'(z) = - \frac{1}{z^2}$ and, by induction, the derivative of $\frac{1}{z^{n}}$ is $-\frac{n}{z^{n + 1}}$.
      More generally, we see that rational functions $\frac{p(z)}{q(z)}$ where $p, q$ are polynomials are differentiable away from the zeros of $q(z)$.
  \end{enumerate}
\end{example}
We would like to know the derivative of $f \circ g$ but it is not possible to do this using limit definition so we would like to introduce an alternative characterisation of the derivative to make this easier:
\begin{lemma}
  Let $f: X \subseteq \C \to \C$.
  \label{derivativeAlternative}
  $f$ is differentiable at $a \in X$ if and only if there exists $A \in \C$ and function:
  \[
    \varepsilon: \{z: z + a \in X\} \subseteq \C \to \C
  \]
  satisfying $\varepsilon(h) \to 0$ as $h \to 0$ s.t.
  \[
    f(a + h) = f(a) + Ah + \varepsilon(h)|h|
  \]
\end{lemma}
\begin{remark}[Remarks]
  \begin{enumerate}
    \item This says that $f(a + h) \approx f(a) + Ah$ and the function $\varepsilon(h)$ quantifies the error we make in this approximation.
    \item We can also write $f(a + h) = f(a) + Ah + o(|h|)$ to mean the same thing using \textit{little-}$o$ notation from IA Differential Equations.
    \item In the proof we see that in both directions, $A = f'(a)$.
  \end{enumerate}
\end{remark}
\begin{proof}
  \begin{proofdirection}{Suppose $f$ is differentiable at $a$}
    Set $A = f'(a)$ so that:
    \[
      A = \lim_{h \to 0} \frac{f(a + h) - f(a)}{h} \implies \lim_{h \to 0} \frac{f(a + h) - f(a) - Ah}{h} = 0
    \]
    Then if we define:
    \[
      \varepsilon(h) = \begin{cases}
      \frac{f(a + h) - f(a) - Ah}{|h|} & \text{ if }h\neq0 \\
      0 & \text{ if } h = 0
      \end{cases}
    \]
    The value of $\varepsilon(0)$ is irrelevant as we are taking the limit but we choose it to be $0$ so that $\varepsilon$ is continuous.
    $\varepsilon(h) \to 0$ as $h \to 0$ and we see that $\varepsilon(h)$ satisfies:
    \[
      \varepsilon(h)|h| = f(a + h) - f(a) - Ah
    \]
    for all $h$.
  \end{proofdirection}
  \begin{proofdirection}{Suppose we have such an $\varepsilon$ and $A$}
    We can try and compute the derivative of $f$ at $A$:
    \begin{align*}
      \lim_{h \to 0} \frac{f(a + h) - f(a)}{h} &= \lim_{h \to 0} \frac{f(a) + Ah + \varepsilon(h)|h| - f(a)}{h} \\
                                               &= \lim_{h \to 0} \frac{Ah + \varepsilon(h)|h|}{h} \\
                                               &= A + \lim_{h \to 0} \left(\varepsilon(h) \frac{h}{|h|}\right) \\
                                               &= A \text{ since $\frac{h}{|h|}$ is bounded and $\varepsilon(h) \to 0$}
    \end{align*}
    We see that the limit exists and so $f$ is differentiable at $a$ with $f'(a) = A$.
  \end{proofdirection}
\end{proof}
\begin{proposition}[Chain Rule]
  Let $U, V \subseteq \C$ and $f: U \to V$ and $g: V \to \C$.
  If $f$ is differentiable at $a \in U$ and $g$ is differentiable at $f(a) \in V$, then $g \circ f: U \to \C$ is differentiable at $a \in U$ and $(g \circ f)'(a) = g'(f(a))f'(a)$.
\end{proposition}
\begin{proof}
  Using \cref{derivativeAlternative}, since $f$ is differentiable at $a$, $\exists \varepsilon_f(h)$ with $\varepsilon_f \to 0$ as $h \to 0$ such that:
  \[
    f(a + h) = f(a) + hf'(a) + \varepsilon_f(h)|h|
  \]
  and since $g$ is differentiable at $f(a)$, $\exists \varepsilon_g(k)$ with $\varepsilon_g \to 0$ as $k \to 0$ such that:
  \[
    g(f(a) + k) = g(f(a)) + kg'(f(a)) + \varepsilon_g(k)|k|
  \]
  We can now compute the difference between $(g \circ f)(a + h)$ and $(g \circ f)(a)$ and then use the reverse direction of \cref{derivativeAlternative} to find the derivative of $g \circ f$.
  \begin{align*}
    (g \circ f)(a + h) - (g \circ f)(a) &= g(f(a + h)) - g(f(a)) \\
                                        &= g(f(a) + \underbrace{hf'(a) + \varepsilon_f(h)|h|}_{k}) - g(f(a)) \\
                                        &= g(f(a)) + kg'(f(a)) + \varepsilon_g(k)|k| - g(f(a)) \\
                                        &= kg'(f(a)) + \varepsilon_g(k)|k| \\
                                        &= (hf'(a) + \varepsilon_f(h)|h|)g'(f(a)) + \varepsilon_g(k)|k| \\
                                        &= hf'(a)g'(f(a)) + \varepsilon_f(h)|h|g'(f(a)) + \varepsilon_g(k)|k|
  \end{align*}
  We see $hf'(a)g'(f(a))$ so our goal is to make the last two terms of this the error term, we can rewrite them as:
  \[
    \varepsilon_f(h)|h|g'(f(a)) + \varepsilon_g(hf'(a) + \varepsilon_f(h)|h|)|hf'(a) + \varepsilon_f(h)|h||
  \]
  and then define:
  \[
    \varepsilon(h) = \varepsilon_f(h)g'(f(a)) + \varepsilon_g(hf'(a) + \varepsilon_f(h)|h|)|f'(a) + \varepsilon_f(h)|
  \]
  and so $(g \circ f)(a + h) - (g \circ f)(a) = hf'(a)g'(f(a)) + \varepsilon(h)|h|$.

  We now just need to check that $\varepsilon(h) \to 0$ as $h \to 0$.

  Considering the first term of $\varepsilon(h)$:
  \[
    \lim_{h \to 0} [\varepsilon_f(h)g'(f(a))] = g'(f(a)) \lim_{h \to 0} \varepsilon_f(h) = 0
  \]
  and the second term is:
  \begin{align*}
    \lim_{h \to 0} \varepsilon_g(hf'(a) + \varepsilon_f(h)|h|)|f'(a) + \varepsilon_f(h)| &= \lim_{h \to 0} \varepsilon_g(hf'(a) + \varepsilon_f(h)|h|) \lim_{h \to 0} |f'(a) + \varepsilon_f(h)| \\
                                                                                         &= f'(a) \lim_{h \to 0} \varepsilon_g(hf'(a) + \varepsilon_f(h)|h|) \\
                                                                                         &= f'(a) \cdot 0 = 0
  \end{align*}
  So $\varepsilon(h) \to 0$ and so $(g \circ f)'(a)$ must be the coefficient of $h$ and so $(g \circ f)'(a) = g'(f(a))f'(a)$.
\end{proof}
\begin{example}
  Consider the function:
  \[
    f(x) = \begin{cases}
    x \sin\left(\frac{1}{x}\right) & x \neq 0 \\
    0 & x =0
    \end{cases}
  \]
  At $x \neq 0$, we can use the product and chain rule to find the derivative:
  \begin{align*}
    f'(x) &= 1 \cdot \sin\left(\frac{1}{x}\right) + x \cos \left(\frac{1}{x}\right) \cdot\left(-\frac{1}{x^2}\right) \\
          &= \sin\left(\frac{1}{x}\right) - \frac{1}{x} \cos\left(\frac{1}{x}\right)
  \end{align*}
  At $x = 0$, the function is not differentiable as the limit:
  \[
    \lim_{h \to 0} \frac{f(h) - f(0)}{h} = \lim_{h \to 0} \frac{f(h)}{h} = \lim_{h \to 0} \sin\left(\frac{1}{h}\right)
  \]
  does not exist.
\end{example}
There is also a relationship between the differentiability and continuity of a function.
\begin{lemma}
  If $f: X \subseteq \C \to \C$ is differentiable at $a \in X$, then it must also be continuous at $a$.
  \label{diffCont}
\end{lemma}
\begin{proof}
  Since $f$ is differentiable at $a$, by \cref{derivativeAlternative}, $\exists \varepsilon_f(h)$ such that $f(a + h) = f(a) + Ah + \varepsilon_f(h)$ and $\varepsilon_f(h) \to 0$ as $h \to 0$.

  We can then write:
  \begin{align*}
    \lim_{x \to a} f(x) &= \lim_{h \to 0} f(a + h) \\
                        &= \lim_{h \to 0} (f(a) + Ah + \varepsilon_f(h)|h|) \\
                        &= f(a) + A \lim_{h \to 0} h + \lim_{h \to 0} \varepsilon_f(h)|h| \\
                        &= f(a)
  \end{align*}
  and so $f$ is continuous at $a$ by \cref{continuityLimit}.
\end{proof}
\section{Mean Value Theorems}
We can think of derivatives as \textit{instantaneous rates of change} and in this section we will relate these to \textit{average rates of change} over an interval.
\begin{proposition}[Rolle's Theorem]
  If $f: [a, b] \to \R$ is continuous on $[a, b]$, differentiable on $(a, b)$ and satisfies $f(a) = f(b)$, then $\exists c \in (a, b)$ such that $f'(c) = 0$.
  \label{rollesTheorem}
\end{proposition}
\begin{remark}[Intuition]
  This tells us that such a function must have a \textit{stationary point} (where $f'(c) = 0$) at some $c \in (a, b)$.
\end{remark}
\begin{proof}
  Intuitively, we want to look for $c$ that are local maxima/minima we can show that these have $f'(c) = 0$.

  By the extreme value theorem (\cref{EVT}), $f$ attains its minimum and maximum on $[a, b]$, that is, $\exists x_{\text{min}}, x_{\text{max}} \in [a, b]$ such that:
  \[
    f(x_{\text{min}}) = \inf_{z \in [a, b]} f(z) \text{ and } f(x_{\text{max}}) = \sup_{z \in [a, b]} f(z)
  \]

  Let $h_n$ be a sequence that decreases to $0$ from above.
  If $x_{\text{min}} \in (a, b)$, then:
  \[
   \frac{f(x_{\text{min}} + h_n) - f(x_{\text{min}})}{h_n} \geq 0 \text{ and }
   \frac{f(x_{\text{min}} - h_n) - f(x_{\text{min}})}{-h_n} \leq 0
  \]
  since $f(x_{\text{min}})$ is a minimum.
  So as $h_n \to 0$, the derivative of $f$ at $x_{\text{min}}$ is trapped below by 0 and above by $0$ and so $f'(x_{\text{min}}) = 0$.

  Similarly, if $x_{\text{max}} \in (a, b)$, then:
  \[
   \frac{f(x_{\text{max}} + h_n) - f(x_{\text{max}})}{h_n} \leq 0 \text{ and }
   \frac{f(x_{\text{max}} - h_n) - f(x_{\text{max}})}{-h_n} \geq 0
  \]
  since $f(x_{\text{max}})$ is a maximum and so by the same argument, $f'(x_{\text{max}}) = 0$.

  So if either of $x_{\text{min}}$ or $x_{\text{max}}$ is in $(a, b)$, then we have found $c$ such that $f'(c) = 0$.

  If $f$ is constant, then $f'(c) = 0\ \forall c \in (a, b)$ and so we are done.
  Otherwise, either $f(x_{\text{max}}) > f(a)$, which means that $x_{\text{max}} \in (a, b)$, or $f(x_{\text{min}}) < f(a)$, which means that $x_{\text{min}} \in (a, b)$, in either case, we are done by the above.
\end{proof}
\begin{theorem}[Mean Value Theorem]
  If $f: [a, b] \to \R$ is continuous on $[a, b]$ and differentiable on $(a, b)$, then $\exists c \in (a, b)$ such that:
  \label{MVT}
  \[
    f'(c) = \frac{f(b) - f(a)}{b - a}
  \]
\end{theorem}
\begin{remark}[Intuition]
  We can think of $\frac{f(b) - f(a)}{b - a}$ as the average rate of change of $f$ over $[a, b]$ and so the mean value theorem tells us that at some point in $(a, b)$, the instantaneous rate of change (i.e. the derivative) must be equal to this average rate of change.
\end{remark}
\begin{proof}
  We would like to be able to use Rolle's Theorem to prove this and so we wish to construct a function $\phi$ satisfying $\phi(a) = \phi(b)$.

  A natural way to do this to use the line $\ell(x)$ that passes through $(a, f(a))$ and $(b, f(b))$:
  \[
    \ell(x) = f(a) + \frac{f(b) - f(a)}{b - a}(x - a)
  \]
  We then set $\phi(x) = f(x) - \ell(x)$ so that $\phi(a) = \phi(b) = 0$.
  Furthermore, $\ell$ and $f$ are continuous on $[a, b]$ so since addition preserves continuity (\cref{continuityLaws}), $\phi$ is also continuous on $[a, b]$.
  Thus, by Rolle's Theorem (\cref{rollesTheorem}), $\exists c \in (a, b)$ such that $\phi'(c) = 0$.

  Computing $\phi'(x)$ we have:
  \[
    \phi'(x) = f'(x) - \frac{f(b) - f(a)}{b - a}
  \]
  Therefore:
  \[
    \phi'(c) = 0  \implies f'(c) = \frac{f(b) - f(a)}{b - a}
  \]
  and so we are done.
\end{proof}
\begin{remark}
  We can alternatively think of this as given $h$ such that $a + h \in [a, b]$, $\exists \theta = \theta(h) \in (0, 1)$ such that $f(a + h) = f(a) + hf'(a + \theta h)$ by setting $b = a + h$ above.
\end{remark}
\begin{corollary}
  If $f: [a, b] \to \R$ is continuous on $[a, b]$ and differentiable on $(a, b)$, then:
  \label{derivativeIncreasingRelation}
  \begin{enumerate}
    \item $f' \geq 0$ on $(a, b) \implies f$ is increasing.
    \item $f' > 0$ on $(a, b) \implies f$ is strictly increasing.
    \item $f' \leq 0$ on $(a, b) \implies f$ is decreasing.
    \item $f' < 0$ on $(a, b) \implies f$ is strictly decreasing.
    \item $f' = 0$ on $(a, b) \implies f$ is constant.
  \end{enumerate}
\end{corollary}
\begin{proof}
  See Example Sheet 2 Q1e.
\end{proof}
\begin{remark}[Warning]
  We cannot just replace $[a, b]$ with any arbitrary $X \subseteq \R$.

  For instance, consider the function:
  \[
    f: \Q \to \R,\
    x \mapsto \begin{cases}
    0 & \text{ if } x^2 > 2 \\
    1 & \text{ if } x^2 < 2
    \end{cases}
  \]
  This is a continuous function and, since $\sqrt{2}$ is not in the domain of $f$, $f'(x) = 0\ \forall x \in \Q$.
  However, this function is not constant.
\end{remark}
We can generalise the relation between $f'(z) = 0$ and constant functions into $\C$:
\begin{corollary}
  If $f: \C \to \C$ is differentiable in $\C$ and $f'(z) = 0\ \forall z \in \C$, then $f$ is constant.
\end{corollary}
\begin{proof}
  Fix some arbitrary $z \in \C$ and define:
  \[
    g: [0, 1] \to \C, t \mapsto f(tz)
  \]
  Since $f$ is continuous and differentiable, $g$ is continuous on $[0, 1]$ and $g$ is differentiable on $(0, 1)$ since it is a composition of $f$ and $tz$.
  We can therefore apply the MVT to the real and imaginary parts of $g$ separately.

  If we differentiate $g$, we see that:
  \[
    g'(t) = z f'(tz) = 0 \implies \Re(g'(t)) = 0,\ \Im(g'(t)) = 0
  \]
  and so the real and imaginary parts of $g$ are constant, i.e. $g$ is constant $\forall t \in [0, 1]$.
  Setting $t = 1$ and then $t = 0$ yields $f(z) = g(1) = g(0) = f(0)$.
  Therefore, since $z$ was arbitrary, $f(z) = f(0)\ \forall z \in \C$.
\end{proof}
Now that we have introduced differentiation and some associated theorems, we can prove the full inverse function theorem that we saw a more basic version of in \cref{inverseFunctionMonotone}.
\begin{theorem}[Inverse Function Theorem -- Version 2]
  If $f: [a, b] \to \R$ is continuous and differentiable on $(a, b)$ and $f'(x) > 0\ \forall x \in (a, b)$, then $f: [a, b] \to [f(a), f(b)]$ is bijective and $f^{-1} : [f(a), f(b)] \to [a, b]$ is continuous and differentiable in $(f(a), f(b))$ with derivative:
  \[
    (f^{-1})'(y) = \frac{1}{f'(f^{-1}(y))}
  \]
\end{theorem}
\begin{proof}
  By \cref{derivativeIncreasingRelation}, since $f'(x) > 0\ \forall x \in (a, b)$, $f$ is strictly increasing and so our previous version of the inverse function theorem (\cref{inverseFunctionMonotone}) tells us that $f$ is a bijection to its image and $f^{-1}$ is continuous.

  We just need to show the differentiability of $f^{-1}$.
  Consider any arbitrary $y \in (f(a), f(b))$, since $f$ is a bijection there is a unique $x \in [a, b]$ such that $x = f^{-1}(y)$.

  Given arbitrary $h$ such that $y + h \in (f(a), f(b))$, define $k(h)$ such that $y + h = f(x + k)$.
  Doing this allows us to utilise the differentiability of $f$.

  Noting that $k = f^{-1}(y + h) - x$, we can attempt to find the derivative of $f^{-1}$:
  \[
    \frac{f^{-1}(y + h) - f^{-1}(y)}{h} = \frac{x + k - x}{f(x + k) - y} = \frac{k}{f(x + k) - f(x)} = \left[\frac{f(x + k) - f(x)}{k}\right]^{-1}
  \]
  Since we want to take the limit of the left hand side of this as $h \to 0$, we want to know what happens to $k(h)$ as $h \to 0$.
  \[
    \lim_{h \to 0} k(h) = \lim_{h \to 0} (f^{-1}(y + h) - f^{-1}(y)) = f^{-1}(y) - f^{-1}(y) = 0
  \]
  as $f^{-1}$ is continuous.

  Taking the limit of the derivative expression, we have:
  \[
    \lim_{h \to 0} \frac{f^{-1}(y + h) - f^{-1}(y)}{h} = \lim_{k \to 0}  \left[\frac{f(x + k) - f(x)}{k}\right]^{-1} = \frac{1}{f'(x)}
  \]
  The limit exists so $f^{-1}$ is differentiable with derivative:
  \[
    (f^{-1})'(y) = \frac{1}{f'(x)} = \frac{1}{f'(f^{-1}(y))}
  \]
  so we are done.
\end{proof}
\begin{example}
  \begin{enumerate}
    \item Fix some $R > 0$ and define $f: [0, R] \to \R$, $x \mapsto x^{N}$ for some $N \in \N$.
      $f$ is continuous on $[0, R]$ and differentiable on $(0, R)$ with $f'(x) = N x^{N - 1} > 0\ \forall x \in (0, R)$.
      Thus by the inverse function theorem, $f: [0, R] \to [0, R^{N}]$ is a bijection and $f^{-1}(y) = y^{\frac{1}{N}}$ is differentiable with derivative:
      \[
        (f^{-1})'(y) =  \frac{1}{f'(f^{-1}(y))} = \frac{1}{N(f^{-1}(y))^{N - 1}} = \frac{1}{N}y^{\frac{1}{N} - 1}
      \]
      Combining this with \cref{powerRule}, we have that for any $q \in \Q$:
      \[
        f(x) = x^{q} \implies f'(x) = qx^{q - 1}
      \]
    \item $f(x) = e^{x}$ is continuous and differentiable on $\R$ with $f'(x) =  e^{x} > 0\ \forall x \in \R$.
      So, using the inverse function theorem, there exists inverse $f^{-1}(y)$, which we will call $\log y$, such that:
      \[
        (f^{-1})'(y) = \frac{1}{e^{f^{-1}(y)}} = \frac{1}{e^{\log y}} = \frac{1}{y}
      \]
  \end{enumerate}
\end{example}
\begin{proposition}[Cauchy Mean Value Theorem]
  If $f, g: [a, b] \to \R$  are continuous and differentiable on $(a, b)$, then $\exists c \in (a, b)$ such that:
  \[
    g'(c)[f(b) - f(a)] = f'(c)[g(b) - g(a)]
  \]
\end{proposition}
\begin{remark}
  If we take $g(x) = x$, then we recover the standard MVT.

  Geometrically, this tells us that if we consider a line between two points on a parametric plot $(f(t), g(t))$, then there is some point between those two which has the same derivative as the gradient of the line.
\end{remark}
\begin{proof}
  We want to reduce to Rolle's Theorem (\cref{rollesTheorem}) as we did for the proof of MVT \cref{MVT}.

  Consider $\phi(x)$ defined by:
  \[
    \phi(x) = [g(x) - g(a)][f(b) - f(a)] - [g(b) - g(a)][f(x) - f(a)] \\
  \]
  Since $f, g$ are continuous and differentiable, so is $\phi$.
  Computing its derivative, we have:
  \[
    \phi'(x) = g'(x)[f(b) - f(a)] - f'(x)[g(b) - g(a)]
  \]
  We also see that $\phi(a) = \phi(b) = 0$, thus we can apply Rolle's theorem and so $\exists c \in (a, b)$ such that:
  \[
    \phi'(c) = 0 \implies g'(c)[f(b) - f(a)] - f'(c)[g(b) - g(a)] = 0
  \]
\end{proof}
\begin{proposition}[L' H\^opital's Rule]
  Let $-\infty \leq a < b \leq \infty$ and let $f, g: [a, b] \to \R$ be continuous and differentiable on $(a, b)$.
  Suppose $g(x) \neq 0\ \forall x \in (a, b)$ and $\lim_{x \to a} \frac{f'(x)}{g'(x)} = A$ for some $-\infty \leq A \leq \infty$ (where the cases of $\pm \infty$ are suitably interpreted).

  If either $f(x), g(x) \to 0$ as $x \to a$ or $f(x), g(x) \to \pm \infty$ as $x \to a$, then:
  \[
    \lim_{x \to a} \frac{f(x)}{g(x)} = A
  \]
\end{proposition}
\begin{proof}
  See Example Sheet 3.
\end{proof}
\begin{example}
  In \cref{sinxExample}, we saw that $\lim_{x \to 0} \frac{\sin x}{x} = 1$.
  Since $\sin x \to 0$ and $x \to 0$ as $x \to 0$, we can apply L' H\^opital's rule:
  \[
    \lim_{x \to 0} \frac{\sin x}{x} = \lim_{x \to 0} \frac{\cos x}{1} = 1
  \]
\end{example}
\end{document}
