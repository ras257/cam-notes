\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Differentiation}
\section{Basics}
\subsection{Definition of Differentiability}
What is the derivative of a function at a point?
\begin{definition}[Differentiable]
  Let $f: X \subseteq \C \to \C$ and let $a \in X$.
  We say that $f$ is \textit{differentiable} at $a$ if the limit:
  \[
    \lim_{x \to a} \frac{f(x) - f(a)}{x - a} = \lim_{h \to 0} \frac{f(a + h) - f(a)}{h}
  \]
  exists.

  The value of this limit is then called the \textit{derivative} of $f$ at $a$ and is denoted $f'(a)$ or $\deriv{f}{x}(a)$.
\end{definition}
\begin{remark}[Note]
  Here both $x \in X$ and $a + h \in X$, we cannot approach it from outside of the domain.
\end{remark}
\begin{remark}
  We can't make sense of the derivative at an isolated point of the domain.
  We \textit{could} define it to have a particular value but it would not lead to any interesting behaviour.
\end{remark}
For accumulation points, we can distinguish how we approach $a$:
\begin{itemize}
  \item For interior points, we can approach in any direction.
    If the limits from different direction disagree, then the limit does not exist.
  \item For non-interior points, the domain restricts how we can approach $a$.
\end{itemize}
We need to be careful about non-interior points however most of the theory for this course will build on the first case.
\begin{example}
  \begin{enumerate}
    \item Consider $f(z) = z$.
      This is differentiable at all points as:
      \[
        f'(z) = \lim_{h \to 0} \frac{f(z + h) - f(z)}{h} = \lim_{h \to 0} 1 =1
      \]
    \item Consider $f(z) = \overline{z}$.
      This is not differentiable at any point.

      If we approach it along the real line by setting $h = \lambda \in \R$:
      \[
        \lim_{\lambda \to 0} \frac{f(z + \lambda) - f(z)}{\lambda} = \lim_{\lambda \to 0} \frac{\lambda}{\lambda} = 1
      \]
      But if we approach it along the imaginary axis by setting $h = i\lambda$:
      \[
        \lim_{\lambda \to 0} \frac{f(z + i\lambda) - f(z)}{i\lambda} = \lim_{\lambda \to 0} \frac{-i\lambda}{\lambda} = -1
      \]
      So the limit from different directions disagrees and so does not exist.
    \item Consider $f(x) = \sin x$. This is differentiable at all points on $\R$:
      \begin{align*}
        f'(x) &= \lim_{h \to 0} \frac{\sin(x + h) - \sin(x)}{h} \\
              &= \lim_{h \to 0} \frac{\sin h\cos x}{h} + \lim_{h \to 0} \frac{\sin x (\cos h - 1)}{h} \\
              &= \cos x \cancelto{1}{\lim_{h \to 0} \frac{\sin h}{h}} + \sin x \cancelto{0}{\lim_{h \to 0} \frac{\cos h - 1}{h}} \\
              &= \cos x
      \end{align*}
  \end{enumerate}
\end{example}
\subsection{Rules for Differentiation}
We can derive some properties of derivatives from the properties of limits (\cref{limitLaws}).
\begin{lemma}[Differentiation Rules]
  Let $f, g: X \subseteq \C \to \C$ be differentiable at $a \in X$, then so are:
  \begin{enumerate}
    \item $f + g$ with $(f + g)' = f' + g'$
    \item $fg$ with $(fg)' = fg' + f'g$ (\textit{Product Rule})
    \item $\frac{1}{f}$ provided $f(z) \neq 0\ \forall z \in X$ and $\left(\frac{1}{f}\right)' = - \frac{f'}{f^2}$ (\textit{Reciprocal Rule})
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}
    \item Since we know that $f$ and $g$ are differentiable at $a$, we can split up the limit as follows:
      \begin{align*}
        (f + g)'(a) &= \lim_{h \to 0} \frac{1}{h} [f(a + h) + g(a + h) - f(a) - g(a)] \\
                    &= \lim_{h \to 0} \frac{1}{h} [f(a + h) - f(a)] + \lim_{h \to 0} \frac{1}{h} [g(a + h) - g(a)] \\
                    &= f'(a) + g'(a)
      \end{align*}
    \item
      To show that it is differentiable, we can show that the limit exists at $a$ by finding its value:
      \begin{align*}
        (fg)'(a) &= \lim_{h \to 0} \frac{1}{h}[f(a + h)g(a + h) - f(a)g(a)] \\
                 &= \lim_{h \to 0} \frac{1}{h}[(f(a + h) - f(a))g(a + h) + (g(a + h) - g(a))f(a))] \\
                 &= \lim_{h \to 0} \frac{1}{h}[(f(a + h) - f(a))g(a + h)] + f(a) \lim_{h \to 0} \frac{1}{h}[g(a + h) - g(a)] \\
                 &= \lim_{h \to 0} g(a + h) \lim_{h \to 0} \frac{1}{h}[f(a + h) - f(a)] + f(a)g'(a) \\
                 &= g(a) f'(a) + f(a)g'(a) \text{ since $g$ is continuous at $a$}
      \end{align*}
      We know that $g$ is continuous at $a$ since it is differentiable there, a proof of which will be given later in \cref{diffCont}.
    \item
      Proceeding similarly to above, we have:
      \begin{align*}
        \left(\frac{1}{f}\right)'(a) &= \lim_{h \to 0} \frac{1}{h} \left[\frac{1}{f(a + h)} - \frac{1}{f(a)}\right] \\
                                     &= \lim_{h \to 0} \frac{1}{h} \left[\frac{f(a) - f(a + h)}{f(a + h)f(a)}\right] \\
                                     &= -\lim_{h \to 0} \frac{1}{h} [f(a + h) - f(a)] \lim_{h \to 0} \frac{1}{f(a + h)f(a)} \\
                                     &= -\frac{f'(a)}{(f(a))^2} \text{since $f$ is continuous at $a$}
      \end{align*}
  \end{enumerate}
\end{proof}
\begin{example}
  \label{powerRule}
  \begin{enumerate}
    \item Using induction and the product rule, we can show that $f(z) = z^{n}$ is differentiable with $f'(z) = nz^{n - 1}$.
      Combining this with the addition of derivatives, this means that polynomials are always differentiable.
    \item $f(z) = \frac{1}{z}$ is differentiable on $\C \setminus \{0\}$ with $f'(z) = - \frac{1}{z^2}$ and, by induction, the derivative of $\frac{1}{z^{n}}$ is $-\frac{n}{z^{n + 1}}$.
      More generally, we see that rational functions $\frac{p(z)}{q(z)}$ where $p, q$ are polynomials are differentiable away from the zeros of $q(z)$.
  \end{enumerate}
\end{example}
We would like to know the derivative of $f \circ g$ but it is not possible to do this using limit definition so we would like to introduce an alternative characterisation of the derivative to make this easier:
\begin{lemma}
  Let $f: X \subseteq \C \to \C$.
  \label{derivativeAlternative}
  $f$ is differentiable at $a \in X$ if and only if there exists $A \in \C$ and function:
  \[
    \varepsilon: \{z: z + a \in X\} \subseteq \C \to \C
  \]
  satisfying $\varepsilon(h) \to 0$ as $h \to 0$ s.t.
  \[
    f(a + h) = f(a) + Ah + \varepsilon(h)|h|
  \]
\end{lemma}
\begin{remark}[Remarks]
  \begin{enumerate}
    \item This tells us that $f(a + h) \approx f(a) + Ah$ and the function $\varepsilon(h)$ quantifies the error we make in this approximation.
    \item We can also write $f(a + h) = f(a) + Ah + o(|h|)$ to mean the same thing using \textit{little-}$o$ notation from IA Differential Equations.
    \item In the proof we see that in both directions, $A = f'(a)$.
  \end{enumerate}
\end{remark}
\begin{proof}
  \begin{proofdirection}{Suppose $f$ is differentiable at $a$}
    Set $A = f'(a)$ so that:
    \[
      A = \lim_{h \to 0} \frac{f(a + h) - f(a)}{h} \implies \lim_{h \to 0} \frac{f(a + h) - f(a) - Ah}{h} = 0
    \]
    Then if we define:
    \[
      \varepsilon(h) = \begin{cases}
      \frac{f(a + h) - f(a) - Ah}{|h|} & \text{ if }h\neq0 \\
      0 & \text{ if } h = 0
      \end{cases}
    \]
    The value of $\varepsilon(0)$ is irrelevant as we are taking the limit but we choose it to be $0$ so that $\varepsilon$ is continuous.
    $\varepsilon(h) \to 0$ as $h \to 0$ and we see that $\varepsilon(h)$ satisfies:
    \[
      \varepsilon(h)|h| = f(a + h) - f(a) - Ah
    \]
    for all $h$.
  \end{proofdirection}
  \begin{proofdirection}{Suppose we have such an $\varepsilon$ and $A$}
    We can try and compute the derivative of $f$ at $A$:
    \begin{align*}
      \lim_{h \to 0} \frac{f(a + h) - f(a)}{h} &= \lim_{h \to 0} \frac{f(a) + Ah + \varepsilon(h)|h| - f(a)}{h} \\
                                               &= \lim_{h \to 0} \frac{Ah + \varepsilon(h)|h|}{h} \\
                                               &= A + \lim_{h \to 0} \left(\varepsilon(h) \frac{h}{|h|}\right) \\
                                               &= A \text{ since $\frac{h}{|h|}$ is bounded and $\varepsilon(h) \to 0$}
    \end{align*}
    We see that the limit exists and so $f$ is differentiable at $a$ with $f'(a) = A$.
  \end{proofdirection}
\end{proof}
\begin{proposition}[Chain Rule]
  Let $U, V \subseteq \C$ and $f: U \to V$ and $g: V \to \C$.
  If $f$ is differentiable at $a \in U$ and $g$ is differentiable at $f(a) \in V$, then $g \circ f: U \to \C$ is differentiable at $a \in U$ and $(g \circ f)'(a) = g'(f(a))f'(a)$.
\end{proposition}
\begin{proof}
  Using \cref{derivativeAlternative}, since $f$ is differentiable at $a$, $\exists \varepsilon_f(h)$ with $\varepsilon_f \to 0$ as $h \to 0$ such that:
  \[
    f(a + h) = f(a) + hf'(a) + \varepsilon_f(h)|h|
  \]
  and since $g$ is differentiable at $f(a)$, $\exists \varepsilon_g(k)$ with $\varepsilon_g \to 0$ as $k \to 0$ such that:
  \[
    g(f(a) + k) = g(f(a)) + kg'(f(a)) + \varepsilon_g(k)|k|
  \]
  We can now compute the difference between $(g \circ f)(a + h)$ and $(g \circ f)(a)$ and then use the reverse direction of \cref{derivativeAlternative} to find the derivative of $g \circ f$.
  \begin{align*}
    (g \circ f)(a + h) - (g \circ f)(a) &= g(f(a + h)) - g(f(a)) \\
                                        &= g(f(a) + \underbrace{hf'(a) + \varepsilon_f(h)|h|}_{k}) - g(f(a)) \\
                                        &= g(f(a)) + kg'(f(a)) + \varepsilon_g(k)|k| - g(f(a)) \\
                                        &= kg'(f(a)) + \varepsilon_g(k)|k| \\
                                        &= (hf'(a) + \varepsilon_f(h)|h|)g'(f(a)) + \varepsilon_g(k)|k| \\
                                        &= hf'(a)g'(f(a)) + \varepsilon_f(h)|h|g'(f(a)) + \varepsilon_g(k)|k|
  \end{align*}
  We see $hf'(a)g'(f(a))$ so our goal is to make the last two terms of this the error term, we can rewrite them as:
  \[
    \varepsilon_f(h)|h|g'(f(a)) + \varepsilon_g(hf'(a) + \varepsilon_f(h)|h|)|hf'(a) + \varepsilon_f(h)|h||
  \]
  and then define:
  \[
    \varepsilon(h) = \varepsilon_f(h)g'(f(a)) + \varepsilon_g(hf'(a) + \varepsilon_f(h)|h|)|f'(a) + \varepsilon_f(h)|
  \]
  and so $(g \circ f)(a + h) - (g \circ f)(a) = hf'(a)g'(f(a)) + \varepsilon(h)|h|$.

  We now just need to check that $\varepsilon(h) \to 0$ as $h \to 0$.

  Considering the first term of $\varepsilon(h)$:
  \[
    \lim_{h \to 0} [\varepsilon_f(h)g'(f(a))] = g'(f(a)) \lim_{h \to 0} \varepsilon_f(h) = 0
  \]
  and the second term is:
  \begin{align*}
    \lim_{h \to 0} \varepsilon_g(hf'(a) + \varepsilon_f(h)|h|)|f'(a) + \varepsilon_f(h)| &= \lim_{h \to 0} \varepsilon_g(hf'(a) + \varepsilon_f(h)|h|) \lim_{h \to 0} |f'(a) + \varepsilon_f(h)| \\
                                                                                         &= f'(a) \lim_{h \to 0} \varepsilon_g(hf'(a) + \varepsilon_f(h)|h|) \\
                                                                                         &= f'(a) \cdot 0 = 0
  \end{align*}
  So $\varepsilon(h) \to 0$ and so $(g \circ f)'(a)$ must be the coefficient of $h$ and so $(g \circ f)'(a) = g'(f(a))f'(a)$.
\end{proof}
\begin{example}
  Consider the function:
  \[
    f(x) = \begin{cases}
    x \sin\left(\frac{1}{x}\right) & x \neq 0 \\
    0 & x =0
    \end{cases}
  \]
  At $x \neq 0$, we can use the product and chain rule to find the derivative:
  \begin{align*}
    f'(x) &= 1 \cdot \sin\left(\frac{1}{x}\right) + x \cos \left(\frac{1}{x}\right) \cdot\left(-\frac{1}{x^2}\right) \\
          &= \sin\left(\frac{1}{x}\right) - \frac{1}{x} \cos\left(\frac{1}{x}\right)
  \end{align*}
  At $x = 0$, the function is not differentiable as the limit:
  \[
    \lim_{h \to 0} \frac{f(h) - f(0)}{h} = \lim_{h \to 0} \frac{f(h)}{h} = \lim_{h \to 0} \sin\left(\frac{1}{h}\right)
  \]
  does not exist.
\end{example}
There is also a relationship between the differentiability and continuity of a function.
\begin{lemma}
  If $f: X \subseteq \C \to \C$ is differentiable at $a \in X$, then it must also be continuous at $a$.
  \label{diffCont}
\end{lemma}
\begin{proof}
  Since $f$ is differentiable at $a$, by \cref{derivativeAlternative}, $\exists \varepsilon_f(h)$ such that $f(a + h) = f(a) + Ah + \varepsilon_f(h)$ and $\varepsilon_f(h) \to 0$ as $h \to 0$.

  We can then write:
  \begin{align*}
    \lim_{x \to a} f(x) &= \lim_{h \to 0} f(a + h) \\
                        &= \lim_{h \to 0} (f(a) + Ah + \varepsilon_f(h)|h|) \\
                        &= f(a) + A \lim_{h \to 0} h + \lim_{h \to 0} \varepsilon_f(h)|h| \\
                        &= f(a)
  \end{align*}
  and so $f$ is continuous at $a$ by \cref{continuityLimit}.
\end{proof}
\section{Mean Value Theorems}
We can think of derivatives as \textit{instantaneous rates of change} and in this section we will relate these to \textit{average rates of change} over an interval.
\begin{proposition}[Rolle's Theorem]
  If $f: [a, b] \to \R$ is continuous on $[a, b]$, differentiable on $(a, b)$ and satisfies $f(a) = f(b)$, then $\exists c \in (a, b)$ such that $f'(c) = 0$.
  \label{rollesTheorem}
\end{proposition}
\begin{remark}[Intuition]
  This tells us that such a function must have a \textit{stationary point} (where $f'(c) = 0$) at some $c \in (a, b)$.
\end{remark}
\begin{proof}
  Intuitively, we want to look for $c$ that are local maxima/minima we can show that these have $f'(c) = 0$.

  By the extreme value theorem (\cref{EVT}), $f$ attains its minimum and maximum on $[a, b]$, that is, $\exists x_{\text{min}}, x_{\text{max}} \in [a, b]$ such that:
  \[
    f(x_{\text{min}}) = \inf_{z \in [a, b]} f(z) \text{ and } f(x_{\text{max}}) = \sup_{z \in [a, b]} f(z)
  \]

  Let $h_n$ be a sequence that decreases to $0$ from above.
  If $x_{\text{min}} \in (a, b)$, then:
  \[
   \frac{f(x_{\text{min}} + h_n) - f(x_{\text{min}})}{h_n} \geq 0 \text{ and }
   \frac{f(x_{\text{min}} - h_n) - f(x_{\text{min}})}{-h_n} \leq 0
  \]
  since $f(x_{\text{min}})$ is a minimum.
  So as $h_n \to 0$, the derivative of $f$ at $x_{\text{min}}$ is trapped below by 0 and above by $0$ and so $f'(x_{\text{min}}) = 0$.

  Similarly, if $x_{\text{max}} \in (a, b)$, then:
  \[
   \frac{f(x_{\text{max}} + h_n) - f(x_{\text{max}})}{h_n} \leq 0 \text{ and }
   \frac{f(x_{\text{max}} - h_n) - f(x_{\text{max}})}{-h_n} \geq 0
  \]
  since $f(x_{\text{max}})$ is a maximum and so by the same argument, $f'(x_{\text{max}}) = 0$.

  So if either of $x_{\text{min}}$ or $x_{\text{max}}$ is in $(a, b)$, then we have found $c$ such that $f'(c) = 0$.

  If $f$ is constant, then $f'(c) = 0\ \forall c \in (a, b)$ and so we are done.
  Otherwise, either $f(x_{\text{max}}) > f(a)$, which means that $x_{\text{max}} \in (a, b)$, or $f(x_{\text{min}}) < f(a)$, which means that $x_{\text{min}} \in (a, b)$, in either case, we are done by the above.
\end{proof}
\begin{theorem}[Mean Value Theorem]
  If $f: [a, b] \to \R$ is continuous on $[a, b]$ and differentiable on $(a, b)$, then $\exists c \in (a, b)$ such that:
  \label{MVT}
  \[
    f'(c) = \frac{f(b) - f(a)}{b - a}
  \]
\end{theorem}
\begin{remark}[Intuition]
  We can think of $\frac{f(b) - f(a)}{b - a}$ as the average rate of change of $f$ over $[a, b]$ and so the mean value theorem tells us that at some point in $(a, b)$, the instantaneous rate of change (i.e. the derivative) must be equal to this average rate of change.
\end{remark}
\begin{proof}
  We would like to be able to use Rolle's Theorem to prove this and so we wish to construct a function $\phi$ satisfying $\phi(a) = \phi(b)$.

  A natural way to do this to use the line $\ell(x)$ that passes through $(a, f(a))$ and $(b, f(b))$:
  \[
    \ell(x) = f(a) + \frac{f(b) - f(a)}{b - a}(x - a)
  \]
  We then set $\phi(x) = f(x) - \ell(x)$ so that $\phi(a) = \phi(b) = 0$.
  Furthermore, $\ell$ and $f$ are continuous on $[a, b]$ so since addition preserves continuity (\cref{continuityLaws}), $\phi$ is also continuous on $[a, b]$.
  Thus, by Rolle's Theorem (\cref{rollesTheorem}), $\exists c \in (a, b)$ such that $\phi'(c) = 0$.

  Computing $\phi'(x)$ we have:
  \[
    \phi'(x) = f'(x) - \frac{f(b) - f(a)}{b - a}
  \]
  Therefore:
  \[
    \phi'(c) = 0  \implies f'(c) = \frac{f(b) - f(a)}{b - a}
  \]
  and so we are done.
\end{proof}
\begin{remark}
  We can alternatively think of this as given $h$ such that $a + h \in [a, b]$, $\exists \theta = \theta(h) \in (0, 1)$ such that $f(a + h) = f(a) + hf'(a + \theta h)$ by setting $b = a + h$ above.
\end{remark}
\begin{corollary}
  If $f: [a, b] \to \R$ is continuous on $[a, b]$ and differentiable on $(a, b)$, then:
  \label{derivativeIncreasingRelation}
  \begin{enumerate}
    \item $f' \geq 0$ on $(a, b) \implies f$ is increasing.
    \item $f' > 0$ on $(a, b) \implies f$ is strictly increasing.
    \item $f' \leq 0$ on $(a, b) \implies f$ is decreasing.
    \item $f' < 0$ on $(a, b) \implies f$ is strictly decreasing.
    \item $f' = 0$ on $(a, b) \implies f$ is constant.
  \end{enumerate}
\end{corollary}
\begin{proof}
  See Example Sheet 2 Q1e.
\end{proof}
\begin{remark}[Warning]
  We cannot just replace $[a, b]$ with any arbitrary $X \subseteq \R$.

  For instance, consider the function:
  \[
    f: \Q \to \R,\
    x \mapsto \begin{cases}
    0 & \text{ if } x^2 > 2 \\
    1 & \text{ if } x^2 < 2
    \end{cases}
  \]
  This is a continuous function and, since $\sqrt{2}$ is not in the domain of $f$, $f'(x) = 0\ \forall x \in \Q$.
  However, this function is not constant.
\end{remark}
We can generalise the relation between $f'(z) = 0$ and constant functions into $\C$:
\begin{corollary}
  If $f: \C \to \C$ is differentiable in $\C$ and $f'(z) = 0\ \forall z \in \C$, then $f$ is constant.
\end{corollary}
\begin{proof}
  Fix some arbitrary $z \in \C$ and define:
  \[
    g: [0, 1] \to \C, t \mapsto f(tz)
  \]
  Since $f$ is continuous and differentiable, $g$ is continuous on $[0, 1]$ and $g$ is differentiable on $(0, 1)$ since it is a composition of $f$ and $tz$.
  We can therefore apply the MVT to the real and imaginary parts of $g$ separately.

  If we differentiate $g$, we see that:
  \[
    g'(t) = z f'(tz) = 0 \implies \Re(g'(t)) = 0,\ \Im(g'(t)) = 0
  \]
  and so the real and imaginary parts of $g$ are constant, i.e. $g$ is constant $\forall t \in [0, 1]$.
  Setting $t = 1$ and then $t = 0$ yields $f(z) = g(1) = g(0) = f(0)$.
  Therefore, since $z$ was arbitrary, $f(z) = f(0)\ \forall z \in \C$.
\end{proof}
Now that we have introduced differentiation and some associated theorems, we can prove the full inverse function theorem that we saw a more basic version of in \cref{inverseFunctionMonotone}.
\begin{theorem}[Inverse Function Theorem -- Version 2]
  If $f: [a, b] \to \R$ is continuous and differentiable on $(a, b)$ and $f'(x) > 0\ \forall x \in (a, b)$, then $f: [a, b] \to [f(a), f(b)]$ is bijective and $f^{-1} : [f(a), f(b)] \to [a, b]$ is continuous and differentiable in $(f(a), f(b))$ with derivative:
  \[
    (f^{-1})'(y) = \frac{1}{f'(f^{-1}(y))}
  \]
\end{theorem}
\begin{proof}
  By \cref{derivativeIncreasingRelation}, since $f'(x) > 0\ \forall x \in (a, b)$, $f$ is strictly increasing and so our previous version of the inverse function theorem (\cref{inverseFunctionMonotone}) tells us that $f$ is a bijection to its image and $f^{-1}$ is continuous.

  We just need to show the differentiability of $f^{-1}$.
  Consider any arbitrary $y \in (f(a), f(b))$, since $f$ is a bijection there is a unique $x \in [a, b]$ such that $x = f^{-1}(y)$.

  Given arbitrary $h$ such that $y + h \in (f(a), f(b))$, define $k(h)$ such that $y + h = f(x + k)$.
  Doing this allows us to utilise the differentiability of $f$.

  Noting that $k = f^{-1}(y + h) - x$, we can attempt to find the derivative of $f^{-1}$:
  \[
    \frac{f^{-1}(y + h) - f^{-1}(y)}{h} = \frac{x + k - x}{f(x + k) - y} = \frac{k}{f(x + k) - f(x)} = \left[\frac{f(x + k) - f(x)}{k}\right]^{-1}
  \]
  Since we want to take the limit of the left hand side of this as $h \to 0$, we want to know what happens to $k(h)$ as $h \to 0$.
  \[
    \lim_{h \to 0} k(h) = \lim_{h \to 0} (f^{-1}(y + h) - f^{-1}(y)) = f^{-1}(y) - f^{-1}(y) = 0
  \]
  as $f^{-1}$ is continuous.

  Taking the limit of the derivative expression, we have:
  \[
    \lim_{h \to 0} \frac{f^{-1}(y + h) - f^{-1}(y)}{h} = \lim_{k \to 0}  \left[\frac{f(x + k) - f(x)}{k}\right]^{-1} = \frac{1}{f'(x)}
  \]
  The limit exists so $f^{-1}$ is differentiable with derivative:
  \[
    (f^{-1})'(y) = \frac{1}{f'(x)} = \frac{1}{f'(f^{-1}(y))}
  \]
  so we are done.
\end{proof}
\begin{remark}
  It is crucial that we have $f'(x) > 0\ \forall x \in [a, b]$ for the above.
\end{remark}
\begin{proposition}[Local Inverse Function Theorem]
  If $f: [a, b] \to \R$ is continuous and differentiable on $(a, b)$ and $\exists x_0 \in (a, b)$ such that $f'(x_0) \neq 0$, then in some region around $x_0$ there is an inverse function $f^{-1}$ that is differentiable.
\end{proposition}
\begin{proof}
  See Example Sheet 3.
\end{proof}
\begin{remark}
  This local inverse function theorem can be generalised into $\C$, we will see this in Analysis II.
\end{remark}
\begin{example}
  \begin{enumerate}
    \item Fix some $R > 0$ and define $f: [0, R] \to \R$, $x \mapsto x^{N}$ for some $N \in \N$.
      $f$ is continuous on $[0, R]$ and differentiable on $(0, R)$ with $f'(x) = N x^{N - 1} > 0\ \forall x \in (0, R)$.
      Thus by the inverse function theorem, $f: [0, R] \to [0, R^{N}]$ is a bijection and $f^{-1}(y) = y^{\frac{1}{N}}$ is differentiable with derivative:
      \[
        (f^{-1})'(y) =  \frac{1}{f'(f^{-1}(y))} = \frac{1}{N(f^{-1}(y))^{N - 1}} = \frac{1}{N}y^{\frac{1}{N} - 1}
      \]
      Combining this with \cref{powerRule}, we have that for any $q \in \Q$:
      \[
        f(x) = x^{q} \implies f'(x) = qx^{q - 1}
      \]
    \item $f(x) = e^{x}$ is continuous and differentiable on $\R$ with $f'(x) =  e^{x} > 0\ \forall x \in \R$.
      So, using the inverse function theorem, there exists inverse $f^{-1}(y)$, which we will call $\log y$, such that:
      \[
        (f^{-1})'(y) = \frac{1}{e^{f^{-1}(y)}} = \frac{1}{e^{\log y}} = \frac{1}{y}
      \]
  \end{enumerate}
\end{example}
\begin{proposition}[Cauchy Mean Value Theorem]
  If $f, g: [a, b] \to \R$  are continuous and differentiable on $(a, b)$, then $\exists c \in (a, b)$ such that:
  \[
    g'(c)[f(b) - f(a)] = f'(c)[g(b) - g(a)]
  \]
\end{proposition}
\begin{remark}
  If we take $g(x) = x$, then we recover the standard MVT.

  Geometrically, this tells us that if we consider a line between two points on a parametric plot $(f(t), g(t))$, then there is some point between those two which has the same derivative as the gradient of the line.
\end{remark}
\begin{proof}
  We want to reduce to Rolle's Theorem (\cref{rollesTheorem}) as we did for the proof of MVT \cref{MVT}.

  Consider $\phi(x)$ defined by:
  \[
    \phi(x) = [g(x) - g(a)][f(b) - f(a)] - [g(b) - g(a)][f(x) - f(a)] \\
  \]
  Since $f, g$ are continuous and differentiable, so is $\phi$.
  Computing its derivative, we have:
  \[
    \phi'(x) = g'(x)[f(b) - f(a)] - f'(x)[g(b) - g(a)]
  \]
  We also see that $\phi(a) = \phi(b) = 0$, thus we can apply Rolle's theorem and so $\exists c \in (a, b)$ such that:
  \[
    \phi'(c) = 0 \implies g'(c)[f(b) - f(a)] - f'(c)[g(b) - g(a)] = 0
  \]
\end{proof}
\begin{proposition}[L' H\^opital's Rule]
  Let $-\infty \leq a < b \leq \infty$ and let $f, g: [a, b] \to \R$ be continuous and differentiable on $(a, b)$.
  Suppose $g(x) \neq 0\ \forall x \in (a, b)$ and $\lim_{x \to a} \frac{f'(x)}{g'(x)} = A$ for some $-\infty \leq A \leq \infty$ (where the cases of $\pm \infty$ are suitably interpreted).

  If either $f(x), g(x) \to 0$ as $x \to a$ or $f(x), g(x) \to \pm \infty$ as $x \to a$, then:
  \[
    \lim_{x \to a} \frac{f(x)}{g(x)} = A
  \]
\end{proposition}
\begin{proof}
  See Example Sheet 3.
\end{proof}
\begin{example}
  In \cref{sinxExample}, we saw that $\lim_{x \to 0} \frac{\sin x}{x} = 1$.
  Since $\sin x \to 0$ and $x \to 0$ as $x \to 0$, we can apply L' H\^opital's rule:
  \[
    \lim_{x \to 0} \frac{\sin x}{x} = \lim_{x \to 0} \frac{\cos x}{1} = 1
  \]
\end{example}
\section{Higher Derivatives}
\begin{definition}[$n$-times differentiable]
  Suppose $f: X \subseteq \C \to \C$ is differentiable on $X$.
  We say that $f$ is \textit{twice differentiable} on $X$ if $f': X \to \R$ is differentiable on $X$ and denote the \textit{second derivative} as $f''$ or $f^{(2)}$.

  Iterating this, we obtain the notion of an \textit{$n$-times differentiable} function, whose \textit{$n$-th derivative} is denoted $f^{(n)}$.
\end{definition}
\begin{definition}[Continiously Differentiable]
  We say that $f$ is \textit{$n$-times} continuously differentiable and write $f \in C^{n}(X)$ if $f$ is $n$-times differentiable and $f^{(n)}: X \to \R$ is continuous.
\end{definition}
\begin{remark}
  Recall from \cref{diffCont} that if $f$ is differentiable at $a$, then it is also continuous at $a$.
  This means that if $f$ is $n$-times differentiable, then $f^{(j)}$ is continuous for all $j \in \{0, \ldots, k - 1\}$.
\end{remark}
\begin{definition}[Smooth]
  We say that $f: X \to \C$ is \textit{smooth} if $f$ is $n$-times differentiable for every $n \in \N$ and denote this $f \in C^{\infty}(X)$.
\end{definition}
We saw in \cref{derivativeAlternative} that if $f$ is differentiable at $a$, then it can be well approximated by a linear function near $a$:
\[
  f(x) \approx f(a) + f'(a)(x - a)
\]
If $f$ is twice differentiable, then we can do use a similar approximation for $f'$ and so:
\begin{align*}
  &f'(x) \approx f'(a) + f''(a)(x - a) \\
  \implies& f(x) \approx C + f'(a)(x - a) + \frac{1}{2} f''(a)(x - a)^2
\end{align*}
for some constant $C$, which by substituting $x = a$, we see $C = f(a)$ and so:
\[
  f(x) \approx f(a) + f'(a)(x - a) + \frac{1}{2} f''(a)(x - a)^2
\]
So we might be led to think that if $f$ is $n$ times differentiable then:
\[
  f(x) \approx \sum_{k = 0}^{n} \frac{f^{(k)}(a)}{k!}(x - a)^{k}
\]
We would like to know how good of an approximation we can make using this under appropriate conditions and quantify the error that we are making.
\begin{definition}[Taylor Polynomial and Series]
  For $f: X \to \C$ that are $n$-times differentiable and $x_0 \in X$, we define the \textit{degree $n$ Taylor polynomial for $f$ about $x_0$} as:
  \[
    T_{n, f, x_0}(x) = \sum_{k = 0}^{n} \frac{f^{(n)}(x_0)}{k!}(x - x_0)^{k}
  \]
  and, if $f$ is smooth, then the \textit{Taylor Series} of $f$ about $x_0$ is:
  \[
    \sum_{k = 0}^{\infty} \frac{f^{(n)}(x_0)}{k!}(x - x_0)^{k}
  \]
\end{definition}
\begin{definition}[Taylor Remainder]
  For $f: X \to \C$ that are $n$-times differentiable, $x_0 \in X$ and $h$ such that $x_0 + h \in X$, we define the \textit{Taylor remainder} $R_{n, f, x_0}(h)$ as:
  \begin{align*}
    R_{n, f, x_0}(h) &= f(x_0 + h) - T_{n - 1, f, x_0}(x_0 + h) \\
                     &= f(x_0 + h) - \sum_{k = 0}^{n - 1} \frac{f^{(k)}(x_0)}{k!}h^{k}
  \end{align*}
\end{definition}
\begin{theorem}[Taylor's Theorem -- Lagrange Remainder]
  If $f: [a, a + h]$ is continuous and $n$ times differentiable on $(a, a + h)$ with its first $n - 1$ derivatives also continuous, then $\exists \theta \in (0, 1)$ such that:
  \label{lagrangeRemainder}
  \[
    R_{n, f, a}(h) = \frac{h^{n}}{n!}f^{(n)}(a + \theta h)
  \]
  or equivalently:
  \[
    f(a + h) = \sum_{k = 0}^{n - 1} \frac{f^{(k)}(a)}{k!}h^{k} + \frac{h^{n}}{n!}f^{(n)}(a + \theta h)
  \]
\end{theorem}
\begin{remark}[Remarks]
  \begin{itemize}
    \item All of the requirements above are satisfied if $f \in C^{n}((c, d))$ and $[a, a + h] \subset (c, d)$.
      This is usually true when we are using Taylor's theorem and the above statement requires as little as possible about the function.
    \item For $n = 1$ this reduces to the mean value theorem (\cref{MVT}).
    \item We do not need $h > 0$, as if $h = -t < 0$ and $f$ has $n - 1$ derivatives that are continuous on $[a - t, a]$ and is $n$ times differentiable on $(a - t, a)$, then we can apply the above to $g(x) = f(-x)$:
      \begin{align*}
        f(a + h) = g(-a + t) &= \sum_{n=0}^{n - 1} \frac{g^{(k)}(-a)}{k!} t^{k} + \frac{t^{n}}{n!}g^{(n)}(-a + \theta t) \\
                             &= \sum_{n=0}^{n - 1} \frac{(-1)^{k}f^{(k)}(a)}{k!} t^{k} + \frac{(-1)^{n}t^{n}}{n!}f^{(n)}(a - \theta t) \\
                             &= \sum_{n=0}^{n - 1} \frac{f^{(k)}(a)}{k!} (-t)^{k} + \frac{(-t)^{n}}{n!}f^{(n)}(a - \theta t) \\
                             &= \sum_{n=0}^{n - 1} \frac{f^{(k)}(a)}{k!} (h)^{k} + \frac{(-t)^{n}}{n!}f^{(n)}(a + \theta h)
      \end{align*}
    \item If $f \in C^{n}((c, d))$, with $[a, a + h] \subset (c, d)$, then $f^{(n)}$ is continuous and hence bounded on $[a, a + h]$ and so $\exists M_n$ s.t. $M_n = \sup\limits_{x \in [a, a + h]} |f^{(n)}(x)|$.

      The taylor remainder then satisfies:
      \[
        |R_{n, f, a}(h)| \leq M_n \cdot \frac{h^{n}}{n!}
      \]
      So $R_{n, f, a}(n) = O(h^{n})$ as $h \to 0$.
      Note that this does not tell us that $R_{n, f, a}(h) \to 0$ as $n \to \infty$, even if $f \in C^{\infty}$ as we do not know how $M_n$ behaves with $n$.
  \end{itemize}
\end{remark}
\begin{proof}[1 -- Similarly to MVT (\cref{MVT})]\par
  WLOG take $a = 0$, as if $a \neq 0$, then we can just apply the theorem to $g(x) = f(x + a)$.

  Similarly to the proof of the MVT, we would like to construct a function that we can apply Rolle's Theorem (\cref{rollesTheorem}) and so we define:
  \[
    \phi: [0, h] \to \R,\ t \mapsto f(t) - T_{n - 1, f, 0}(t) - \frac{t^{n}}{n!} B
  \]
  We see that $\phi(0) = 0$ and we want some $\theta h \in (0, h)$ so pick $B$ to be such that $\phi(h) = 0$ so that when we apply Rolle's Theorem, we find some $\theta h \in (0, h)$.

  We know that $\phi$ and its first $n - 1$ derivatives are continuous because they are for $f$.
  So by Rolle's Theorem:
  \[
    \phi(0) = \phi(h) = 0 \implies \exists \theta_1 \in (0, 1) \text{ s.t. } \phi'(\theta_1 h) = 0
  \]
  To apply Rolle's theorem again on $\phi$, we already known that $\phi'(\theta_1 h) = 0$ so we need another point where $\phi' = 0$.
  By differentiating $\phi$, we see that $\phi^{(k)}(0) = 0\ \forall k \in \{0, \ldots, n - 1\}$.
  So applying Rolle's theorem again:
  \[
    \phi'(0) = \phi'(\theta_1h) = 0 \implies \exists \theta_2 \in (0, 1) \text{ s.t. } \phi''(\theta_2 \theta_1h) = 0
  \]
  Since we know that $\phi^{(k)}(0) = 0\ \forall k \in \{0, \ldots, n - 1\}$ and that $\phi$ is $n$ times differentiable on $(0, h)$,  we can repeat this $n$ times total and so:
  \[
    \phi^{(n - 1)}(0) = \phi^{(n - 1)}(\theta_{n-1} \cdots \theta_1 h) = 0 \implies \exists \theta_n \in (0, 1) \text{ s.t. } \phi^{(n)}(\theta_n \cdots \theta_1 h) = 0
  \]
  We now define $\theta = \theta_{n} \cdots \theta_1 \in (0, 1)$ and so we have shown that:
  \[
    \exists \theta \in (0, 1) \text{ s.t. } \phi^{(n)}(\theta h) = 0
  \]
  The $n$-th derivative of $\phi$ is:
  \[
    \phi^{(n)}(t) = f^{(n)}(t) - B \implies B = f^{(n)}(\theta h)
  \]
  and since we constructed $B$ such that $\phi(h) = 0$, we see that:
  \[
    f(h) = T_{n - 1, f, 0}(h) + \frac{h^{n}}{n!} f^{(n)}(\theta h)
  \]
  which is the desired result.
\end{proof}
\begin{proof}[2 -- Using a clever function]
  Again, WLOG, take $a = 0$.
  We want to reduce to Rolle's Theorem again, but there is a cleverer choice for our function.

  First, define:
  \[
    g: [0, h] \to \R,\ t \mapsto f(h) - \sum_{k = 0}^{n - 1} \frac{f^{(k)}(t)}{k!}(h - t)^{k}
  \]
  and note that:
  \[
    g(0) = f(h) - T_{n - 1, f, 0} (h)
  \]
  Here, $g$ is differentiable only once on $(0, h)$ since $g$ includes $f^{(n - 1)}$ and $f$ is only $n$ times differentiable on $(0, h)$.
  The derivative of $g$ is:
  \begin{align*}
    g'(t) &= -\sum_{n=0}^{n - 1} \left(\frac{f^{(k + 1)}(t)}{k!}(h - t)^{k} - \frac{f^{(k)}(t)}{k!}k(h - t)^{k}\right) \\
          &= \sum_{k = 1}^{n - 1} \frac{f^{(k)}(t)}{(k - 1)!}(h - t)^{k - 1} - \sum_{n=0}^{n - 1} \frac{f^{(k + 1)}(t)}{k!}(h - t)^{k} \\
          &= \sum_{k = 0}^{n - 2} \frac{f^{(k + 1)}(t)}{k!}(h - t)^{k} - \sum_{n=0}^{n - 1} \frac{f^{(k + 1)}(t)}{k!}(h - t)^{k} \\
          &= -\frac{f^{(n)}(t)}{(n - 1)!}(h - t)^{n - 1}
  \end{align*}
  Now, we need to construct a function to apply Rolle's theorem (\cref{rollesTheorem}) on, so, for $p = 1, \ldots, n$ define:
  \[
    \phi_p(t) = g(t) - \frac{(h - t)^{p}}{h^{p}}g(0)
  \]
  then since $\phi_p$ is continuous on $[0, h]$ and differentiable on $(0, h)$:
  \[
    \phi_p(h) = \phi_p(0) = 0 \implies \exists \theta \in (0, 1) \text{ s.t. } \phi_p'(\theta h) = 0
  \]
  but $\phi_p'(\theta h)$ is:
  \begin{align*}
    \phi'_p(\theta h) &= g'(\theta h) + \frac{p(h - \theta h)^{p - 1}}{h^{p}}g(0)\\
                      &= g'(\theta h) + \frac{p(1 - \theta)^{p - 1}}{h}g(0) = 0
  \end{align*}
  So using our expression for $g'(t)$:
  \begin{align*}
    &-\frac{h^{n - 1}(1 - \theta)^{n - 1}}{(n - 1)!} f^{(n)}(\theta h) + \frac{p(1 - \theta)^{p - 1}}{h}g(0) = 0 \\
    &\implies g(0) = \frac{h^{n}(1 - \theta)^{n - p}}{p(n - 1)!}f^{(n)}(\theta h)
  \end{align*}
  Finally, since $g(0) = f(h) - T_{n - 1, f, 0}(h)$, we see that:
  \[
    f(h) = T_{n - 1, f, 0}(h) \frac{h^{n}}{p(n - 1)!}(1 - \theta)^{n - p}f^{(n)}(\theta h)
  \]
  so taking $p = n$ yields the desired result.
\end{proof}
\begin{theorem}[Taylors Theorem -- Cauchy Remainder]
  Suppose we have $f$ satisfying the requirements from \cref{lagrangeRemainder}, then $\exists \theta \in (0, 1)$ such that:
  \[
    R_{n, f, a}(h) =  \frac{h^{n - 1}}{(n - 1)!} (1 - \theta)^{n - 1}f^{(n)}(a + \theta h)
  \]
  or equivalently:
  \[
    f(a + h) = \sum_{k = 0}^{n - 1} \frac{f^{(k)}(a)}{k!}h^{k} + \frac{h^{n - 1}}{(n - 1)!} (1 - \theta)^{n - 1}f^{(n)}(a + \theta h)
  \]
\end{theorem}
\begin{proof}
  Covered next lecture.
\end{proof}
\end{document}
