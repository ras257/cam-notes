\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Basic Calculus}
\section{Introduction}
Differential equations appear in almost all branches of science and applied mathematics.
\begin{example}[Newton's Second Law]
  \[
    \underbrace{m}_{\text{Mass}} \ddot{x} = \underbrace{F(x, t)}_{\text{Force}}
  \]
  This equation relates the rate of change of position $x$, the \textit{dependant variable}, with time $t$, the \textit{independent variable}.
\end{example}
\section{Differentiation}
\begin{definition}[Derivative]
  The \textit{derivative} of a function $f(x)$ with respect to its argument $x$ is defined as:
  \[
    \deriv{f}{x} = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
  \]
\end{definition}
\begin{remark}[Notation]
  We sometimes also use the notation $f'(x)$ or $\dot{f}(x)$ (The dot notation is usually used when $f$ is a function  of time).
\end{remark}
The derivative of a function allows us to determine the gradient of a function at a particular point.
% Maybe add plot?
\begin{definition}[Informal Definition of a Limit]
  Informally, if $\lim_{x \to x_0} f(x) = A$ then $f(x)$ can be made arbitrarily close to $A$ by making $x$ sufficiently close to $x_0$.
  (Note that we don't require that $f(x_0) = A$).
  This is explored further in Analysis I.
\end{definition}
\begin{remark}[Note]
  For the derivative to exist we require both the left and right-handed limits to exist and be equal.
  \[
    \lim_{h \to 0^-} \text{ and } \lim_{h \to 0^+} \text{ exist and are equal}
  \]
\end{remark}
\begin{example}
  Consider $f(x) = |x|$ so
  \[
    \at{\deriv{f}{x}}{0} = \lim_{h \to 0} \frac{|h|}{h}
  \]
  But note that the left and right-handed limits are different ($-1$ and $1$) so $f$ is \textbf{not} differentiable at $x = 0$ but is everywhere else.
\end{example}
We can define higher order derivatives for sufficiently smooth functions:
\[
  \deriv{}{x}\left(\deriv{f}{x}\right) = \deriv[2]{f}{x} = f''(x) = \ddot{f}(x)
\]
\begin{remark}[Notation]
  For the $n$th derivative we use the notation:
  \[
    \deriv[n]{f}{x} = f^{(n)}(x) 
  \]
\end{remark}
\subsection{Order Parameters}
Order parameters allow us to compare the behaviour of functions close to a limiting point, usually denoted $x_0$.
\begin{definition}[Big-$O$ Notation]
  \begin{proofcases}
    \begin{case}{$x_0$ finite}
      $f(x)$ is $O(g(x))$ as $x \to x_0$ if $\exists \delta > 0 \text{ and } M > 0$ such that $\forall x$ with $0 < |x - x_0| < \delta$, then $|f(x)| \leq M |g(x)|$.
    \end{case}
    \begin{case}{$x_0$ infinite}
      $f(x)$ is $O(g(x))$ as $x \to \infty$ if $\exists x_1 \text{ and } M > 0 \text{ such that } \forall x > x_1$, then $|f(x)| \leq M|g(x)|$.
    \end{case}
  \end{proofcases}
\end{definition}
From the definition it follows that if $f(x)$ is $O(g(x))$ then $f(x)/g(x)$ is bounded by $M$ as $x \to x_0$.
\begin{remark}[Notation]
This is often written $f(x) = O(g(x))$ although note that the use of the equals sign is an abuse of notation.
This is because $O(g(x))$ is a \textit{class} of functions but in practice using equals is simply more convenient.
\end{remark}
\begin{example}
  As $x \to 0$, $x^2 = O(x)$ but $x \neq O(x^2)$.
\end{example}
% Plot with x^2 x sqrt(x)
\begin{example}
  $\sin 2x = O(x)$ as $x \to 0$ since $|\sin 2x| \leq 2|x|\;\forall x$.
\end{example}
\begin{example}
  $f(x) = 2x^3 - 4x + 12 = O(x^3)$ as $x \to \infty$ since $\forall x > 1$
  \begin{align*}
    |2x^3 - 4x + 12| &\leq 2|x^3| + |-4x| + 12 \\
                     &\leq 6|x^3| + 4|x^3| + 12|x^3| \\
                     &\leq 22|x^3|
  \end{align*}
  So the definition is satisfied with $x_0 = 1 \text{ and } M = 22$.
\end{example}
\begin{definition}[Little-$o$ Notation]
  $f(x) = o(g(x))$ as $x \to x_0$ if \textbf{for every} $\varepsilon > 0, \exists \delta > 0 \text{ such that } \forall x \text{ with } 0 < |x - x_0| < \delta$ ($x$ sufficiently close to $x_0$) then $|f(x)| \leq \varepsilon |g(x)|$.

  If $g \neq 0$ is in the vicinity of $x_0$ but not necessarily at $x_0$ then an equivalent statement is that:
  \[
    \lim_{x \to x_0} \frac{f(x)}{g(x)} = 0
  \]
\end{definition}
\begin{remark}[Notation]
  Often written as $f(x) = o(g(x))$ or $\underline{o}(g(x))$ when handwritten to differentiate it from big O.
\end{remark}

$f(x) = o(g(x))$ loosely means that $f(x)$ is \textbf{much smaller} than $g(x)$ as $x \to x_0$. 
\begin{example}
 $x^2 = o(x)$ as $x \to 0$ as $\lim_{x \to 0} x^2/x = 0$
\end{example}
\begin{example}
  $\sqrt{x} = o(x)$ as $x \to \infty$ as $\lim_{x \to \infty} \sqrt{x}/x = 0$
\end{example}
\begin{remark}[Notes]
  \begin{itemize}
    \item $f(x) = o(g(x))$ is a stronger statement that $f(x) = O(g(x))$. 
      This is because little-o means that we are bounded by any multiple whereas big-O means were are bounded by a given multiple.

      So $f(x) = o(g(x)) \implies f(x) = O(g(x))$ but $f(x) = O(g(x)) \centernot\implies f(x) = o(g(x))$

      E.g. $f(x) = 2x = O(x)$ as $x \to 0$ but $f(x) \neq o(x)$ because $\lim_{x \to 0} 2x/x = 2$.
    \item Constants don't matter. If $f(x) = O(g(x))$ then $af(x) = O(g(x))$ and $f(x) = O(ag(x))$ for any non-zero constant $a$.
  \end{itemize} 
\end{remark}
Order parameters are useful to classify remainder terms before taking limits.
\begin{proposition}
  \[
    f(x_0 + h) = f(x_0) + hf'(x_0) + o(h) \text{ as } h\to0
    \label{derivLittle}
  \]
\end{proposition}
\begin{proof}
  Suppose that we have a remainder/error term $\epsilon(h)$ before taking the limit in a derivative:
  \begin{align*}
    f(x_0 + h) - f(x_0) &= hf'(x_0) + \epsilon(h) \\
    \lim_{h \to 0} \left[\frac{f(x_0 + h) - f(x_0)}{h}\right] &= f'(x_0) + \lim_{h \to 0} \frac{\epsilon(h)}{h}
  \end{align*}
  From the definition of $f'(x)$ the final limit term vanishes so we have $\lim_{h \to 0} \epsilon(h)/h = 0$ and so $\epsilon(h) = o(h)$ as $h\to0$.
  This means that the error term is of order $o(h)$ so we can write:
  \[
    f(x_0 + h) = f(x_0) + hf'(x_0) + o(h) \text{ as } h\to0
  \]
\end{proof}
\subsection{Rules for Differentiation}
We can introduce some rules of differentiation to make it easier to differentiate composite functions.
\begin{theorem}[Chain Rule]
 If $f(x) = F(g(x))$, then:
 \[
   \deriv{f}{x} = F'(g(x))\deriv{g}{x} = \at{\deriv{F}{g}}{g(x)} \at{\deriv{g}{x}}{x}
 \]
\end{theorem}
\begin{proof}
  \begin{align*}
    \deriv{f}{x} &= \lim_{h \to 0} \frac{1}{h} [F(g(x + h)) - F(g(x))] \\
                 &= \lim_{h \to 0} \frac{1}{h} [F(g(x) + hg'(x) + o(h)) - F(g(x))] \\
                 &= \lim_{h \to 0} \frac{1}{h} [\cancel{F(g(x))} + (hg'(x) + o(h))F'(g(x)) + o(hg'(x) + o(h)) - \cancel{F(g(x))}] \\
                 &= \lim_{h \to 0} \frac{1}{h} [hF'(g(x))g'(x) + o(h)F'(g(x)) + o(hg'(x) + o(h))]
  \end{align*}
  For a given $x$, $F'(g(x))$ is constant so $o(h)F'(g(x)) = o(h)$ and similarly $o(hg'(x) + o(h)) = o(h)$. So:
  \begin{align*}
    \deriv{f}{x} &= F'(g(x))g'(x) + \lim_{h \to 0} \frac{o(h)}{h} \\
                 &= F'(g(x))g'(x)
  \end{align*}
\end{proof}
\begin{example}
  $\deriv{}{x} \sin(x^2 - x + 2) = \cos(x^2 - x + 2) \cdot (2x - 1)$
\end{example}
\begin{theorem}[Product Rule]
  If $f(x) = u(x)v(x)$, then:
  \[
    \deriv{f}{x} = v \deriv{u}{x} + u \deriv{v}{x}
\]
\end{theorem}
If we replace $v \mapsto v^{-1}$ and also apply the chain rule then we get the quotient rule:
\[
  \deriv{}{x}\left(\frac{u}{v}\right) = \frac{vu' - uv'}{v^2}
\]
We can also generalise the product rule to the $n$-th derivative using Leibniz's rule:
\begin{theorem}[Leibniz's Rule]
  If $f(x) = u(x)v(x)$, then:
  \[
    \deriv[n]{f}{x} = \sum_{r=0}^{n} \binom{n}{r}\deriv[r]{u}{x}\deriv[n-r]{v}{x}
  \]
\end{theorem}
\begin{proof}
  By induction
\end{proof}
\section{Taylor Series}
\begin{definition}[Taylor Series]
 For $f(x)$ infinitely differentiable at $x = x_0$, the \textit{Taylor series} of the function about $x_0$ is:
 \begin{align*}
   T_f(x) &= f(x_0) + (x - x_0)f'(x_0) + \frac{1}{2!}(x - x_0)^2 f''(x_0) + \cdots \\
          &= \sum_{n=0}^{\infty} \frac{1}{n!} (x-x_0)^{n} f^{(n)}(x_0)
 \end{align*}
\end{definition}
\begin{definition}[Taylor Polynomial]
  For $f(x)$ $n$-times differentiable at $x = x_0$, the \textit{taylor polynomial} of degree $n$ of the function about $x_0$ is:
  \begin{align*}
    P_n(x) &= f(x_0) + (x - x_0)f'(x_0) + \cdots + \frac{1}{n!} (x - x_0)^{n} f^{(n)}(x_0) \\
           &= \sum_{r=0}^{n} \frac{1}{r!} (x-x_0)^{r} f^{(r)}(x_0)
  \end{align*}
\end{definition}
To figure out how well $P_n(x)$ approximates $f(x)$, we can extend \cref{derivLittle} to Taylor's Theorem.
\begin{theorem}[Taylor's Theorem]
  For $f(x)$ $n$ times differentiable at $x_0$ we have:
  \[
    f(x_0 + h) = f(x_0) + hf'(x_0) + \frac{h^2}{2!}f''(x_0) + \cdots + \frac{h^{n}}{n!} f^{(n)} (x_0) + E_n
  \]
  Where $E_n$ is an ``error/remainder'' and $E_n = o(h^{n})$ as $h \to 0$.

  If $f^{(n+1)}(x)$ exists and is continuous $\forall x \in (x_0, x_0 + h)$ then $E_n = O(h^{n+1})$ as $h \to 0$.
  \label{taylorsThm}
\end{theorem}
\begin{remark}
  $E_n = O(h^{n + 1})$ is  a stronger statement than $o(h^{n})$.

  E.g. $h^{n + \frac{1}{2}}$ is $o(h^{n})$ but it is not $O(h^{n + 1})$.
\end{remark}
We can set $x = x_0 + h$ which allows us to rewrite \cref{taylorsThm}:
\begin{align*}
  f(x) &= f(x_0) + (x - x_0)f'(x_0) + \frac{1}{2!}(x - x_0)^2f''(x_0) + \cdots + \frac{1}{n!}(x - x_0)^{n} f^{(n)}(x_0) + E_n \\
       &= P_n(x) + E_n
\end{align*}
This means that $P_n(x)$ provides a \textbf{local approximation} to $f(x)$ in the vicinity of $x_0$ with error $o(h^{n})$ or $O(h^{n + 1})$ depending on the conditions.

If $\lim_{n \to \infty} E_n = 0$ then the Taylor series $T_f(x)$ converges to $f(x)$.

\begin{theorem}[Mean-value form of the remainder]
If $f^{(n+1)}(x)$ exists and is continuous $\forall x \in (x_0, x_0 + h)$ then:
\[
  E_n = f^{(n+1)}(x_n) \frac{h^{n+1}}{(n+1)!} = f^{(n + 1)}(x_n)\frac{(x - x_0)^{n + 1}}{(n + 1)!}
\]
for some $x_n \in (x_0, x_0 + h)$.
\end{theorem}
\begin{proof}
  See Analysis I
\end{proof}

This can be used to establish a bound on the error of a taylor polynomial approximation when $x = x_0 + h$.
\begin{example}
  Let $f(x)$ be the $n$-th degree Taylor polynomial of $\exp(x)$ about $x_0 = 0$.
  This is differentiable infinitely many times so we can use the stronger case:
  \[
    E_n = \frac{h^{n + 1}}{(n + 1)!}\exp(x_n)\quad 0 \leq x_n \leq h
  \]
  So the fractional error of the approximation when $x = 0 + h$ is:
  \begin{align*}
    \text{fractional error} &= \frac{E_n}{\exp(h)} = \frac{h^{n+1}}{(n+1)!}\underbrace{\exp(x_n - h)}_{0 < \cdot \leq 1} \\
                            &\leq \frac{h^{n + 1}}{(n + 1)!}
  \end{align*}
  This specifies how large $n$ must be if we want a given target accuracy at $x = x_0 + h$.
\end{example}
\section{L' H\^opital's Rule}
L'H\^opital's rule allows us to deal with limits of indeterminate forms.
\begin{theorem}[L'H\^opital's Rule]
  Let $f(x)$ and $g(x)$ be differentiable at $x_0$, and:
  \begin{align*}
    \lim_{x \to x_0} f(x) &= f(x_0) = 0 \\
    \lim_{x \to x_0} g(x) &= g(x_0) = 0
  \end{align*}
  Then if $g'(x_0) \neq 0$ we have that:
  \[
    \lim_{x \to x_0} \frac{f(x)}{g(x)} = \lim_{x \to x_0} \frac{f'(x)}{g'(x)}
  \]
  provided the limit on the RHS exists.
\end{theorem}
\begin{proof}[of a specific case]
  From \cref{taylorsThm}, as $x \to x_0$:
  \[
    f(x) = \underbrace{f(x_0)}_{=0} + (x - x_0)f'(x_0) + o(x - x_0)
  \]
  \[
    g(x) = \underbrace{g(x_0)}_{=0} + (x - x_0)g'(x_0) + o(x - x_0)
  \]
  Thus we have:
  \begin{align*}
    \lim_{x \to x_0} \frac{f(x)}{g(x)} &= \lim_{x \to x_0} \left[\frac{f'(x_0) + \frac{o(x-x_0)}{x-x_0}}{g'(x_0) + \frac{o(x-x_0)}{x-x_0}}\right] \\
                                       &= \lim_{x \to x_0} \frac{f'(x)}{g'(x)} \text{ by continuity of first derivatives}
  \end{align*}
\end{proof}
\end{document}
