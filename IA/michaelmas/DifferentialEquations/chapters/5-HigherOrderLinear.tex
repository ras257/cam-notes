\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Higher Order Linear ODEs}
We will focus on 2nd order here, but many methods are also applicable to higher orders.
\section{Constant Coefficients}
The general form of a 2nd order linear ODE with constant coefficients is:
\begin{equation}
  \label{general2ndConst}
  \underbrace{a \deriv[2]{y}{x} + b \deriv{y}{x} + cy}_{\mathcal{D}y} = f(x)
\end{equation}
with $a, b, c$ constants.
Where $\mathcal{D}y$ is a linear differential operator, defined as:
\[
  \mathcal{D} \equiv a \deriv[2]{}{x} + b \deriv{}{x} + c
\]
\begin{definition}[Linear Operator]
  A differential operator, $\mathcal{D}$ is \textit{linear} if for any $y_1(x)$ and $y_2(x)$ and constants $\alpha$ and $\beta$:
  \[
    \mathcal{D}(\alpha y_1 + \beta y_2) = \alpha \mathcal{D}y_1 + \beta \mathcal{D}y_2
  \]
  This is also known as the \textit{principle of superposition}.
\end{definition}
We can exploit this property to solve \cref{general2ndConst}:
\begin{enumerate}
  \item Find complimentary functions (C.F.), $y_c$, that satisfy the corresponding homogeneous equation:
    \[
      a \deriv[2]{y_c}{x} + b \deriv{y_c}{x} + c y_c = 0
    \]
  \item Find \textbf{any} particular integral (P.I.), $y_p$ that satisfies the full equation.
  \item Then a solution of the full equation is then $y_c + y_p$ as:
    \[
      \mathcal{D}(y_c + y_p) = \underbrace{\mathcal{D}y_c}_{=0} + \mathcal{D}y_p = f(x)
    \]
    So it satisfies the full equation.
\end{enumerate}
A 2nd order ODE has \textbf{two} linearly independent complimentary functions, so the general solution to is:
\[
  y(x) = C_1 y_{c_1}(x) + C_2 y_{c_2}(x) + y_p(x)
\]
with $C_1, C_2$ constants.
\begin{definition}[Linear Dependance]
  A set of $n$ functions $\{f_i(x)\}$ is \textit{linearly dependant} if:
  \[
    \sum_{i=1}^{n} C_i f_i(x) = 0\ \forall x
  \]
  for $n$ constants, $C_i$, \textbf{not} all of which are $0$.
  Otherwise they are \textit{linearly independent}.
\end{definition}
\begin{remark}
  This is the same idea as linear dependence for vectors.
\end{remark}
Equivalently, if one or more of the functions $f_i(x)$  can be written as a linear combination of the others, they are linearly \textbf{dependant}.
\subsection{Complimentary Functions}
Recall that:
\[
  \deriv{}{x} e^{\lambda x} = \lambda e^{\lambda x} \text{ (Eigenfunction)}
\]
$e^{\lambda x}$ is also an eigenfunction of $\mathcal{D}$ because:
\begin{align*}
  \mathcal{D}(e^{\lambda x}) &= a \deriv[2]{}{x} e^{\lambda x} + b \deriv{}{x} e^{\lambda x} + c \\
                             &= \underbrace{(a\lambda^2 + b\lambda + c)}_{\text{eigenvalue}} e^{\lambda x}
\end{align*}
The complimentary functions of \cref{general2ndConst} satisfy $\mathcal{D}y_c = 0$, that is, they are eigenfunctions with eigenvalue $0$.
Thus:
\[
  y_c = A e^{\lambda x} \text{ with } \underbrace{a \lambda^2 + b\lambda + c = 0}_{\text{characteristic equation}}
\]
Since the characteristic equation of $\mathcal{D}$ is a 2nd degree polynomial it must have two roots $\lambda_1$ and $\lambda_2$.
\begin{proofcases}
  \begin{case}{$\lambda_1 \neq \lambda_2$}
    We then have two linearly independent complimentary functions:
    \[
      y_{c_1} \propto e^{\lambda_1 x}, y_{c_2} \propto e^{\lambda_2 x}
    \]
    So the most general complimentary function is a linear combination:
    \[
      y_c = C_1 y_{c_1}(x) + C_2 y_{c_2}(x)
    \]
    So $y_{c_1}$ and $y_{c_2}$ for a \textit{basis} for the space of solutions for the homogeneous equation.
    Note that if the roots are complex, we get oscillatory behaviour.
  \end{case}
  \begin{case}{$\lambda = \lambda_2$ -- Degenerate Case}
    So now we have only one linearly independent complimentary function of the form $e^{\lambda_1 x}$.
    See \cref{detuningExample} for how to deal with this case.
  \end{case}
\end{proofcases}
\begin{example}[Real, non-degenerate roots]
  \[
    \deriv[2]{y}{x} - 5\deriv{y}{x} + 6y = 0
  \]
  The characteristic equation is then:
  \[
    \lambda^2 - 5\lambda + 6 = 0 \implies \lambda_1 = 3, \lambda_2 = 2
  \]
  So the general complimentary function is:
  \[
    y_c(x) = A e^{3x} + B e^{2x}
  \]
  with $A, B$ constants.
\end{example}
\begin{example}[Complex, non-degenerate roots]
  \label{complexNonDegenerate}
  \[
    \deriv[2]{y}{x} + 4y = 0
  \]
  The characteristic equation is then:
  \[
    \lambda^2 + 4 = 0 \implies \lambda_1 = 2i, \lambda_2 = -2i
  \]
  So the general complimentary function is:
  \[
    y_c(x) = Ae^{2ix} + Be^{-2ix}
  \]
  Note that $e^{\pm2ix} = \cos(2x) \pm i \sin(2x)$ so:
  \begin{align*}
    y_c(x) &= (A + B)\cos(2x) + (A - B)i\sin(2x) \\
           &= \alpha \cos(2x) + \beta \sin(2x)
  \end{align*}
  Whether $\alpha, \beta$ are complex depends on the boundary conditions of the problem.
\end{example}
\begin{example}[Degenerate roots and ``detuning'']
  \label{detuningExample}
  \[
    \deriv[2]{y}{x} - 4\deriv{y}{x} + 4y = 0
  \]
  So the characteristic equation is:
  \[
    \lambda^2 - 4y + 4 = 0 \implies \lambda = 2
  \]
  Therefore we have degenerate roots and only one linearly independent complimentary function $e^{2x}$.

  \textbf{Detuning - }Remove the degeneracy by considering a slightly modified (\textit{detuned}) equation.
  \[
    \deriv[2]{y}{x} - 4 \deriv{y}{x} + (4 - \varepsilon^2)y = 0 \quad (\varepsilon \ll 1)
  \]
  So the characteristic equation is now:
  \[
    \lambda^2 - 4\lambda + (4 - \varepsilon^2) = 0 \implies \lambda = 2 \pm \varepsilon
  \]
  Which gives the complimentary function:
  \begin{align*}
    y_c(x) &= Ae^{(2 + \varepsilon)x} + Be^{(2 - \varepsilon)x} \\
           &=e^{2x}(Ae^{\varepsilon x} + Be^{-\varepsilon x}) \\
           &=e^{2x}[(A+B) + \varepsilon(A - B)x + O(A\varepsilon^2x^2) + O(B\varepsilon^2x^2)] \text{ (as $\varepsilon \to 0$)} \\
  \end{align*}
  Apply the initial conditions $y(0) = C$ and $y'(0) = D$ to both the original and detuned equations.
  This yields the equations:
  \begin{align*}
    &A + B = C \text{ and } 2C + \varepsilon(A - B) = D \\
    \implies& A = \frac{1}{2}\left(C + \frac{D - 2C}{\varepsilon}\right),\ B = \frac{1}{2}\left(C - \frac{D - 2C}{\varepsilon}\right)
  \end{align*}
  So $O(A) \text{ and } O(B) = O(\frac{1}{\varepsilon})$ as $\varepsilon \to 0$.
  Therefore $O(A\varepsilon^2x^2) \text{ and } O(B\varepsilon^2x^2) = O(\varepsilon x^2) \to 0$ as $\varepsilon \to0$.

  Now let:
  \[
    \alpha = A + B, \beta = \varepsilon(A - B)
  \]
  Both $\alpha$ and $\beta$ are $O(1)$ as they do not depend on $\varepsilon$ so are unchanged as $\varepsilon \to 0$.
  As we take $\varepsilon \to 0$, the detuned equation becomes the original equation, so the general solution of the original degenerate equation is:
  \[
    y = \alpha e^{2x} + \beta x e^{2x}
  \]
\end{example}
\begin{remark}[Note]
  In general, if $y_{c_1}$ is a degenerate complimentary function of a linear ODE with constant coefficients then $y_{c_2} = x y_{c_1}$ is a second linearly independent complimentary function.
\end{remark}
\section{Homogeneous Second Order ODEs with Non-constant Coefficients}
The general form of a homogeneous second order ODE with non-constant coefficients is:
\[
  y'' + p(x)y' + q(x)y = 0
\]
\subsection{Second Complimentary Function -- Reduction of order}
\label{reductionOrder}
The following method allows us to find a second solution, $y_2(x)$, given one solution, $y_1(x)$.

We try a solution of the form $y_2(x) = v(x)y_1(x)$.
Therefore:
\[
  y_2' = v'y_1 + vy_1' \text{ and } y_2'' = v''y_1 + 2v'y_1' + vy_1''
\]
If $y_2$ satisfies the original ODE then we require that:
\begin{align*}
  v''y_1 + v'(2y_1' + py_1) + v(\underbrace{y_1'' + py_1' + qy_1}_{0}) &= 0 \\
  v''y_1 + v'(2y_1' + py_1) &= 0
\end{align*}
We can then let $u = y'$ to reduce the order of the DE:
\[
  u'y_1 + u(2y_1' + py_1) = 0
\]
This is now a separable first order ODE for $u$.
\begin{align*}
  \frac{u'}{u} &= -\frac{2y_1'}{y_1} - p \\
  \implies \ln u &= -2 \ln y_1  - \int_{0}^{x} p(t) \d{t} + \ln A \\
  \implies u(x) &= \frac{A}{y^{2}_{1}}\exp\left[-\int_{0}^{x} p(t) \d{t}\right]
\end{align*}
We can then, in theory, integrate this expression for $u(x)$ to obtain $v(x)$.
\begin{example}
  Consider again the DE from \cref{detuningExample}:
  \[
    y'' - 4y' + 4y = 0
  \]
  So $p(x) = -4, q(x) = 4$.
  We know that one solution is $y_1(x) = e^{2x}$.
  \[
    \frac{u'}{u} = -\frac{4e^{2x}}{e^{2x}} - (-4) = 0
  \]
  So $u$ and therefore $v'$ are constants so:
  \[
    v(x) = Ax+B
  \]
  Therefore we have a second solution $y_2$ given by:
  \[
    y_2(x) = (Ax + B)e^{2x}
  \]
  The $Be^{2x}$ replicates $y_1$ so $xe^{2x}$ is a second linearly independent solution.
\end{example}
\subsection{Phase Space}
The general form of an $n$-th order linear ODE is:
\[
  y^{(n)} + p(x)y^{(n-1)} + \cdots + q(x)y = f(x)
\]
This means that $y^{(n)}$ is determined by $y^{(0)}, \ldots, y^{(n-1)}$.
When we differentiate the equation, we see that higher order derivatives can also be determined by $y^{(0)}, \ldots, y^{(n-1)}$.
This means that we can construct a Taylor series about $x_0$ if we specify $y^{(0)}(x_0), \ldots, y^{(n-1)}(x_0)$.

In other words, the \textit{state of the system} is fully specified at any $x$ by an $n$-dimensional \textit{solution vector}:
\[
  \vec{Y}(x) = \begin{pmatrix}
  y(x) \\
  y^{(1)}(x) \\
  \vdots \\
  y^{(n-1)}(x) \\
  \end{pmatrix}
\]
That is, given such a $\vec{Y}(x_0)$ at any fixed $x_0$, we can determine the Taylor series for the solution and use this to determine $y$ and all of its derivatives at any $x$.
\begin{remark}
  \begin{itemize}
    \item At any $x$, $\vec{Y}(x)$ defines a point in $n$-dimensional \textit{phase space}.
    \item As $x$ varies, $\vec{Y}(x)$ traces a trajectory through \textit{phase space}.
  \end{itemize}
\end{remark}
\begin{example}
  Consider again the DE from \cref{complexNonDegenerate}:
  \[
    y'' + 4y = 0
  \]
  We know that $y_1(x) = \cos 2x$ and $y_2(x) = \sin 2x$ so the solution vectors are:
  \[
    \vec{Y}_1 = \begin{pmatrix}
    y_1 \\
    y_1' \\
    \end{pmatrix} =
    \begin{pmatrix}
    \cos 2x \\
    -2\sin 2x \\
    \end{pmatrix},\
    \vec{Y}_2 = \begin{pmatrix}
    y_2 \\
    y_2 \\
    \end{pmatrix} =
    \begin{pmatrix}
    \sin 2x \\
    2 \cos 2x \\
    \end{pmatrix}
  \]
  So in 2D phase space, $\vec{Y}_1$ and $\vec{Y}_2$ lie on the same ellipse:
  \begin{center}
  \begin{tikzpicture}[scale=1.7, >=stealth]
    \draw[-] (-1.5, 0) -- (1.5, 0);
    \draw[-] (0, -2.5) -- (0, 2.5);

    \draw[thick, gray!70] (0, 0) ellipse (1 and 2);

    \draw[->, very thick] (0, 0) -- (0.7539, -1.3139) node[below right] {$\vec{Y}_1(x)$};
    \draw[->, very thick] (0, 0) -- (0.6569, 1.5078) node[right] {$\vec{Y}_2(x)$};
  \end{tikzpicture}
  \end{center}
  $\vec{Y}_1$ and $\vec{Y}_2$ are linearly independent vectors.
  This means that they form a basis for the 2D phase space.
\end{example}
\section{Wronskian and Linear Dependence}
\label{wronskian}
Recall that a set of functions $\{y_i(x)\}$ are linearly dependant if:
\[
  \sum_{i=1}^{n} c_i y_i(x) = 0\ \forall x,\ c_i \text{ not all 0}
\]
Since this holds for all $x$ we can differentiate it $1, 2, \ldots, n-1$  times.
Therefore:
\[
  \sum_{i=1}^{n} c_i y^{(k)}_i(x) = 0\ \forall x, k = 1, \ldots, n-1
\]
This sum the $k$-th entry in the sum of all the solution vectors, so we have:
\[
  \sum_{i=1}^{n} c_i \vec{Y}_i(x) = 0\ \forall x
\]
So if $\{y_i\}$ is linearly dependant, then $\{\vec{Y}_i(x)\}$ are linearly dependant for all $x$.

We can form a \textit{fundamental matrix}, $\Psi(x)$, whose columns are the solution vectors:
\[
  \Psi(x) = \begin{pmatrix}
  \uparrow & \uparrow &  & \uparrow \\
  \vec{Y}_1 & \vec{Y}_2 & \cdots & \vec{Y}_n \\
  \downarrow & \downarrow &  & \downarrow \\
  \end{pmatrix}
\]
\begin{definition}[Wronskian]
  The \textit{Wronskian} of $n$ functions $\{y_i\}$ is defined to be the following determinant:
  \[
    W(x) =
    \begin{vmatrix}
      y_1 & y_2 & \cdots & y_n \\
      y_1' & y_2' & \cdots & y_n' \\
      \vdots & \vdots & \ddots & \vdots \\
      y^{(n-1)}_1 & y^{(n-1)}_2 & \cdots & y^{(n-1)}_n \\
    \end{vmatrix}
  \]
\end{definition}
\begin{remark}[Note]
  In this case, the Wronskian is the determinant of the fundamental matrix, that is:
  \[
    W(x) =
    \det(\Psi(x)) =
    \begin{vmatrix}
      \uparrow & \uparrow &  & \uparrow \\
      \vec{Y}_1 & \vec{Y}_2 & \cdots  & \vec{Y}_n \\
      \downarrow & \downarrow &  & \downarrow \\
    \end{vmatrix}
  \]
\end{remark}
Recall that if a matrix has linearly dependant columns then its determinant is 0.
Therefore we have:
\[
  \{y_i(x)\} \text{ is linearly \textbf{dependant}} \implies \{\vec{Y}_i(x)\} \text{ is linearly \textbf{dependant}} \implies W(x) = 0\ \forall x
\]
Taking the contrapositive, it follows that if $W(x) \neq 0$ for some $x$ then $\{y_i(x)\}$ are linearly \textbf{independent}.
\begin{remark}[Warning]
  $W(x) = 0\ \forall x$  does \textbf{not} necessarily imply that $\{y_i(x)\}$ are linearly dependant.
\end{remark}
\begin{example}
  \label{wronskianExample}
  Consider again the DE from \cref{complexNonDegenerate}:
  \[
    y'' + 4y = 0
  \]
  We can calculate the Wronskian using the solution vectors we found earlier:
  \[
    W(x) = \begin{vmatrix}
    y_1 & y_2 \\
    y_1' & y_2' \\
    \end{vmatrix} = \begin{vmatrix}
    \cos 2x & \sin2x \\
    -2\sin 2x & 2\cos 2x \\
    \end{vmatrix}
    = 2(\cos^2 2x + \sin^2 2x) = 2
  \]
  Since $W(x) \neq 0\ \forall x$, $y_1$ and $y_2$ are linearly independent.
\end{example}
\section{Abel's Theorem}
\begin{theorem}
  Given any two solutions of:
  \[
    y'' + p(x)y' + q(x)y = 0
  \]
  If $p(x)$ and $q(x)$ are continuous on an interval $I$, then either $W(x) = 0\ \forall x \in I$ or $W(x) \neq 0\ \forall x \in I$.
\end{theorem}
\begin{proof}
  \begin{align*}
    W(x) &= y_1 y_2' - y_2 y_1' \\
    W'(x) &= \cancel{y_1' y_2'} + y_1 y_2'' - \cancel{y_2' y_1'} - y_2 y_1'' \\
          &= y_1 y_2'' - y_2 y_1'' \\
          &= -y_1(py_2' + qy_2) + y_2(py_1' + qy_1) \\
          &= -p(x)(y_1 y_2' - y_2 y_1') \\
          &= -p(x)W(x)
  \end{align*}
  So we now have a separable ODE for $W(x)$, solving yields:
  \[
    W(x) = W(x_0)\underbrace{\exp\left[-\int_{x_0}^{x} p(u) \d{u}\right]}_{\neq0}
  \]
  This is known as \textit{Abel's identity}.
  The exponential is never 0 so:
  \begin{align*}
    W(x_0) = 0 &\implies W(x) = 0\ \forall x \\
    W(x_0) \neq 0 &\implies W(x) \neq 0\ \forall x
  \end{align*}
\end{proof}
\begin{remark}
  The geometric interpretation of this is the solution vectors are always collinear or never collinear as $x$ varies.
\end{remark}
\begin{remark}[Generalisation]
  Abel's theorem holds for solutions of $n$-th order linear homogeneous ODEs.
  There is also a generalisation of Abel's identity, see Example Sheet 3, Q7.
\end{remark}
\begin{corollary}
  If $p(x) = 0$ then $W(x)$ is a constant.
\end{corollary}
\begin{proof}
  Then $W'(x) = 0 \implies W(x) = C$.
\end{proof}
We can find $W(x)$ without knowing solutions to the ODE.
\begin{example}[Bessel's Equation]
  Consider the ODE:
  \begin{align*}
    x^2 y'' + xy' + (x^2 - n^2)y &= 0 \\
    y'' + \underbrace{\frac{1}{x}}_{p(x)}y' + \left(1 - \frac{n^2}{x^2}\right)y &= 0
  \end{align*}
  We can then apply Abel's identity so:
  \begin{align*}
    W(x) &= W(x_0)\exp\left[-\int_{x_0}^{x} \frac{\d{u}}{u}\right] \\
         &= W(x_0)\exp\left[-\ln\left(\frac{x}{x_0}\right)\right] \\
         &= \frac{W(x_0)x_0}{x}
  \end{align*}
\end{example}
\subsection{Finding Second Solutions}
Abel's identity can be used to find a second solution $y_2$ given a known solution $y_1$.
\[
  y_1 y_2' + y_2 y_1' = W(x_0)\exp\left[-\int_{x_0}^{x} p(u) \d{u}\right]
\]
Dividing by $y^{2}_{1}$ we have:
\[
  y_2' \frac{1}{y_1} + y_2 \frac{y_1'}{y^{2}_{1}} = \frac{W(x_0)}{y^{2}_{1}}\exp\left[-\int_{x_0}^{x} p(u) \d{u}\right]
\]
So provided we know $y_1$ this is a first order ODE for $y_2$:
\[
  \deriv{}{x}\left(\frac{y_2}{y_1}\right) = \frac{W(x_0)}{y^{2}_{1}}\exp\left[-\int_{x_0}^{x} p(u) \d{u}\right]
\]
This is the same result that we had using the reduction of order method in \cref{reductionOrder} as $W(x_0)$ is a constant.

\section{Linear Equidimensional ODEs}
\begin{definition}
  A linear second order ODE is \textit{equidimensional} if it has form:
  \begin{equation}
    \label{generalEquidimensional}
    ax^2 y'' + bx y' + cy = f(x)
  \end{equation}
  with $a, b, c$ constants.
\end{definition}
\begin{remark}[Note]
  These are called \textit{equidimensional} as quantities on the left hand side have consistent physical dimensions.
\end{remark}
\subsection{Scaling Property}
If it is homogeneous, then we have a scaling property of solutions.
\begin{proposition}
  If $g(x)$ is a solution of \cref{generalEquidimensional} with $f(x) = 0$, then so is $y = g(\alpha x)$ where $\alpha$ is a constant.
\end{proposition}
\begin{proof}
  \begin{align*}
    \deriv{}{x}(g(\alpha x)) &= \alpha g'(\alpha x) \\
    x \deriv{y}{x} &= (\alpha x)g'(\alpha x) \\
    x^2 \deriv[2]{y}{x} &= (\alpha x)^2 g''(\alpha x)
  \end{align*}
  Substituting back into \cref{generalEquidimensional} yields:
  \begin{align*}
    ax^2 \deriv[2]{y}{x} + bx \deriv{y}{x} + cy &= a (\alpha x)^2 g''(\alpha x) + b(\alpha x)g'(\alpha x) + cg(\alpha x) \\
                                                &= au^2 g''(u) + bu g'(u) + cg(u) \\
                                                &=0 \text{ as $g(u)$ is a solution}
  \end{align*}
  so $g(\alpha x)$ is also a solution.
\end{proof}

\subsection{Solving By Eigenfunctions}
\label{solvingByEigenfunctions}
Notice that:
\[
  x \deriv{}{x}(x^{k}) = k x^{k}
\]
so $x^{k}$ is an eigenfunction of $x \deriv{}{x}$ with eigenvalue $k$.
We will then look for a complimentary function of the form $y_c = x^{k}$.
\[
  x^{k}[ak(k - 1) + bk + c] = 0
\]
Since this is true for all $x$ we must have:
\[
  ak^2 + k(b - a) + c = 0
\]
In general this will have two roots $k_1, k_2$.
If $k_1 \neq k_2$ then the general complimentary function is:
\[
  y_c = Ax^{k_1} + Bx^{k_2}
\]
with $A, B$ constants.
\subsection{Solving By Substitution}
Substitute $z = \ln x$, so:
\[
  \deriv{y}{z} = \deriv{x}{z} \deriv{y}{x} = e^{z} \deriv{y}{x} = x \deriv{y}{x}
\]
\begin{align*}
  \deriv[2]{y}{z} &= e^{z} \deriv{y}{x} + e^{2z} \deriv[2]{y}{x} \\
                  &= x \deriv{y}{x} + x^2 \deriv[2]{y}{x}
\end{align*}
So \cref{generalEquidimensional} becomes:
\begin{align*}
  a\left(\deriv[2]{y}{z} - \deriv{y}{z}\right) + b \deriv{y}{z} + cy &= f(e^{z}) \\
  a \deriv[2]{y}{z} + (b - a)\deriv{y}{z} + cy &= f(e^{z})
\end{align*}
This is now a second order linear ODE with constant coefficients.
So $y_c \propto e^{\lambda z}$ and has characteristic equation:
\[
  a\lambda^2 + (b - a)\lambda + c = 0
\]
This will have the same solutions as the equation for $k_1, k_2$ from the eigenfunction method (\cref{solvingByEigenfunctions}).
In the non-degenerate case we have:
\begin{align*}
  y_c &= Ae^{k_1 z} + Be^{k_2 z} \\
      &= Ax^{k_1} + Bx^{k_2}
\end{align*}
We also know from \cref{detuningExample} that in the degenerate case we have:
\begin{align*}
  y_c &= Ae^{kz} + Bze^{kz} \\
      &= Ax^{k} + B(\ln x)x^{k}
\end{align*}

\section{Inhomogeneous (Forced) 2nd Order ODEs}
This section covers methods to find particular integrals.
\subsection{Constant Coefficients}
Recall that these have general form:
\[
  a \deriv[2]{y}{x} + b \deriv{y}{x} + cy = f(x)
\]
After we have found the complimentary function we need to determine a single particular integral.
It is helpful to know what kind of particular integrals are likely to work for different types of forcing function $f(x)$.
In general, this is the most ``general'' form of the type of function $f$ is, for example:
\begin{center}
\begin{tabular}{c|c}
\label{PITable}
$f(x)$ & Try particular integral of form \\
\hline
$e^{mx}$ & $Ae^{mx}$ \\
$\sin kx$ or $\cos kx$ & $A \sin kx + B \cos kx$ \\
Polynomial $p_n(x)$ & Polynomial $q_n (x) = a_nx^{n} + a_{n - 1}x^{n-1} + \cdots + a_1 x + a_0$ \\
\end{tabular}
\end{center}
We can then determine the constants in these particular integrals by substituting into the ODE.
Since it is a linear ODE, we can also superpose terms to try and find a particular integral.

\begin{example}
  Consider the ODE:
  \[
    y'' - 5y' + 6y = 2x + e^{4x}
  \]

  Try a particular integral $y_p = \underbrace{Ax + B}_{\text{for }2x} + \underbrace{Ce^{4x}}_{\text{for }e^{4x}}$.
  \begin{align*}
    y_p' &= A + 4Ce^{4x} \\
    y_p'' &= 16Ce^{4x}
  \end{align*}
  Substituting back into the DE and comparing coefficients:
  \[
    (\underbrace{16C - 20C + 6C}_{=1})e^{4x} + (\underbrace{6A}_{=2})x + (\underbrace{-5A + 6B}_{=0}) = 2x + e^{4x}
  \]
  Therefore we have $2C = 1 \implies C = \frac{1}{2}$, $6A = 2 \implies A = \frac{1}{3}$, $6B = \frac{5}{3} \implies B = \frac{5}{18}$.
  So:
  \[
    y_p = \frac{1}{3}x + \frac{5}{18} + \frac{1}{2}e^{4x}
  \]
  The complimentary function is:
  \[
    y_c = \alpha e^{2x} + \beta e^{3x}
  \]
  Therefore, the general solution is:
  \[
    y = \alpha e^{2x} + \beta e^{3x} + \frac{1}{2}e^{4x} + \frac{1}{3}x + \frac{5}{18}
  \]
  for constants $\alpha, \beta$.
\end{example}
\subsubsection{Resonance}
What if the forcing term, $f(x)$, involves a term that is in a complimentary function?
We can solve this using detuning again.
\begin{example}
  Consider the ODE:
  \[
    \ddot{y} + \omega_0^2 y = \sin(\omega_0 t)
  \]
  The homogeneous form of this is simple harmonic motion with angular frequency $\omega_0$.
  However, we have a forcing term at a \textit{natural frequency} $\omega_0$.
  We say that the oscillator is driven \textit{resonantly} because:
  \[
    y_c(t) = A\sin(\omega_0 t) + B\cos(\omega_0 t)
  \]
  so the oscillator is driven at a forcing frequency equal to the natural frequency.
  Consider the detuned equation:
  \[
    \ddot{y} + \omega^{2}_{0} y = \sin(\omega t)
  \]
  for some $\omega \neq \omega_0$.

  Try a particular integral $y_p(t) = C\sin(\omega t) + D \cos(\omega t)$:
  \begin{align*}
    \dot{y_p} &= C\omega\cos(\omega t) - D\omega\sin(\omega t) \\
    \ddot{y_p} &= -C\omega^2\sin(\omega t) - D\omega^2\cos(\omega t)
  \end{align*}
  Therefore:
  \[
    (C\omega^{2}_{0} - C\omega^2)\sin(\omega t) + (D\omega^{2}_{0} - D\omega)\cos(\omega t) = \sin(\omega t)
  \]
  So $D = 0$ and:
  \[
    (\omega^{2}_{0} - \omega^2)C = 1 \implies C = \frac{1}{\omega^{2}_{0} - \omega^2}
  \]
  However, notice that the limit as $\omega \to \omega_0$ does not exist.

  We can try adding in a complimentary function to regularise the limit.
  \[
    y_p(t) = \frac{1}{\omega^{2}_{0} - \omega^2}(\sin(\omega t) - \underbrace{\sin(\omega_0 t)}_{C.F.})
  \]
  Since we have just added a complimentary function, this still satisfies the detuned equation.
  We can now try to evaluate this indeterminate limit using L'H\^opitals rule, \cref{LHopitals}:
  \[
    \lim_{\omega \to \omega_0} y_p(t) = \lim_{\omega \to \omega_0} \left[\frac{t\cos(\omega t)}{-2\omega}\right] =-\frac{t}{2\omega_0}\cos(\omega_0 t)
  \]
  Therefore the particular integral of the original equation is:
  \[
    y_p(t) = -\frac{t}{2 \omega_0}\cos(\omega_0 t)
  \]
  Notice that the amplitude of this solution grows with $t$, this is referred to as \textit{resonance}.
\end{example}
In general, if the forcing term is a linear combination of complimentary functions, the particular integral is of the form:
\[
  y_p(t) = t \times (\text{Non-resonant P.I})
\]
We can obtain the non-resonant particular integral from the earlier table in \cref{PITable}.

\begin{remark}[Note]
  If the homogeneous equation is degenerate, we may need to try:
  \[
    y_p(t) = t^2 \times (\text{Non-resonant P.I.})
  \]
  as the complimentary function will already have a term with a factor of $t$.
\end{remark}
\subsubsection{Resonances in Equidimensional ODEs}
Consider:
\[
  ax^2y'' + bxy' + cy = f(x)
\]
We know that the general complimentary function is:
\[
  y_c = Ax^{k_1} + Bx^{k_2}
\]
provided we are in the non-degenerate case ($k_1 \neq k_2$).

If $f(x) \propto x^{m}$, then we should try a particular integral of the form $Cx^{m}$  for $m \neq k_1$ and $m \neq k_2$.
In the resonant case, $f(x) \propto x^{k_1} \text{ or }x^{k_2}$, then the particular integral is:
\[
  y_p \propto (\ln x) x^{k_1}
\]
This follows from transforming by $z = \ln x$.
The particular integral is $y_p = ze^{k_1 z}$ in the case of constant coefficients.
This is then $(\ln x)x^{k_1}$ after substituting back in with $x$.
\begin{remark}[Note]
  If the homogeneous equation is degenerate $(k_1 = k_2)$, we may need to try:
  \[
    y_p = (\ln x)^2 x^{k_1}
  \]
\end{remark}
\subsection{Variation of Parameters}
Variation of parameters is a systematic method to find a particular integral given two linearly independent complimentary functions.

Consider the general form of a 2nd order linear ODE with non-constant coefficients:
\[
  y'' + p(x)y' + q(x)y = f(x)
\]
with linearly independent complimentary functions $y_1$ and $y_2$.
Since we know that $y_1$ and $y_2$ are linearly independent, the solution vectors $\vec{Y}_1(x)$ and $\vec{Y}_2(x)$ are linearly independent for all $x$ so we can use them as a basis for phase space at any $x$.

Because they form a basis, we can write the solution vector for the particular integral, $\vec{Y}_p$, as a linear combination of $\vec{Y}_1$ and $\vec{Y}_2$ where the coefficients are functions of $x$:
\[
  \vec{Y}_p(x) = u(x)\vec{Y}_1(x) + v(x)\vec{Y}_2(x)
\]
We can from equations using the components of this to make it easier to solve:
\begin{align*}
  \text{First Component: }&y_p(x) = u(x)y_1(x) + v(x)y_2(x) \\
  \text{Second Component: }&y_p(x)' = u(x)y_1'(x) + v(x)y_2'(x)
\end{align*}
Start by taking the derivative of the second component:
\[
  y_p'' = u y_1'' + u'y_1' + vy_2'' + v'y_2'
\]
from the original ODE we have:
\begin{align*}
  f(x) &= y_p'' + py_p' + qy_p \\
       &= uy_1'' + u' y_1' + vy_2'' + v'y_2' + p(x)[uy_1' + vy_2'] + q(x)[uy_1 + vy_2] \\
       &= u[\underbrace{y_1'' + p(x)y_1' + q(x)y_1}_{=0}] + v[\underbrace{y_2'' + p(x)y_2' + q(x)y_2}_{=0}] + u'y_1' + v'y_2' \\
       &= u'y_1' + v'y_2'
\end{align*}
since $y_1$ and $y_2$ are complimentary functions.

The second component must be consistent with the derivative of the first component, therefore:
\begin{align*}
  u'y_1 + \cancel{uy_1'} + v'y_2 + \cancel{vy_2'} &= \cancel{uy_1'} + \cancel{vy_2'} \\
  u' y_1 + v' y_2 &= 0
\end{align*}
We can then combine these two equations into a matrix equation:
\begin{align*}
  \begin{pmatrix}
  y_1 & y_2 \\
  y_1' & y_2' \\
  \end{pmatrix}
  \begin{pmatrix}
  u' \\
  v' \\
  \end{pmatrix}&=
  \begin{pmatrix}
  0 \\
  f \\
  \end{pmatrix}\\\implies
  \begin{pmatrix}
  u' \\
  v' \\
  \end{pmatrix}&=
  \frac{1}{W}
  \begin{pmatrix}
  y_2' & -y_2 \\
  -y_1' & y_1 \\
  \end{pmatrix}
  \begin{pmatrix}
  0 \\
  f \\
  \end{pmatrix}
\end{align*}
Where $W(x) = y_1 y_2' - y_2 y_1'$ is the Wronskian, \cref{wronskian}.
Therefore:
\[
  u' = - \frac{y_2}{W}f,\ v' = \frac{y_1}{W}f
\]
We can then integrate to yield:
\[
  u = \int^{x} \frac{y_1(t)f(t)}{W(t)} \d{t},\ v = \int^{x} \frac{y_2(t)f(t)}{W(t)} \d{t}
\]
Substituting back into the equation from the first component yields:
\[
  y_p(x) = y_2(x)\int^{x} \frac{y_1(t)f(t)}{W(t)} \d{t} - y_1(x) \int^{x} \frac{y_2(t)f(t)}{W(t)} \d{t}
\]
Note that changing the lower limit/including an integration constant is unnecessary as it only adds multiples of the complimentary functions so the particular integral will still satisfy the original ODE regardless.
\begin{example}
  Consider again the ODE from \cref{complexNonDegenerate}:
  \[
    y'' + 4y = \underbrace{\sin 2x}_{f(x)}
  \]
  This has complimentary functions:
  \[
    y_1 = \sin 2x,\ y_2 = \cos 2x
  \]
  so the Wronskian is $W(x) = -2$ (see \cref{wronskianExample}).

  Note that because $f(x)$ is a linear combination of complimentary functions, the forcing is resonant.
  We can then use variation of parameters to find a particular integral:
  \begin{align*}
    y_p(x) &= -\frac{1}{2}\cos2x \int_{}^{x} \sin(2u)\sin(2u) \d{u} - \left(-\frac{1}{2}\right)\sin2x \int_{}^{x} \cos(2u)\sin(2u) \d{u} \\
           &= -\frac{1}{4}\cos2x \int_{}^{x} (1-\cos 4u) \d{u} + \frac{1}{4} \sin2x \int_{}^{x} \sin 4u \d{u} \\
           &= -\frac{1}{4}\cos2x \left[x - \frac{1}{4}\sin4x\right] + \frac{1}{4}\sin2x \left[-\frac{1}{4}\cos 4x\right] \\
           &= -\frac{1}{4}\cos2x \left[x - \frac{1}{2}\sin2x\cos2x\right] - \frac{1}{16}\sin2x [2\cos^2 2x - 1] \\
           &= -\frac{1}{4}x\cos2x + \underbrace{\frac{1}{16}\sin2x}_{\text{C.F.}}
  \end{align*}
  Since $\sin2x$ is a complimentary function, it can be removed, thus $y_p = -\frac{1}{4}x\cos 2x$ which is $x \times (\text{A complimentary function})$, as expected.
\end{example}
\section{Forced ODEs -- Transients and Damping}
\label{forcedODEs}
Consider a mass $m$ attached to the floor by a spring and a shock absorber that is driven upwards by a driving force $F(t)$.
The spring provides a restoring force $-ky$ and the shock absorber provides a damping force $-b\dot{y}$.

From Newton's Second Law, we have:
\begin{align*}
  m\ddot{y} &= -ky - b\dot{y} + F(t) \\
  m\ddot{y} = b\dot{y} + ky &= F(t)
\end{align*}
For $b = 0$ and $F(t) = 0$, this is simple harmonic motion at angular frequency $\omega_0 = \sqrt{k/m}$.
For convenience, we introduce a dimensionless time coordinate:
\[
  \tau = \omega_0t \implies \deriv{y}{t} = \omega_0 \deriv{y}{\tau}
\]
We then get:
\[
  \underbrace{y''}_{\deriv[2]{y}{\tau}} + \underbrace{2\kappa}_{=\frac{b}{m\omega_0}}y' + y = \underbrace{f(\tau)}_{=\frac{F(t)}{k}}
\]
\subsection{Free Response}
The behaviour is described by one dimensionless parameter $\kappa$:
\[
  y'' + 2\kappa y' + y = 0
\]
This has constant coefficients so we have characteristic equation:
\[
  \lambda^2 + 2\kappa\lambda + 1 = 0 \implies \lambda = -\kappa \pm \sqrt{\kappa^2 - 1}
\]
\begin{proofcases}
  \begin{case}{Light Damping, $\kappa < 1$}
    $\lambda_1$ and $\lambda_2$ are complex:
    \[
      \lambda_1, \lambda_2 = -\kappa \pm i \sqrt{1 - \kappa^2}
    \]
    So the general solution is:
    \[
      y(\tau) = e^{-\kappa \tau}\left[A\sin\left(\tau\sqrt{1-\kappa^2}\right) + B\cos\left(\tau\sqrt{1 - \kappa^2}\right)\right]
    \]
    where $A, B$ are constants.
    \begin{center}
    \begin{tikzpicture}
      \draw[->] (0, 0) -- (10, 0) node[right] {$\tau$};
      \draw[->] (0, -2.5) -- (0, 2.5) node[right] {$y(\tau)$};

      \def\kconst{0.2}
      \node at (2.6, 1.7) {$e^{-\kappa \tau}$};
      \node at (2.6, -1.7) {$-e^{-\kappa \tau}$};

      \draw[gray!70, dashed, domain=0:10, smooth, samples=100] plot (\x,{2.1 * exp(-\kconst*\x)});
      \draw[gray!70, dashed, domain=0:10, smooth, samples=100] plot (\x,{- 2.1 * exp(-\kconst*\x)});
      \draw[thick, domain=0:10, smooth, samples=100] plot (\x,{2.1 * exp(-\kconst*\x) * (-0.1138 * sin(\x * sqrt(1 - \kconst^2) r) + 0.8 * cos(\x * sqrt(1 - \kconst^2)r))});
    \end{tikzpicture}
    \end{center}
    The oscillations are at $\omega_{\text{free}} = \omega_0\sqrt{1 -\kappa^2}$.
    Note that as $\kappa \to 0$, $\omega_{\text{free}} \to \omega_0$.
    This makes sense as when $\kappa = 0$, there is no damping so we expect it to oscillate at the natural frequency $\omega_0$.
    The period of the oscillations is:
    \[
      T=\frac{2\pi}{\omega_{\text{free}}} = \frac{2\pi}{\omega_0\sqrt{1 - \kappa^2}}
    \]
  \end{case}
  \begin{case}{Critical Damping, $\kappa = 1$}
    This is the degenerate case, $\lambda_1 = \lambda_2 = -\kappa$.
    So the general solution is:
    \[
      y(\tau) = e^{-\kappa \tau}(A + B\tau)
    \]
    where $A, B$ are constants.
    \begin{center}
    \begin{tikzpicture}
      \draw[->] (0, 0) -- (10, 0) node[right] {$\tau$};
      \draw[->] (0, 0) -- (0, 3) node[right] {$y(\tau)$};

      \node at (7.5, 2.5) {$A > 0,\ B > \kappa A$};

      \def\kconst{0.6}
      \draw[thick, domain=0:10, smooth, samples=100] plot (\x,{2.1 * exp(-\kconst*\x) * (0.5 + 1.7 * \x)});
    \end{tikzpicture}
    \end{center}
  \end{case}
  \begin{case}{Heavy Damping, $\kappa > 1$}
    $\lambda_1$ and $\lambda_2$ are real.
    WLOG, take $|\lambda_1| < |\lambda_2|$.
    \[
      \lambda_1 = \underbrace{-\kappa + \sqrt{\kappa^2 -1}}_{<0},\ \lambda_2 = \underbrace{-\kappa -\sqrt{\kappa^2 - 1}}_{<0}
    \]
    So the general solution is:
    \[
      y(\tau) = Ae^{-|\lambda_1|\tau} + Be^{-|\lambda_2|\tau}
    \]
    Since $|\lambda_1| > |\lambda_2|$, the first term dominates the long term behaviour provided $A \neq 0$.
    \begin{center}
    \begin{tikzpicture}
      \draw[->] (0, 0) -- (10, 0) node[right] {$\tau$};
      \draw[->] (0, -2) -- (0, 2.5) node[right] {$y(\tau)$};

      \def\kconst{1.5}
      \pgfmathsetmacro{\lone}{-\kconst + sqrt(\kconst^2 - 1)}
      \pgfmathsetmacro{\ltwo}{-\kconst - sqrt(\kconst^2 - 1)}
      \def\aconst{2.1}
      \def\bconst{-1.6}
      \node at (1.3, 2.0) {$Ae^{-|\lambda_1| \tau}$};
      \node at (1.2, -1) {$Be^{-|\lambda_2| \tau}$};
      \node at (4.5, 0.8) {$y(\tau)$};

      \draw[gray!70, dashed, domain=0:10, smooth, samples=100] plot (\x,{\aconst * exp(\lone*\x)});
      \draw[gray!70, dashed, domain=0:10, smooth, samples=100] plot (\x,{\bconst * exp(\ltwo*\x)});
      \draw[thick, domain=0:10, smooth, samples=100] plot (\x,{\aconst * exp(\lone*\x) + \bconst * exp(\ltwo * \x)});
    \end{tikzpicture}
    \end{center}
    As you can see, the initial behaviour of the solution is similar to that of $Be^{-|\lambda_2|\tau}$ but eventually the $Ae^{-|\lambda_1|\tau}$ dominates.
  \end{case}
\end{proofcases}
\begin{remark}[Note]
  In all three cases, the free/unforced response decays eventually towards 0.
\end{remark}
\subsection{Forced Response}
Initially, the solution will be similar to just C.F. + P.I, this is the \textit{transient response}.
But as time progresses, the system settles down to a specific particular integral, the \textit{steady state response}, for any choice of initial conditions.
\begin{example}
  Consider the ODE.
  Note that we are now just using normal $t$ instead of $\tau$.
  \[
    \ddot{y} + \mu \dot{y} + \omega^{2}_{0}y = \frac{F_0}{m}\sin \omega t \quad \mu = \frac{b}{m},\ \kappa = \frac{\mu}{2\omega_0}
  \]
  Assuming light damping ($\mu < 2 \omega_0$).
  So the complimentary function is:
  \[
    y_c = e^{-\frac{\mu t}{2}}(A\sin\omega_{\text{free}}t + B\cos\omega_{\text{free}}t) \quad \omega_{\text{free}} = \sqrt{\omega^{2}_{0} - \mu^2/4}
  \]
  Try a particular integral of the form:
  \[
    y_p(t) = \frac{F_0}{m}(C\sin\omega t+ D\cos \omega t)
  \]
  Substituting into the DE:
  \[
    \sin \omega t(-C\omega^2-D\mu\omega + \omega^{2}_{0}C) + \cos \omega t(-D\omega^2 + C\mu\omega + \omega^{2}_{0}D) = \sin \omega t
  \]
  Comparing coefficients of $\sin$ and $\cos$ yields:
  \[
    D(\omega^{2}_{0} - \omega^2) = -C\mu\omega,\ C(\omega^{2}_{0} - \omega^2) = 1 + D\mu\omega
  \]
  We can then eliminate $C$:
  \[
    -\frac{D(\omega^{2}_{0} - \omega^2)^2}{\mu \omega} = 1 + D\mu\omega \implies D=\frac{-\mu\omega}{(\omega^{2}_{0}-\omega^2)^2 + \mu^2\omega^2}
  \]
  and therefore:
  \[
    C = \frac{\omega^{2}_{0} - \omega^2}{(\omega^{2}_{0} - \omega^2)^2 + \mu^2\omega^2}
  \]
  Note that even if the driving has $\omega = \omega_0$, we do not end up with resonance issues because of the damping.
  If $\mu = 0$, then this would not be the case.

  Finally we can write $y_p$ as:
  \[
    y_p(t) = \frac{F_0/m}{(\omega^{2}_{0} - \omega^2)^2 +\mu^2\omega^2}[(\omega^{2}_{0} - \omega^2)\sin\omega t - \mu \omega \cos \omega t]
  \]
  If we then plot $y(t) = y_c + y_p$ together with $y_c$ and $y_p$, we see that the solution is originally influenced by both $y_c$ and $y_p$.
  This is the \textit{transient response}.
  Then as $t$ increases, $y_c$ decays so the solution approaches $y_p$.
  This is the \textit{steady-state response}.
  \begin{center}
  \begin{tikzpicture}
    \draw[->] (0, 0) -- (10, 0) node[right] {$t$};
    \draw[->] (0, -2.5) -- (0, 2.5) node[right] {$y(t)$};

    \def\omegazero{2.5}
    \def\omegaconst{1.5}
    \def\muconst{0.8}
    \def\fmconst{8}
    \def\cfamp{1.8}
    \def\cfphase{-0.5}
    \pgfmathsetmacro\omegafree{sqrt(\omegazero^2 - 0.25 * \muconst^2)}
    \pgfmathsetmacro\amplitude{(\fmconst)/(sqrt((\omegazero^2 - \omegaconst^2)^2 + \muconst^2 * \omegaconst^2))}
    \pgfmathsetmacro\tempconst{(\fmconst)/((\omegazero^2 - \omegaconst^2)^2 + \muconst^2 * \omegaconst^2)}

    \draw[gray!70, dashed] (0, \amplitude) -- (10, \amplitude);
    \draw[gray!70, dashed] (0, -\amplitude) -- (10, -\amplitude);
    \node[right] at (10, \amplitude) {$y_p$ amplitude};
    \node[right] at (10, -\amplitude) {$y_p$ amplitude};

    \draw[gray!70, domain=0:10, smooth, samples=100] plot (\x,{exp(-0.5 * \muconst * \x) * \cfamp * cos((\omegafree * \x - \cfphase) r)});
    \draw[gray!90, dotted, domain=0:10, smooth, samples=100] plot (\x,{exp(-0.5 * \muconst * \x) * \cfamp});
    \draw[gray!90, dotted, domain=0:10, smooth, samples=100] plot (\x,{-exp(-0.5 * \muconst * \x) * \cfamp});

    \draw[gray!70, domain=0:10, smooth, samples=100] plot (\x,{\tempconst * ((\omegazero^2 - \omegaconst^2) * sin(\omegaconst * \x r) - \muconst * \omegaconst * cos(\omegaconst * \x r))});

    \draw[thick, domain=0:10, smooth, samples=100] plot (\x,{\tempconst * ((\omegazero^2 - \omegaconst^2) * sin(\omegaconst * \x r) - \muconst * \omegaconst * cos(\omegaconst * \x r)) + exp(-0.5 * \muconst * \x) * \cfamp * cos((\omegafree * \x - \cfphase) r)});

    \node at (5.5, 2.2) {$y(t) = y_p + y_c$};
    \node at (1.3, 2.2) {$y_p$};
    \node at (1.05, -1.4) {$y_c$};

    \draw[<->] (0, -2.7) -- (3.6, -2.7) node[midway, below] {\small Transient Response};
    \draw[<->] (3.65, -2.7) -- (10, -2.7) node[midway, below] {\small Steady-State Response};
  \end{tikzpicture}
  \end{center}
\end{example}
\section{Impulses and Point Forces}
Consider a system that experiences a sudden force between time $t = T - \varepsilon$ and $t = T + \varepsilon$ for small $\varepsilon$:
\begin{center}
\begin{tikzpicture}[scale=1.3]
  \draw[->] (0, 0) -- (7, 0) node[right] {$t$};
  \draw[->] (0, 0) -- (0, 3) node[right] {$F(t)$};

  \node[below] at (2, 0) {$T - \varepsilon$};
  \node[below] at (3.5, 0) {$T$};
  \node[below] at (5, 0) {$T + \varepsilon$};

  \clip (0, 0) rectangle (8, 3);
  \draw[thick, domain=2:5, smooth, samples=100] plot (\x,{2.8*exp(-(\x - 3.5)^2) - 0.3});
\end{tikzpicture}
\end{center}
For example, when modelling striking a mass on a spring or a car going over a kerb.
\begin{example}
  \label{impulseExample}
  Recall from \cref{forcedODEs} the ODE that models a damped forced oscillator:
  \[
    m\ddot{y} + b\dot{y} + ky = F(t)
  \]
  It is mathematically convenient to consider the limit of a sudden impulse as $\varepsilon \to 0$.

  We can integrate the ODE from $T - \varepsilon$ to $T + \varepsilon$ and take then limit as $\varepsilon \to 0$:
  \[
    \lim_{\varepsilon \to 0^{+}} \left(m \eval{\dot{y}}{T - \varepsilon}{T + \varepsilon} + \underbrace{b \eval{y}{T - \varepsilon}{T+\varepsilon}}_{0 \text{ if $y$ continuous}} + \underbrace{k \int_{T-\varepsilon}^{T+\varepsilon} y \d{t}}_{0 \text{ if $y$ finite}}\right) = \underbrace{\lim_{\varepsilon \to 0^{+}} \int_{T-\varepsilon}^{T+\varepsilon} F(t) \d{t}}_{I\text{, Impulse}}
  \]
  We see that, if $y$ is finite and continuous, as $\varepsilon \to 0^{+}$, the only term that survives is the first term:
  \[
    \lim_{\varepsilon \to 0^{+}} \left(m \eval{\dot{y}}{T-\varepsilon}{T+\varepsilon}\right) =I
  \]
  So the velocity, $\dot{y}$, is discontinuous at $t = T$.
  As $\varepsilon \to 0$, only the impulse $I$ matters for subsequent motion.

  \begin{center}
  \begin{tikzpicture}
    \draw[->] (0, 0) node[left] {0} -- (10, 0) node[right] {$t$};
    \draw[->] (0, -2) -- (0, 2) node[right] {$y$};

    \def\kconst{0.2}
    \def\xoffset{4}
    \draw[thick, domain=2.25:10, smooth, samples=100] plot (\x,{2.1 * exp(-\kconst*(\x-\xoffset)) * (-0.1138 * sin((\x-\xoffset) * sqrt(1 - \kconst^2) r) + 0.8 * cos((\x - \xoffset)* sqrt(1 - \kconst^2)r))});

    \draw[thick] (0, 0) -- (2.26, 0) node[below] {$T$};
  \end{tikzpicture}
  \end{center}
\end{example}
\subsection{Dirac Delta Function}
The Dirac delta function formalises the concept of an impulsive force.
Consider the family of functions $D(t; \varepsilon)$ such that:
\[
  \lim_{\varepsilon \to 0} D(t; \varepsilon) = 0\ \forall t\neq0 \text{ and } \int_{-\infty}^{\infty} D(t; \varepsilon) \d{t} = 1
\]
The impulsive force we considered earlier in \cref{impulseExample} is then:
\[
  F(t) = ID(t - T; \varepsilon)
\]

One example of such a $D(t; \varepsilon)$ is:
\[
  D(t;\varepsilon) = \frac{e^{-t^2/\varepsilon^2}}{\varepsilon\sqrt{\pi}}
\]
For smaller $\varepsilon$, the distribution is more concentrated at 0, whereas for larger $\varepsilon$ it is more spread out.
\begin{center}
\begin{tikzpicture}
  \draw[->] (-5, 0) -- (5, 0) node[right] {$t$};
  \draw[->] (0, 0) node[below] {0} -- (0, 4) node[right] {$D(t; \varepsilon)$};

  \foreach \econst in {0.2, 0.6, 1.2} {
    \draw[thick, domain=-5:5, smooth, samples=100] plot (\x,{exp(-(\x/\econst)^2)/(\econst * sqrt(pi))});
  }
\end{tikzpicture}
\end{center}
Note that the area under the curve is always 1 regardless of $\varepsilon$ (See Example Sheet 1, Q14).

This family of functions is not unique, but for any family that has these properties, the limit as $\varepsilon \to 0$ yields the Dirac delta function.
\begin{definition}[Dirac Delta Function]
  The \textit{Dirac delta function} is defined by:
  \[
    \delta(t) = \lim_{\varepsilon \to 0} D(t; \varepsilon)
  \]
\end{definition}
\begin{remark}[Note]
  This is technically a \textit{distribution} not a function and only ``makes sense'' when used inside of an integral.
\end{remark}
\subsubsection{Properties}
\begin{enumerate}
  \item $\delta(t) = 0\ \forall t\neq 0$
  \item $\int_{-\infty}^{\infty} \delta(t) \d{t} = 1$
  \item For all functions $g(t)$ that are continuous at $t = 0$:
    \begin{align*}
      \int_{-\infty}^{\infty} g(t)\delta(t) \d{t} &= \lim_{\varepsilon \to 0} \int_{-\infty}^{\infty} g(t)D(t; \varepsilon) \d{t} \\
                                                  &=g(0) \lim_{\varepsilon \to 0} \int_{-\infty}^{\infty} D(t; \varepsilon) \d{t} \\
                                                  &=g(0)
    \end{align*}
    This is known as the \textit{sampling property}.
\end{enumerate}
More generally, for a function $g(t)$ continuous at $t = t_0$:
\[
  \int_{a}^{b} g(t)\delta(t - t_0) \d{t} = \begin{cases}
  g(t_0) & \text{ if }  a < t_0 < b \\
  0 & \text{ if $t_0 < a$ or $t_0 > b$} \\
  \text{undefined} & \text{ if $t_0 = a$ or $t_0 = b$}
  \end{cases}
\]
where $b > a$.
\subsection{Delta Function Forcing}
Consider a 2nd order linear ODE with non-constant coefficients and forcing term $\delta(t)$:
\[
  y'' + p(x)y' + q(x)y = \delta(x)
\]
where $p, q$ are assumed to be continuous and $y$ is continuous at $x = 0$.
For $x < 0$ and $x > 0$, we just have the homogenous equation:
\[
  y'' + p(x)y' + q(x)y = 0
\]
But we have a discontinuity in $y'$ at $x = 0$.
Integrating from $-\varepsilon$ to $\varepsilon$ and then taking the limit as $\varepsilon \to 0^{+}$ yields:
\[
  \lim_{\varepsilon \to 0^{+}} (\eval{y'}{-\varepsilon}{\varepsilon}) + p(0) \underbrace{\lim_{\varepsilon \to 0^{+}} (\eval{y}{-\varepsilon}{\varepsilon})}_{0 \text{ if $y$ continuous}} + \underbrace{\lim_{\varepsilon \to 0^{+}} \left(\int_{-\varepsilon}^{\varepsilon} q(x)y \d{x}\right)}_{0 \text{ if $y$ finite}} = 1
\]
So we then have:
\[
  \lim_{\varepsilon \to 0^{+}} \eval{y'}{-\varepsilon}{\varepsilon} = 1
\]
This is known as the \textit{jump condition}.
\begin{remark}[Note]
  The continuity of $y$ at $x = 0$ is required otherwise $y'$ is undefined at $x = 0$.
  We would have $y' \propto \delta(x)$ with $y''$ being even worse behaved.
  (Further discussion on this later)
\end{remark}
In general, the highest-order term in the ODE ``addresses'' the delta function forcing as all the lower order terms must be finite (else we would run into the issues described above) so will vanish in the limit.
\begin{example}
  Find $y(x)$ for $0 \leq x \leq \pi$, where:
  \[
    y'' - y = 3\delta\left(x - \frac{\pi}{2}\right)
  \]
  with $y(0) = 0$ and $y(\pi) = 0$.

  For $0 \leq x < \frac{\pi}{2}$, $y'' - y= 0$ so:
  \[
    y = \alpha e^{x} + \beta e^{-x}
  \]
  Applying $y(0) = 0$ we see that $\alpha = - \beta$ thus $y = A \sinh x$ for some constant $A$.

  For $\frac{\pi}{2} < x \leq \pi$, we have the same DE but a different boundary condition, $y(\pi) = 0$:
  \[
    \alpha e^{\pi} + \beta e^{-\pi} = 0 \implies \beta = - \alpha e^{2\pi}
  \]
  Therefore:
  \[
    y = \alpha(e^{x} - e^{2\pi - x}) = -\alpha e^{\pi}(e^{\pi - x} - e^{-(\pi - x)}) = C\sinh(\pi - x)
  \]
  for some constant $C$.

  For $y$ to be continuous at $\pi / 2$, we need to join the solutions at $x = \frac{\pi}{2}$.
  \[
    A\sinh(\pi/2) = C\sinh(\pi/2) \implies A = C
  \]

  Now applying the jump condition:
  \begin{align*}
    \lim_{\varepsilon \to 0^{+}} \eval{y'}{\pi/2 - \varepsilon}{\pi/2 + \varepsilon} &= 3 \\
    -A\cosh(\pi/2) - A\cosh(\pi/2) &= 3 \implies A  = C = -\frac{3}{2\cosh(\pi/2)}
  \end{align*}
  So the final solution is:
  \[
    y(x) = \begin{cases}
    -\frac{3\sinh(x)}{2\cosh(\pi/2)} & \text{ if } 0 \leq x \leq \frac{\pi}{2} \\
    -\frac{3\sinh(\pi - x)}{2\cosh(\pi/2)} & \text{ if } \frac{\pi}{2} \leq x \leq \pi
    \end{cases}
  \]

  From the plot, we can see that $y$ is continuous at $\pi/2$ but $y'$ is not.
  \begin{center}
  \begin{tikzpicture}[scale=1.5]
    \draw[->] (0, 0) node[left] {0} -- (7, 0) node[right] {$t$};
    \draw[->] (0, -3) -- (0, 0.5) node[right] {$y$};

    \draw[dashed] (pi, 0) node[above] {$\frac{\pi}{2}$} -- (pi, -2.6);
    \node[above] at (2*pi, 0) {$\pi$};

    \pgfmathsetmacro{\Aconst}{-(3 / 2)*cosh(pi/2) * 0.3}

    \draw[thick, domain=0:pi, smooth, samples=100] plot (\x,{\Aconst * sinh(\x * 0.5)});
    \draw[thick, domain=pi-0.001:2*pi, smooth, samples=100] plot (\x,{\Aconst * sinh(pi -\x * 0.5)});
  \end{tikzpicture}
  \end{center}
\end{example}
\subsection{Heaviside Step Function}
\begin{definition}[Heaviside Step Function]
  The \textit{Heaviside} step function is defined by:
  \[
    H(x) = \int_{-\infty}^{x} \varepsilon(x') \d{x'}
  \]
\end{definition}
From the properties of the Dirac delta, it follows that:
\[
  H(x) = \begin{cases}
  0 & \text{ if } x < 0 \\
  1 & \text{ if } x > 0 \\
  \text{undefined} & \text{ if } x = 0
  \end{cases}
\]
From the fundamental theorem of calculus (\cref{FTC}), we also see that $H'(x) = \varepsilon(x)$.
This makes sense as it is constant everywhere apart from 0 where it suddenly jumps up to 1.
When plotted, $H(x)$ looks like a step:
\begin{center}
\begin{tikzpicture}[scale=1.3]
  \draw[->] (-4, 0) -- (4, 0) node[right] {$x$};
  \draw[->] (0, 0) node[below] {0} -- (0, 3) node[right] {$H(x)$};

  \draw[very thick] (-4, 0) -- (0, 0);
  \draw[very thick] (0, 2) node[left] {1} -- (4, 2);

  \filldraw[fill=white, draw=black] (0, 2) circle (1.3pt);
  \filldraw[fill=white, draw=black] (0, 0) circle (1.3pt);
\end{tikzpicture}
\end{center}
\end{document}
