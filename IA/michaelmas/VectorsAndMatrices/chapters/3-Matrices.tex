\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Matrices}
\section{Linear Maps}
\begin{definition}[Linear Map]
  Consider two vector spaces $V$ and $W$.
  A \textit{linear map} is a function $T: V \to W$ such that:
  \[
    T(\lambda\vec{x} + \mu\vec{y}) = \lambda T(\vec{x}) + \mu T(\vec{y})
  \]
  for all $\vec{x}, \vec{y} \in V$ and all scalars $\lambda, \mu \in \R \text{ or }\C$ depending on if $V$ and $W$ are real or complex vector spaces.
\end{definition}
\begin{definition}[Domain and Codomain]
  For a linear map $T: V \to W$, $V$ is called the \textit{domain} and $W$ is called the \textit{codomain}.
\end{definition}
\begin{definition}[Image]
  The \textit{image} of a linear map $T$ is:
  \[
    \im T = \{\vec{x}' \in W : \vec{x}' = T(\vec{x}) \text{ for some } \vec{x} \in V\}
  \]
  If $\vec{x}' = T(\vec{x})$ then $\vec{x}'$ is the \textit{image} of $\vec{x}$ under $T$.
\end{definition}
\begin{definition}[Kernel]
  The \textit{kernel} of a linear map $T$ is:
  \[
    \ker T = \{ \vec{x} \in V : T(\vec{x}) = \vec{0}\}
  \]
  If $\vec{x} \in V$ is such that $T(\vec{x}) = \vec{0}$ then we say that $\vec{x}$ belongs to the \textit{kernel} of $T$.
\end{definition}
\begin{proposition}
  For a linear map $T: V \to W$:
  \begin{enumerate}
    \item $\im T$ is a subspace of $W$
    \item $\ker T$ is a subspace of $V$
  \end{enumerate}
\end{proposition}
\begin{proof}
\begin{enumerate}
  \item Let $\vec{u}', \vec{v}' \in \im T$.
    Therefore there exists, $\vec{u}, \vec{v} \in V$ such that $T(\vec{u}) = \vec{u}'$ and $T(\vec{v}) = \vec{v}'$.
    For any scalars $\lambda, \mu$ we have that $\lambda \vec{u} + \mu \vec{v} \in V$ so $T(\lambda \vec{u} + \mu \vec{v}) \in \im T$.
    By linearity, $T(\lambda \vec{u} + \mu \vec{v}) = \lambda \vec{u}' + \mu \vec{v}'$.
    Thus $\lambda \vec{u}' + \mu \vec{v}' \in \im T$ and $\im T \subseteq W$ so $\im T$ is a subspace of $W$.
  \item Let $\vec{u}, \vec{v} \in \ker T$. Therefore $T(\vec{u}) = T(\vec{v}) = \vec{0}$.
    Consider for any scalars $\lambda, \mu$:
    \[
      T(\lambda \vec{u} + \mu \vec{v}) = \lambda T(\vec{u}) + \mu T(\vec{v}) = \vec{0}
    \]
    Thus $\lambda \vec{u} + \mu \vec{v} \in \ker T$ and $\ker T \subseteq V$ so $\ker T$ is a subspace of $V$.
\end{enumerate}
\end{proof}
\begin{definition}[Rank]
  For a linear map $T$ we define the \textit{rank} to be the dimension of the image. That is:
  \[
    \rank T = \dim (\im T)
  \]
\end{definition}
\begin{definition}[Nullity]
  For a linear map $T$ we define the \textit{nullity} to be the dimension of the kernel. That is:
  \[
    \nullity T = \dim (\ker T)
  \]
\end{definition}
\begin{remark}
  If we have a map $T: V \to W$ then $\nullity T = \dim(\ker T) \leq \dim V$ as the kernel is a subspace of $V$ and $\rank T = \dim(\im T) \leq \dim(W)$ as the image is a subspace of $W$.
\end{remark}
\begin{example}
  \begin{enumerate}
    \item \textbf{Zero linear map -} $T: V \to W$ with $\vec{x} \mapsto \vec{0}$.
      Then we have $\im T = \{\vec{0}\}$ and $\ker T = V$.
    \item \textbf{Identity map -} $T: V \to V$ with $\vec{x} \mapsto \vec{x}$.
      Then we have $\im T = V$ and $\ker T = \{\vec{0}\}$.
    \item Consider $T: \R^2 \to \R^2$ and $\vec{x}' = T(\vec{x})$ with:
      \[
        x'_1 = 2x_1 + x_2 \text{ and } x'_2 = x_1 - 3x_2
      \]
      Then:
      \[
        \begin{pmatrix}
        x'_1 \\
        x'_2 \\
        \end{pmatrix} =
        x_1 \begin{pmatrix}
        2 \\
        1 \\
        \end{pmatrix} +
        x_2 \begin{pmatrix}
        1 \\
        -3 \\
        \end{pmatrix}
      \]
      Since $(2, 1)$ and $(1, -3)$ are linearly independent:
      \[
        \im T = \Span\left\{
        \begin{pmatrix}
        2 \\
        1 \\
        \end{pmatrix},
        \begin{pmatrix}
        1 \\
        -3 \\
        \end{pmatrix}
        \right\} =
        \left\{
        \lambda\begin{pmatrix}
        2 \\
        1 \\
        \end{pmatrix} +
        \mu\begin{pmatrix}
        1 \\
        -3 \\
        \end{pmatrix},\ \lambda, \mu \in \
        \right\}
      \]
      Furthermore, $2x_1 + x_2 = 0$ and $x_1 - 3x_2 = 0 \implies x_1 = x_2 = 0$ so:
      \[
        \ker T = \left\{\begin{pmatrix}
        0 \\
        0 \\
        \end{pmatrix}\right\}
      \]
  \end{enumerate}
\end{example}
\subsubsection{Linear Combinations}
Consider linear maps $T, S: V \to W$.
Then $\alpha T + \beta S: V \to W$ is also a linear map defined by:
\[
  (\alpha T + \beta S)(\vec{x}) = \alpha T(\vec{x}) + \beta S(\vec{x})
\]
for $\alpha, \beta \in \R \text{ or } \C$.
\subsubsection{Composition}
Take linear maps $T: V \to W$ and $S: U \to V$.
Then $U \xrightarrow{S} V \xrightarrow{T} W$ so $T \circ S: U \to W$.
This composition is also a linear map, defined by:
\[
  (T \circ S)(\vec{x}) = T(S(\vec{x}))
\]
\begin{theorem}[Rank-nullity Theore]
  For a linear map $T: V \to W$ then:
  \[
    \label{rankNullity}
    \rank T + \nullity T = \dim V
  \]
\end{theorem}
\begin{proof}
\nonexaminable
Let $n$ be $\dim V$ and $m$ be $\nullity T$.
\[
  \nullity T = \dim (\ker T) \leq \dim V \iff m \leq n
\]
So we only need to consider $m \leq n$.
\begin{proofcases}
  \begin{case}{$m = n$}
    Then $\dim(\ker T) = \dim V$ and since $\ker T$ is a subspace of $V$, we must have $\ker T = V$.
    Therefore $\im T = \{\vec{0}\}$.
    So $\rank T + \nullity T = \dim(\im T) + \dim(\ker T) =0 + n = n$.
  \end{case}
  \begin{case}{$m < n$}
    Let $\{\vec{e}_1, \ldots, \vec{e}_m\}$ be a basis of the kernel of T.
    So $T(\vec{e}_i) = \vec{0}$ for all $i = 1, \ldots, m$.
    We can now extend $\{\vec{e}_1, \ldots, \vec{e}_m\}$ to form a basis of $V$.
    Since $\dim V = n$ the basis must have $n$ elements so:
    \[
      \{\vec{e}_1, \ldots, \vec{e}_m, \vec{e}_{m+1}, \ldots, \vec{e}_n\}
    \]
    We now need to show that $B = \{T(\vec{e}_{m + 1}), \ldots, T(\vec{e}_n)\}$ is a basis of $\im T$.
    \begin{enumerate}
      \item
      To show that $B$ spans $\im T$, take an element $\vec{y} \in \im T$.
      Then by definition, there exists $\vec{x} \in V$ such that $T(\vec{x}) = \vec{y}$.
      Since $\vec{x} \in V$, we can write:
      \[
        \vec{x} = \alpha_1 \vec{e}_1 + \cdots + \alpha_n \vec{e}_n
      \]
      Applying $T$ to this yields:
      \begin{align*}
        \vec{y} = T(\vec{x}) &= T(\alpha_1 \vec{e}_1 + \cdots + \alpha_n \vec{e}_n) \\
                             &= \alpha_1 T(\vec{e}_1) + \cdots \alpha_n T(\vec{e}_n) \\
                             &= \alpha_{m + 1} T(\vec{e}_{m + 1}) + \cdots + \alpha_n T(\vec{e}_n)
      \end{align*}
      Therefore, any element in $\im T$ can be written as a linear combination of vectors from $B$, thus $B$ spans $\im T$.
      \item
      To show that $B$ is linearly independent, consider:
      \[
        \alpha_{m + 1} T(\vec{e}_{m+1}) + \cdots + \alpha_n T(\vec{e}_n) = \vec{0}
      \]
      We now need to show that all of the coefficients are 0.
      By linearity we can write this as:
      \[
        T(\underbrace{\alpha_{m + 1}\vec{e}_{m + 1} + \cdots + \alpha_n \vec{e}_n}_{\vec{x}}) = \vec{0}
      \]
      So $\vec{x} \in \ker T$ and the basis of $\ker T$ is $\{\vec{e}_1, \ldots, \vec{e}_n\}$.
      Therefore there exists $\alpha_1, \cdots \alpha_m$ such that:
      \[
        \vec{x} = \alpha_1 \vec{e}_1 + \cdots + \alpha_m \vec{e}_m
      \]
      We can now equate our two expressions for $\vec{x}$ to get:
      \[
        \alpha_1 \vec{e}_1 + \cdots \alpha_m \vec{e}_m - \alpha_{m + 1}\vec{e}_{m + 1} - \cdots - \alpha_n \vec{e}_n = \vec{0}
      \]
      However $\{\vec{e}_1, \ldots, \vec{e}_m, \vec{e}_{m + 1}, \ldots, \vec{e}_n\}$ is a basis of $V$.
      Since the elements of a basis must be linearly independent, all the coefficients must be 0.
      Thus:
      \[
        \alpha_1 = \cdots = \alpha_m = \alpha_{m + 1} = \cdots = \alpha_n = 0
      \]
      So $B$ is linearly independent.
    \end{enumerate}
    Therefore $B$ is a basis of $\im T$ and so $\rank T = \dim(\im T) = n - m$ as $B$ has $n - m$ elements.
    Thus:
    \[
      \rank T + \nullity T = n - m + m = n = \dim V
    \]
  \end{case}
\end{proofcases}
\end{proof}
\begin{example}
  \begin{enumerate}
    \item For the zero map, $\nullity T = \dim V$ and $\rank T = 0$ so $\nullity T + \rank T = \dim V$.
    \item For the identity map, $\rank T = \dim V$ and $\nullity T = 0$ so $\nullity T + \rank T = \dim V$.
  \end{enumerate}
\end{example}
\section{Matrices as Linear Maps}
Let $M$ be a matrix with entries $M_{i j} \in \R$, where $i$ labels rows and $j$ labels columns.
Then the map defined by $T: \R^{n} \to \R^{n}$, $T(\vec{x}) = \vec{x}' = M\vec{x}$ for $\vec{x}, \vec{x}' \in \R^{n}$ with $x'_i = M_{i j}x_j$, is a linear map.
\begin{example}
  Given $n = 2$:
  \[
    M = \begin{pmatrix}
    M_{1 1} & M_{1 2} \\
    M_{2 1} & M_{2 2} \\
    \end{pmatrix}
  \]
  so we have:
  \[
    \begin{pmatrix}
    x'_1 \\
    x'_2 \\
    \end{pmatrix}
    =
    \begin{pmatrix}
    M_{1 1} & M_{1 2} \\
    M_{2 1} & M_{2 2} \\
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\
    x_2 \\
    \end{pmatrix}
    =
    \begin{pmatrix}
    M_{1 1}x_1 + M_{1 2}x_2 \\
    M_{2 1}x_1 + M_{2 2}x_2 \\
    \end{pmatrix}
  \]
\end{example}
We can also consider the rows and columns of $M$ as vectors in $\R^{n}$.
We can denote the rows as $\vec{R}_i \in \R^{n}$ and the columns as $\vec{C}_i \in \R^{n}$:
\[
  M = \begin{pmatrix}
  \leftarrow \vec{R}_1 \rightarrow \\
  \cdots \\
  \leftarrow \vec{R}_n \rightarrow
  \end{pmatrix}
  =
  \begin{pmatrix}
  \uparrow & & \uparrow  \\
  \vec{C}_1 & \cdots & \vec{C}_n  \\
  \downarrow &  & \downarrow \\
  \end{pmatrix}
\]
The components are then related in the following form:
\[
  M_{i j} = (\vec{C}_j)_i = (\vec{R}_i)_j
\]
If $\{\vec{e}_1, \ldots, \vec{e}_n\}$ is the standard basis of $\R^{n}$:
\[
  \vec{e}_i \mapsto \vec{e}_i' = T(\vec{e}_i) = M\vec{e}_i = \vec{C}_i
\]
That is, multiplying a matrix by the $i$-th element of the standard basis extracts the $i$-th column.
\[
  \begin{pmatrix}
  M_{1 1} & \cdots & M_{1 n} \\
  \vdots & \ddots & \vdots \\
  M_{n 1} & \cdots & M_{m n} \\
  \end{pmatrix}
  \vec{e}_i = \vec{C}_i
\]
Since $T$ is a linear map for any $\vec{x} \in \R^{n}$:
\[
  \vec{x} = \sum_{i} x_i \vec{e}_i \mapsto T\left(\sum_{i} x_i \vec{e}_i \right) = \sum_{i} x_i \vec{C}_i
\]
Thus $\vec{x} \mapsto x_i \vec{C}_i$.
This means the image of $\vec{x}$ under $T$ a linear combination of the columns of the matrix.
Therefore, $\im T = \im M = \Span\{\vec{C}_1, \ldots, \vec{C}_n\}$.

We can also determine the kernel of $T$ using its matrix representation.
Consider an element of the image, $\vec{x}'$, of $\vec{x}$ under $T$:
\[
  x_i' = M_{i j}x_j = (\vec{R}_i)_j x_j = \vec{R}_i \cdot \vec{x}
\]
so
\[
  \vec{x}' = \begin{pmatrix}
  x_i' \\
  \vdots \\
  x_n' \\
  \end{pmatrix}
  =
  \begin{pmatrix}
  \vec{R}_i \cdot x \\
  \vdots \\
  \vec{R}_n \cdot x \\
  \end{pmatrix}
\]
If $\vec{x} \in \ker T$ then $\vec{x}' = \vec{0}$ so $\vec{R}_1 \cdot \vec{x} = \cdots = \vec{R}_n \cdot \vec{x} = 0$.
Therefore the kernel of $T$ is all vectors that are orthogonal to all rows of the matrix:
\[
  \ker T = \ker M = \{\vec{x} \in \R^{n}: \vec{R}_i \cdot \vec{x} = \vec{0},\ \forall i = 1, \ldots n\}
\]
This is a subspace of $\R^{n}$ as a linear combination of such vectors would also satisfy the above property and therefore would also be in the subspace.
\begin{remark}[Summary]
  \begin{itemize}
    \item $\im M$ is the span of the columns of $M$.
    \item $\ker M$ is the subspace orthogonal to all rows of $M$.
  \end{itemize}
\end{remark}
\begin{example}
  \begin{enumerate}
    \item For $T: \R^{n} \to \R^{n}$, the zero map has representation $\vec{x}' = O\vec{x}$ where $O_{ij} = 0\ \forall i, j$.
      This matrix is called the \textit{zero matrix}.
    \item For $T: \R^{n} \to \R^{n}$, the identity map has representation $\vec{x}' = I\vec{x}$ where $I_{i j} = \delta_{i j}$.
      This matrix is called the \textit{identity matrix}.
    \item For $T: \R^3 \to \R^3$, $\vec{x}' = M \vec{x}$ given by:
      \begin{align*}
        x_1' &= 3x_1 + x_2 + 5x_3 \\
        x_2' &= -x_1 + 2x_3 \\
        x_3' &= 2x_1 + x_2 + 3x_3
      \end{align*}
      This can be converted into matrix form:
      \[
        M = \begin{pmatrix}
        3 & 1 & 5 \\
        -1 & 0 & -2 \\
        2 & 1 & 3 \\
        \end{pmatrix}
      \]
      Since $\im(T)$ is the span of the columns:
      \[
        \im T = \Span\{\vec{C}_1, \vec{C}_2, \vec{C}_3\} = \Span\{\vec{C}_1, \vec{C}_2\}
      \]
      As $\vec{C}_3 = 2\vec{C}_1 - \vec{C}_2$.
      Since $\vec{C}_1, \vec{C}_2$ are linearly independent, $\rank T = \dim(\im T) = 2$.
      From \cref{rankNullity} (rank nullity theorem), we expect $\nullity T = 1$.

      To find $\ker T$ we need to find the subspace orthogonal to all rows.
      \[
        \vec{R}_2 \times \vec{R}_3 = \begin{vmatrix}
        \uvec{i} & \uvec{j} & \uvec{k} \\
        -1 & 0 & -2 \\
        2 & 1 & 3 \\
        \end{vmatrix} =
        \begin{pmatrix}
        2 \\
        -1 \\
        -1 \\
        \end{pmatrix}
      \]
      This is also orthogonal to $\vec{R}_1$ thus:
      \[
        \ker T = \ker M = \Span\left\{
        \begin{pmatrix}
        2 \\
        -1 \\
        -1 \\
        \end{pmatrix}\right\}
      \]
      Since this has dimension 1, $\nullity T = 1$, as expected.
  \end{enumerate}
\end{example}
\section{Geometric Examples\texorpdfstring{ in $\R^2$ and $\R^3$}{}}
\subsection{Examples in \texorpdfstring{$\R^2$}{2D}}
\subsubsection{Rotations}
Consider $\theta$ such that $-\pi < \theta \leq \pi$, then a rotation by angle $\theta$ anticlockwise in $\R^2$ can be represented by:
\[
  \Rot(\theta) = \begin{pmatrix}
  \cos \theta & -\sin \theta \\
  \sin \theta & \cos \theta \\
  \end{pmatrix}
\]
Note that $\det(\Rot(\theta)) = \cos^2 \theta + \sin^2 \theta = 1$.
\begin{center}
\begin{tikzpicture}[scale=2.5,>=latex]
\draw[->, thick] (-0.2,0) -- (1.5,0) node[right] {$x$};
\draw[->, thick] (0,-0.2) -- (0,1.2) node[above] {$y$};

\coordinate (A) at (1, 0.5);
\coordinate (B) at ({cos(30)*1 - sin(30)*0.5}, {sin(30)*1 + cos(30)*0.5});

\draw[->, very thick, gray!70] (0,0) -- (A) node[midway, below right] {$\mathbf{v}$};

\draw[->, very thick, black] (0,0) -- (B) node[midway, above left] {$\mathbf{v}'$};

\draw[-, black] (0.3,0.15) arc[start angle=30, end angle=60, radius=0.33];
\node at (0.35,0.3) {$\theta$};
\end{tikzpicture}
\end{center}
\subsubsection{Reflections}
Consider $\theta$ such that $-\pi < \theta \leq \pi$, then reflection in the line with angle $\frac{\theta}{2}$ anticlockwise from the $x$-axis (i.e. $y = x\tan(\theta/2)$) in $\R^2$ is represented by:
\[
  \Reflect(\theta) = \begin{pmatrix}
  \cos \theta & \sin \theta \\
  \sin \theta & -\cos \theta \\
  \end{pmatrix}
\]
Note that $\det(\Reflect(\theta)) = -\cos^2 \theta - \sin^2 \theta = -1$.
\begin{center}
\begin{tikzpicture}[scale=2.5,>=latex]
\draw[->, thick] (-0.2,0) -- (1.5,0) node[right] {$x$};
\draw[->, thick] (0,-0.2) -- (0,1.2) node[above] {$y$};

\coordinate (A) at (1, 0.5);
\coordinate (B) at (0.5, 1);

\draw [black, dashed] (0, 0) -- (1.2, 1.2);
\draw[->, very thick, gray!70] (0,0) -- (A) node[midway, below right] {$\mathbf{v}$};

\draw[->, very thick, black] (0,0) -- (B) node[midway, above left] {$\mathbf{v}'$};

\draw[-, black] (0.3,0) arc[start angle=0, end angle=45, radius=0.3];
\node at (0.35,0.18) {$\frac{\theta}{2}$};
\end{tikzpicture}
\end{center}
\subsubsection{Properties}
\begin{enumerate}
  \item $\Rot(\theta)\Rot(\phi) = \Rot(\theta + \phi)$
  \item $\Reflect(\theta)\Reflect(\phi) = \Rot(\theta - \phi)$
  \item $\Rot(\theta)\Reflect(\phi) = \Reflect(\phi + \theta)$
  \item $\Reflect(\phi)\Rot(\theta) = \Reflect(\phi - \theta)$
\end{enumerate}
\subsection{Examples in \texorpdfstring{$\R^3$}{3D}}
\subsubsection{Rotation along a basis}
Rotation of angle $\theta$ with axis $\vec{e}_1, \vec{e}_2, \vec{e}_3$:
\begin{align*}
  \Rot_{\vec{e}_1}(\theta) &= \begin{pmatrix}
  1 & 0 & 0 \\
  0 & \cos \theta & -\sin \theta \\
  0 & \sin \theta & \cos \theta \\
  \end{pmatrix} \\
  \Rot_{\vec{e}_2}(\theta) &= \begin{pmatrix}
  \cos \theta & 0 & \sin \theta \\
  0 & 1 & 0 \\
  - \sin \theta & 0 & \cos \theta \\
  \end{pmatrix} \\
  \Rot_{\vec{e}_3}(\theta) &= \begin{pmatrix}
  \cos \theta & -\sin \theta & 0 \\
  \sin \theta & \cos \theta & 0 \\
  0 & 0 & 1 \\
  \end{pmatrix}
\end{align*}
\begin{remark}[Warning]
  These look like they all have the same format, however, for $\vec{e}_2$, the negative is on the other $\sin \theta$.
\end{remark}
\subsubsection{Rotation about a unit vector}
Consider a rotation of $\theta$ anticlockwise about a unit vector $\vec{n}$.
For $x \in \R^{3}$
\[
  \vec{x}' = R\vec{x},\ \vec{x}_i' = R_{i j}x_j
\]
We can write:
\[
  \vec{x}' = (\cos \theta)\vec{x} + (1 - \cos \theta)(\vec{n} \cdot \vec{x})\vec{n} + (\sin \theta)(\vec{n} \times \vec{x})
\]
This has matrix representation, $\Rot(\theta, \vec{n})$, given by:
\[
  R_{i j} = \delta_{i j}\cos\theta + (1 - \cos \theta)n_in_j - (\sin \theta)\levi_{i j k}n_k
\]

To show why this is a rotation of $\vec{x}$ about $\vec{n}$, we can first split $\vec{x}$ into a component parallel to $\vec{n}$ and a component perpendicular to $\vec{n}$:
\[
  \vec{x} = \vec{x}_{\parallel} + \vec{x}_{\perp}
\]
where $\vec{x}_{\parallel} = (\vec{x} \cdot \vec{n})\vec{n}$ and $\vec{x}_{\perp}$ such that $\vec{n} \cdot \vec{x}_{\perp} = 0$.

After applying $R$, $\vec{x}_{\parallel}$ is unchanged as it is parallel to the axis of rotation, so $\vec{x}_{\parallel}' = \vec{x}_{\parallel}$.
To find the image of $\vec{x}_{\perp}$, consider the plane passing through the origin orthogonal to $\vec{n}$ and vector $\vec{n} \times \vec{x}$:
\begin{center}
\begin{tikzpicture}[scale=2.5,>=latex]
\draw[->, thick] (0,0) -- (1.414,0) node[right] {$\vec{x}_{\perp}$};
\draw[->, thick] (0,0) -- (0,1.414) node[above] {$\vec{n} \times \vec{x}$};

\draw[->, very thick, gray!70] (1, 0) -- (1, 1) node[midway, right, black] {$(\sin \theta)(\vec{n} \times \vec{x})$};
\draw[->, very thick, gray!70] (0, 0) -- (1, 0) node[midway, below, black] {$(\cos \theta)\vec{x}_{\perp}$};
\draw[->, very thick] (0,0) -- (1,1) node[midway, below right] {$\vec{x}_{\perp}'$};

\draw[black] (0.3,0) arc[start angle=0, end angle=45, radius=0.3];
\node at (0.35,0.15) {$\theta$};
\end{tikzpicture}
\end{center}
We get $(\sin \theta)(\vec{n} \times \vec{x})$ as $|\vec{x}_{\perp}| = |\vec{n} \times \vec{x}|$ (consider the projection of $\vec{x}$ onto the plane orthogonal to $\vec{n}$) and $\vec{n} \times \vec{x}$ is orthogonal to $\vec{x}_{\perp}$ and lies in the plane.

Therefore:
\[
  \vec{x}_{\perp}' = (\cos \theta)\vec{x}_{\perp} + (\sin \theta)(\vec{n} \times \vec{x})
\]
Since $\vec{x} = \vec{x}_{\parallel} + \vec{x}_{\perp} \implies \vec{x}_{\perp} = \vec{x} - (\vec{x} \cdot \vec{n})\vec{n}$.
So we can write:
\[
  \vec{x}_{\perp}' = (\cos \theta)\vec{x} - (\cos \theta)(\vec{x} \cdot \vec{n})\vec{n} + (\sin \theta)(\vec{n} \times \vec{x})
\]
As it is a linear map, the image of $\vec{x}$ is just the sum of the images of its components so:
\[
  \vec{x}' = \vec{x}_{\parallel}' + \vec{x}_{\perp}' = (\cos \theta)\vec{x} + (1 - \cos\theta)(\vec{x} \cdot \vec{n})\vec{n} + (\sin \theta)(\vec{n} \times \vec{x})
\]
\subsubsection{Reflections}
Reflections in a plane passing through the origin with unit normal vector $\vec{n}$ are represented by:
\[
  \vec{x}' = H\vec{x} = \vec{x} - 2(\vec{x} \cdot \vec{n})\vec{n}
\]
Note that $(\vec{x} \cdot \vec{n})\vec{n}$ is the vector projection of $\vec{x}$ onto $\vec{n}$.
This can be seen in the following diagram:
\begin{center}
\begin{tikzpicture}[scale=2.5,>=latex]
\draw[thick, dotted] (-1.5,0) -- (1.5,0) node[right] {$\text{Plane}$};
\draw[->, thick] (0,0) -- (0,1) node[above] {$\vec{n}$};

\draw[dashed, gray!70] (1, 1) -- (1, -1);
\draw[->, thick] (1, 0) -- (1, 1) node[midway, right] {$(\vec{x} \cdot \vec{n})\vec{n}$};
\draw[->, very thick] (0,0) -- (1,1) node[midway, below right] {$\vec{x}$};
\draw[->, very thick] (0,0) -- (1,-1) node[midway, below left] {$\vec{x}'$};

\node[below left] at (0, 0) {$O$};
\end{tikzpicture}
\end{center}
This has matrix representation given by:
\[
  \vec{x}_i' = H_{i j}x_j,\ H_{i j} = \delta_{i j} - 2n_in_j
\]
So:
\[
  H = \begin{pmatrix}
  1-2n^{2}_{1} & -2n_1n_2 & -2n_1n_3 \\
  -2n_1n_2 & 1-2n^{2}_{2} & -2n_2n_3 \\
  -2n_1n_3 & -2n_2n_3 & 1-2n^{2}_{3} \\
  \end{pmatrix}
\]
\subsubsection{Dilations}
A dilation with factors $\alpha, \beta, \gamma$ is such that
\[
  \vec{x} = x_1 \vec{e}_1 + x_2 \vec{e}_2 + x_3 \vec{e}_3 \mapsto \vec{x}' = \alpha x_1 \vec{e}_1 + \beta x_2 \vec{e}_2 + \gamma x_3 \vec{e}_m
\]
This has matrix representation:
\[
  \vec{x} \mapsto \vec{x}' = M\vec{x},\
  M = \begin{pmatrix}
  \alpha & 0 & 0 \\
  0 & \beta & 0 \\
  0 & 0 & \gamma \\
  \end{pmatrix}
\]
\subsubsection{Shear}
Given orthogonal unit vectors $\vec{a}$ and $\vec{b}$ ($|\vec{a}| = |\vec{b}| = 1$, $\vec{a} \cdot \vec{b}  = 0$), a shear with parameter $\lambda$ is defined by:
\[
  \vec{x}' = S\vec{x} = \vec{x} + \lambda \vec{a} (\vec{x} \cdot \vec{b})
\]
\begin{center}
\begin{tikzpicture}[scale=2,>=Stealth]
\def\sf{0.5}

\draw[dashed] (0,1) -- (1,1) -- (1, 0);
\draw[->,thick] (0,0) -- (1,0) node[midway,below] {$\vec{a}$};
\draw[->,thick] (0,0) -- (0,1) node[midway,left] {$\vec{b}$};

\begin{scope}[xshift=3cm]
    \coordinate (A) at (0,0);
    \coordinate (B) at (1,0);
    \coordinate (C) at (1+\sf,1);
    \coordinate (D) at (\sf,1);

    \draw[dashed] (B)--(C)--(D);
    \draw[->,dashed] (A) -- (0,1) node[midway,left] {$\vec{b}$};
    \draw[->,thick] (A) -- (B) node[midway,below] {$\vec{a}$};
    \draw[->,thick] (A) -- (D) node[midway,right] {$\vec{b}'$};
\end{scope}

\draw[->,thick] (1.4,0.5) -- (2.6,0.5) node[midway,above]{Shear, $\lambda = 0.5$};
\end{tikzpicture}
\end{center}
This has matrix representation:
\[
  x_i' = S_{i j}x_j,\ S_{i j} = \delta_{i j} + \lambda a_i b_j
\]
Note that $S\vec{a} = \vec{a}$, $S\vec{b} = \vec{b} + \lambda \vec{a}$, and $S\vec{v} = \vec{v}$ for all $\vec{v} \parallel \vec{a}$.

\section{Matrices In General}
\subsection{Definitions}
\begin{definition}[$m \times n$ Matrix]
  Consider a linear map $T: V \to W$ where $\dim V = n$ and $\dim W = m$, and bases:
  \[
    \{\vec{e}_1, \ldots, \vec{e}_n\} \text{ for $V$ and }
    \{\vec{f}_1, \ldots, \vec{f}_m\} \text{ for $W$}
  \]Then $T$ can be represented by the \textit{matrix} $M$, which is an \textit{$m \times n$ array} with entries $M_{i j} \in \R \text{ or }\C$ for rows $i = 1, \ldots, m$ and columns $j = 1, \ldots, n$.

  We define:
  \[
    T(\vec{e}_j) = \sum_{i} M_{i j}\vec{f}_i
  \]
\end{definition}
This ensures that for any $\vec{x} \in V$ and its image $\vec{x}' \in W$ where:
\[
  \vec{x} = \sum_{j} x_j \vec{e}_j,\ \vec{x}' = \sum_{i} x_i' \vec{f}_i
\]
the relation $\vec{x}' = T(\vec{x})$ holds if and only if the components of $\vec{x}'$ are:
\[
  x_i'=\sum_{j} M_{i j}x_j
\]
or more explicitly:
\[
  \begin{pmatrix}
  x_1' \\
  \vdots \\
  x_m' \\
  \end{pmatrix} =
  \begin{pmatrix}
  M_{1 1} & \cdots & M_{1 n} \\
  \vdots & \ddots & \vdots \\
  M_{m 1} & \cdots & M_{m n} \\
  \end{pmatrix}
  \begin{pmatrix}
  x_1 \\
  \vdots \\
  x_n \\
  \end{pmatrix}
\]

This can be seen if we take the image of $\vec{x}$ in terms of the basis for $V$:
\begin{align*}
  T(\vec{x}) &= T(x_j \vec{e}_j) \text{ (Using $\Sigma$ convention)} \\
             &= x_j T(\vec{e}_j) \text{ (As $T$ is a linear map)}\\
             &= M_{i j}x_j \vec{f}_i
\end{align*}
\begin{remark}[Summary]\par
  To summarise, given real or complex vector spaces $V$ and $W$ with $\dim V = n$ and $\dim W = m$,
  once we fix the bases for $V$ and $W$, then:
  \begin{enumerate}
    \item $V$ is identified with either $\R^{n}$ or $\C^{n}$
    \item $W$ is identified with either $\R^{m}$ or $\C^{m}$
    \item $T$ is identified with an $m \times n$ matrix $M$
  \end{enumerate}
\end{remark}
\subsubsection{Addition and Scalar Multiplication}
Consider another linear map $S: V \to W$ represented with a matrix $N$ with respect to the same bases as $T$.
Then, for scalars $\alpha$ and $\beta$ we can consider the linear map $\alpha T + \beta S$ represented by a matrix $\alpha M + \beta N$ with coefficients:
\[
  (\alpha M + \beta N)_{i j} = \alpha M_{i j} + \beta N_{i j}
\]
That is, addition and \textbf{scalar} multiplication in matrices takes place entry by entry.
\begin{remark}
  Since addition takes place entry by entry, the dimensions of both matrices must be equal otherwise the matrices do not \textit{conform under addition}.
\end{remark}
\begin{example}
  Consider $V = M_{2\times2}(\R)$ ($2\times2$ matrices with real entries) and $W = \R^{3}$.
  So $\dim V = 4$ and $\dim W = 3$.

  Consider the linear map:
  \[
    T: V \to W,\ \begin{pmatrix}
    a & b \\
    c & d \\
    \end{pmatrix} \mapsto
    \begin{pmatrix}
    a + b \\
    c \\
    d \\
    \end{pmatrix}
  \]
  We now want to represent $T$ in terms of a matrix $M$.
  Take a basis of $V$:
  \[
    \left\{
    \vec{e}_1 = \begin{pmatrix}
    1 & 0 \\
    0 & 0 \\
    \end{pmatrix},\
    \vec{e}_2 = \begin{pmatrix}
    0 & 1 \\
    0 & 0 \\
    \end{pmatrix},\
    \vec{e}_3 = \begin{pmatrix}
    0 & 0 \\
    1 & 0 \\
    \end{pmatrix},\
    \vec{e}_4 = \begin{pmatrix}
    0 & 0 \\
    0 & 1 \\
    \end{pmatrix}
    \right\}
  \]
  And the standard basis for $W$:
  \[
    \{\vec{f}_1 = (1, 0, 0), \vec{f}_2 = (0, 1, 0), \vec{f}_3 = (0, 0, 1)\}
  \]
  To determine $M$, we find $T(\vec{e}_i)$ for $i = 1, \ldots, 4$.
  \[
    T(\vec{e}_1) = \begin{pmatrix}
    1 \\
    0 \\
    0 \\
    \end{pmatrix},\
    T(\vec{e}_2) = \begin{pmatrix}
    1 \\
    0 \\
    0 \\
    \end{pmatrix},\
    T(\vec{e}_3) = \begin{pmatrix}
    0 \\
    1 \\
    0 \\
    \end{pmatrix},\
    T(\vec{e}_4) = \begin{pmatrix}
    0 \\
    0 \\
    1 \\
    \end{pmatrix}
  \]
  Using the basis we just defined, for $a, b, c, d \in \R$:
  \[
    \begin{pmatrix}
    a & b \\
    c & d \\
    \end{pmatrix} =
    a \begin{pmatrix}
    1 & 0 \\
    0 & 0 \\
    \end{pmatrix} +
    b \begin{pmatrix}
    0 & 1 \\
    0 & 0 \\
    \end{pmatrix} +
    c \begin{pmatrix}
    0 & 0 \\
    1 & 0 \\
    \end{pmatrix} +
    d \begin{pmatrix}
    0 & 0 \\
    0 & 1 \\
    \end{pmatrix}
  \]
  Thus:
  \[
    T\begin{pmatrix}
    a & b \\
    c & d \\
    \end{pmatrix} =
    \begin{pmatrix}
    a + b \\
    c \\
    d \\
    \end{pmatrix} =
    \underbrace{
    \begin{pmatrix}
      1 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & 1 \\
    \end{pmatrix}}_{M}
    \begin{pmatrix}
    a \\
    b \\
    c \\
    d\\
    \end{pmatrix}
  \]
  In this example we identified $V$ with $\R^{4}$, $W$ with $\R^{3}$, and represented $T$ as $M\vec{x}$ where $\vec{x} \in \R^4$ and $M$ is a $3\times4$ matrix.
\end{example}
\subsection{Matrix Multiplication}
Consider linear maps $T: V \to U, S: U \to V$ and compose them.
If $T$ is represented by $M$ and $S$ is represented by $N$ then $T \circ S$ will be represented by $L = MN$.
Let:
\begin{align*}
  \{\vec{e}_1, \ldots, \vec{e}_n\}& \text{ be a basis for $V$, $\dim V = n$} \\
  \{\vec{f}_1, \ldots, \vec{f}_m\}& \text{ be a basis for $W$, $\dim W = m$} \\
  \{\vec{g}_1, \ldots, \vec{g}_l\}& \text{ be a basis for $U$, $\dim U = l$}
\end{align*}
The matrix $L$ has coefficients given by:
\[
  L_{i k} = M_{i j}N_{j k}
\]
Note that $M$ is an $m \times n$ matrix, $N$ is an $n \times l$ matrix and $L$ is an $m \times l$ matrix.
\[
  \begin{pmatrix}
  V_{1 1} & \cdots & V_{1 l} \\
  \vdots & \ddots & \vdots \\
  V_{m 1} & \cdots & V_{m l} \\
  \end{pmatrix} =
  \begin{pmatrix}
  M_{1 1} & \cdots & M_{1 n} \\
  \vdots & \ddots & \vdots \\
  M_{m 1} & \cdots & M_{m n} \\
  \end{pmatrix}
  \begin{pmatrix}
  N_{1 1} & \cdots & N_{1 l} \\
  \vdots & \ddots & \vdots \\
  N_{n 1} & \cdots & N_{n l} \\
  \end{pmatrix}
\]
\begin{remark}[Remarks]
  \begin{itemize}
    \item The number of columns of $M$ must match the number of rows of $N$ otherwise the matrices do not \textit{conform under multiplication}.
    \item $L$ has the same number of rows as $M$ and the same number of columns as $N$.
  \end{itemize}
\end{remark}
We can also define the elements of $L$ using the rows of $M$ and the columns of $N$:
\[
  L_{i k} = (MN)_{i k} = [\vec{R}_i(M)]_j [\vec{C}_k(N)]_j = \vec{R}_i(M) \cdot \vec{C}_k(N)
\]
If we apply $MN$ to a vector $\vec{x} \in U$ we obtain:
\[
  (MN)\vec{x} = M(N\vec{x})
\]
with
\[
  [M(N\vec{x})]_i = M_{i j}(N\vec{x})_j
\]
and thus
\[
  (MN)_{i k}x_k = M_{i j}N_{j k}x_k
\]
\subsubsection{Properties of Matrix Multiplication}
For any three matrices $M, N, L$ that have the correct dimensions to conform, and scalars $\lambda, \mu \in \R \text{ or } \C$:
\begin{enumerate}
  \item $(\lambda M + \mu N)L = \lambda(ML) + \mu(NL)$
  \item $L(\lambda M + \mu N) = \lambda(LM) + \mu(LN)$
  \item $(MN)L = M(NL)$
\end{enumerate}
\begin{remark}[Warning]
  In general, matrix multiplication does \textbf{not commute}!
\end{remark}
\subsection{Matrix Inverses}
Consider three matrices $L\ (n \times m),\ M\ (n \times m)$, and $N\ (m \times n)$.

We say that $L$ is a \textit{left inverse} of $M$ if:
\[
  LN = I \text{ (Identity of size $n \times n$)}
\]
We say that $M$ is a \textit{right inverse} of $N$ if:
\[
  NM = I \text{ (Identity of size $m \times m$)}
\]
If $N$ has both a left and right inverse, then it must be square, that is, $m = n$.
We can also write:
\[
  L = L \underbrace{(NM)}_{I} = \underbrace{(LN)}_{I}M = M
\]
So the left and right inverses are equal.
We then denote both $L$ and $M$ as $N^{-1}$ and call this the \textit{inverse} of $N$ where:
\[
  N N^{-1} = N^{-1} N = I
\]
\begin{remark}
  If $N$ has an inverse then $N$ is a square matrix.
  But, $N$ is square $\centernot\implies$ $N$ has an inverse.
  This is because not all square matrices have inverses, for example, the zero matrix.

  A matrix that has an inverse is called \textit{invertible} or \textit{non-singular}.
\end{remark}
\begin{proposition}
  If $M, N$ are invertible matrices with equal dimensions, then:
  \[
    (MN)^{-1} = N^{-1} M^{-1}
  \]
\end{proposition}
\begin{proof}
  \[
    (N^{-1} M^{-1})MN = N^{-1} (M^{-1} M) N = N^{-1} N = I
  \]
  So $(MN)^{-1} = N^{-1} M^{-1}$.
\end{proof}
\begin{remark}
  $M$ and $N$ must both be square matrices as they are invertible and they must share the same dimensions otherwise we would not be able to take the product $MN$.
\end{remark}
\begin{example}[Inverses of Geometric Transforms]
  \begin{enumerate}
    \item \textbf{Rotation -} For $\Rot(\theta, \vec{n})$ we have:
      \[
        (\Rot(\theta, \vec{n}))^{-1} = \Rot(-\theta, \vec{n})
      \]
    \item \textbf{Shear -} If we fix $\vec{a}, \vec{b}$:
      \[
        S(\lambda)^{-1} = S(-\lambda)
      \]
    \item \textbf{Reflection -} If $H$ is a reflection in a plane with unit normal $\vec{n}$ then:
      \[
        H^{-1} = H
      \]
      So $H$ is \textit{self-inverse}.
  \end{enumerate}
\end{example}
\subsection{Transpose and Hermitian Conjugate}
\label{transposeAndConjugate}
\begin{definition}[]
  If $M$ is an $m \times n$ matrix, then the \textit{transpose} of $M$, denoted $M^{\trans}$, is an $n \times m$ matrix defined by:
  \[
    (M^{\trans})_{i j} = M_{j i}
  \]
\end{definition}
or more explicitly:
\[
  \begin{pmatrix}
  M_{1 1} & \cdots & M_{1 n} \\
  \vdots & \ddots & \vdots \\
  M_{m 1} & \cdots & M_{m n} \\
  \end{pmatrix}^{\trans} =
  \begin{pmatrix}
  M_{1 1} & \cdots & M_{m 1} \\
  \vdots & \ddots & \vdots \\
  M_{1 n} & \cdots & M_{m n} \\
  \end{pmatrix}
\]
\subsubsection{Properties of Transpose}
\begin{enumerate}
  \item $(M^{\trans})^{\trans} = M$
  \item If $\vec{x}$ is a column vector, then $\vec{x}^{\trans}$ is a row vector:
    \[
      \begin{pmatrix}
      x_1 \\
      \vdots \\
      x_n \\
      \end{pmatrix}^{\trans} =
      (x_1, \cdots, x_n)
    \]
  \item $(NM)^{\trans} = M^{\trans}N^{\trans}$
  \item $(\alpha M + \beta N)^{\trans} = \alpha M^{\trans} + \beta N^{\trans}$
\end{enumerate}
\begin{definition}[Symmetric and Antisymmetric]
  If $M$ is a square matrix then $M$ is:
  \begin{itemize}
    \item \textbf{Symmetric -} If $M^{\trans} = M$ or equivalently, $M_{i j} = M_{j i}$
    \item \textbf{Antisymmetric -} If $M^{\trans} = -M$ or equivalently, $M_{i j} = -M_{j i}$
  \end{itemize}
\end{definition}
\begin{definition}[Hermitian Conjugate]
  The \textit{hermitian conjugate} of a matrix $M$ of size $m \times n$ with complex entries is given by:
  \[
    M^{\dag} = \overline{(M^{\trans})}
  \]
  This has coefficients:
  \[
    (M^{\dag})_{i j} = \overline{M_{j i}}
  \]
\end{definition}
\begin{remark}[Notation]
  The notation $\overline{M}$ denotes $M$ with complex conjugated entries.
\end{remark}
\subsubsection{Properties of Hermitian Conjugate}
\begin{enumerate}
  \item $(\alpha M + \beta N)^{\dag} = \overline{\alpha}M^{\dag} + \overline{\beta}N^{\dag}$
  \item $(MN)^{\dag} = N^{\dag} M^{\dag}$
\end{enumerate}
\begin{definition}[Hermitian and Anti-Hermitian]
  If $M$ is square, then $M$ is:
  \begin{itemize}
    \item \textbf{Hermitian -} If $M^{\trans} = M$ or $M_{i j} = \overline{M_{j i}}$
    \item \textbf{Anti-Hermitian -} If $M^{\trans} = -M$ or $M_{i j} = -\overline{M_{j i}}$
  \end{itemize}
  Anti-Hermitian is sometimes called \textit{skew-hermitian}.
\end{definition}
\begin{remark}[Note]
  If the matrices and scalars are all real, the hermitian conjugate just reduces to the regular matrix transpose.
\end{remark}
\subsection{Trace}
\begin{definition}[Trace]
  For any $n \times n$ matrix, its \textit{trace} is defined by:
  \[
    \tr(M) = M_{i i} = M_{1 1} + \cdots M_{n n}
  \]
  It is the sum of the diagonal entries of the matrix.
\end{definition}
\subsubsection{Properties of Trace}
\begin{enumerate}
  \item $\tr(\alpha M + \beta N) = \alpha \tr(M) + \beta \tr(N)$
  \item $\tr(MN) = \tr(NM)$
  \item $\tr(M^{\trans}) = \tr(M)$
  \item $\tr(I) = n$ where $I$ is an $n \times n$ identity matrix.
\end{enumerate}
\subsection{Decomposition of a Square Matrix}
Any real square matrix can be decomposed into a sum of symmetric and antisymmetric parts.
For a matrix $M$ we can write:
\[
  M  = S + A,\ S = \frac{1}{2}(M + M^{\trans}),\ A = \frac{1}{2}(M - M^{\trans})
\]
Note that $S^{\trans} = \frac{1}{2}(M^{\trans} + M) = S$, so $S$ is symmetric and $A^{\trans} = \frac{1}{2}(M^{\trans} - M) = -A$, so $A$ is antisymmetric.

The symmetric part can be further decomposed.
Let:
\[
  T = S - \frac{1}{n}\tr(S)I \quad \text{($I$ of size $n \times n$)}
\]
Note that this means that $\tr(T) = \tr(S) - \frac{\tr(S)}{n}\tr(I) = 0$, so $T$ is called \textit{traceless}.
We also have that $\tr(S) = \tr(M)$ and $\tr(A) = 0$.
Therefore $S$ is decomposed further into:
\[
  S = T + \frac{1}{n}\tr(M)I
\]
Substituting back into the original decomposition yields:
\[
  M = T + A + \frac{1}{n}\tr(M)I
\]
Where $T$ is symmetric and traceless, $A$ is antisymmetric, and $\frac{1}{n}\tr(M)I$ is diagonal with all entries equal, this is called the \textit{isotropic} part.
\subsection{Orthogonal and Unitary Matrices}
\begin{definition}[Orthogonal Matrix]
  A real $n \times n$ matrix $U$ is \textit{orthogonal} if and only if:
  \[
    U^{\trans}U = UU^{\trans} = I
  \]
  or equivalently $U^{-1} = U^{\trans}$.
\end{definition}
This means that the columns and the rows of $U$ are orthonormal.
Equivalently, $U$ is orthogonal if and only if it preserves inner products, that is:
\[
  (U\vec{x}) \cdot (U\vec{y}) = \vec{x} \cdot \vec{y} \quad \forall \vec{x}, \vec{y} \in \R^{n}
\]
and from this, $U$ preserves norms and angles.
\begin{definition}[Unitary Matrix]
  A complex $n \times n$ matrix, is \textit{unitary} if and only if:
  \[
    U^{\dag}U = UU^{\dag} = I
  \]
  or equivalently $U^{-1} = U^{\dag}$.
\end{definition}
Equivalently, $U$ is unitary if and only if it preserves inner products:
\[
  (U\vec{z})^{\dag} \cdot (U\vec{w}) = \vec{z}^{\dag} \vec{w}
\]
\begin{example}[Orthogonal Matrices in $\R^2$]
  Consider a $2 \times 2$ orthogonal matrix $U$ and the canonical basis for $\R^2$:
  \[
    \left\{\begin{pmatrix}
    1 \\
    0 \\
    \end{pmatrix},
    \begin{pmatrix}
    0 \\
    1 \\
    \end{pmatrix}\right\}
  \]
  $U$ preserves norms so:
  \[
    U\begin{pmatrix}
    1 \\
    0 \\
    \end{pmatrix} =
    \begin{pmatrix}
    \cos \theta \\
    \sin \theta \\
    \end{pmatrix}
    \text{ for some $\theta \in \R$}
  \]
  Since $U$ preserves angles and $(1, 0)$ and $(0, 1)$ are orthogonal, their images under $U $must also be orthogonal, thus:
  \[
    U \begin{pmatrix}
    0 \\
    1 \\
    \end{pmatrix} =
    \begin{pmatrix}
    \cos(\theta \pm \pi/2) \\
    \sin(\theta \pm \pi/2) \\
    \end{pmatrix} =
    \pm \begin{pmatrix}
    -\sin \theta \\
    \cos \theta \\
    \end{pmatrix}
  \]
  Hence $U$ is either:
  \[
    U = \Rot(\theta) = \begin{pmatrix}
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta \\
    \end{pmatrix}
    \text{ or }
    U = \Reflect(\theta) = \begin{pmatrix}
    \cos \theta & \sin \theta \\
    \sin \theta &  -\cos \theta \\
    \end{pmatrix}
  \]
\end{example}
\section{Determinants}
Consider a map $\R^{n} \to \R^{n}$ given by a real $n \times n$ matrix $M$:
\[
  \vec{x}' = M\vec{x}
\]
Assume that $M$ is invertible so has inverse $M^{-1}$, then:
\[
  \vec{x} = M^{-1}\vec{x}'
\]
\subsection{Determinants in \texorpdfstring{$\R^2$}{2D}}
Consider:
\[
  M = \begin{pmatrix}
  M_{1 1} & M_{1 2} \\
  M_{2 1} & M_{2 2} \\
  \end{pmatrix} \text{ and }
  \widetilde{M} = \begin{pmatrix}
  M_{2 2} & -M_{1 2} \\
  -M_{2 1} & M_{1 1} \\
  \end{pmatrix}
\]
Note that:
\[
  \widetilde{M}M =
  (M_{1 1}M_{2 2} - M_{1 2}M_{2 1})I
\]
Then we have:
\[
  \vec{x}' = M\vec{x} \implies \widetilde{M}\vec{x}' = \widetilde{M} M \vec{x} = (\det M)\vec{x}
\]
with $\det M = M_{1 1}M_{2 2} - M_{1 2}M_{2 1}$

Therefore, if $\det M \neq 0$, we have:
\[
  M^{-1} = \frac{\widetilde{M}}{\det M}
\]
and so $M$ is invertible.
\begin{remark}
  Note that we can write $\det(M) = [M\vec{e}_1, M\vec{e}_2]$ using the scalar cross product from \cref{scalarCrossProduct}.
  Therefore, $\det M$ is the signed area of the parallelogram spanned by the images of the basis vectors under $M$.
\end{remark}
\subsection{Determinants in \texorpdfstring{$\R^3$}{3D}}
Take a linear map $\vec{x} \mapsto \vec{x}' = M\vec{x}$ for a $3 \times 3$ matrix $M$.
Similarly to in $\R^2$, we want a matrix $\widetilde{M}$ and a scalar $\det M$ such that:
\[
  \widetilde{M}M = (\det M)I
\]
Recall the scalar triple product from \cref{scalarTripleProduct}:
\[
  [\vec{a}, \vec{b}, \vec{c}] = \vec{a} \cdot (\vec{b} \times \vec{c}) = \levi_{i j k} a_i b_j c_k
\]
This gives the signed volume of the parallelepiped defined by the three vectors.

Under the action of a $3 \times 3$ matrix $M$, volumes will be scaled by a factor of $\det M$.
To find this scale factor, we compute the triple product of the images of the basis vectors:
\begin{align*}
  [M\vec{e}_1, M\vec{e}_2, M\vec{e}_3] &= [\vec{C}_1(M), \vec{C}_2(M), \vec{C}_3(M)] \\
                                       &= [M_{i 1}\vec{e}_i, M_{j 2}\vec{e}_j, M_{k 3}\vec{e}_k] \\
                                       &= M_{i 1}M_{j 2}M_{k 3}[\vec{e}_i, \vec{e}_j, \vec{e}_k] \\
                                       &= M_{i 1}M_{j 2}M_{k 3} \levi_{i j k}
\end{align*}
noting that $[\vec{e}_i, \vec{e}_j, \vec{e}_k] = \levi_{i j k} (\vec{e}_i)_i (\vec{e}_j)_j (\vec{e}_k)_k = \levi_{i j k}$.
So under $M$, the parallelepiped defined by the basis vectors has signed volume $M_{i 1}M_{j 2}M_{k 3} \levi_{i j k}$.

We then define $\det M$ in $\R^{3}$ as:
\[
  \det M = M_{i 1}M_{j 2}M_{k 3} \levi_{i j k}
\]
\begin{remark}[Notation]
  We also sometimes use vertical bars to denote the determinant:
  \[
    \det M = \begin{vmatrix}
    M_{1 1} & M_{1 2} & M_{1 3} \\
    M_{2 1} & M_{2 2} & M_{2 3} \\
    M_{3 1} & M_{3 2} & M_{3 3} \\
    \end{vmatrix}
  \]
\end{remark}
To construct $\widetilde{M}$, we need rows of $\widetilde{M}$ such that $\vec{R}_i(\widetilde{M}) \cdot \vec{C}_j(M) = (\det M) \delta_{i j}$ so that $(\widetilde{M}M)_{i j} = (\det M)\delta_{i j}$ and thus $\widetilde{M}M = (\det M)I$.

This means that we need rows so that if they have a different index to the columns, they are orthogonal.
That is, $\vec{R}_1(\widetilde{M})$ needs to be orthogonal to $\vec{C}_2(M)$ and $\vec{C}_3(M)$ and so on.
The following rows have this property:
\begin{align*}
  \vec{R}_1(\widetilde{M}) &= \vec{C}_2(M) \times \vec{C}_3 (M) \\
  \vec{R}_2(\widetilde{M}) &= \vec{C}_3(M) \times \vec{C}_1 (M) \\
  \vec{R}_3(\widetilde{M}) &= \vec{C}_1(M) \times \vec{C}_2 (M)
\end{align*}
We then have:
\begin{align*}
  \vec{R}_i(\widetilde{M}) \cdot \vec{C}_j(M) &= \vec{C}_1(M) \cdot (\vec{C}_2(M) \times \vec{C}_3(M))\delta_{i j} \\
                                              &= [\vec{C}_1(M), \vec{C}_2(M), \vec{C}_3(M)]\delta_{i j} \\
                                              &= (\det M)\delta_{i j}
\end{align*}
So these rows are such that $\widetilde{M}M = (\det M)I$.
Therefore, provided $\det M \neq 0$, similarly to in $\R^2$:
\[
  M^{-1} = \frac{\widetilde{M}}{\det M}
\]

This process also provides us with multiple statements that are equivalent to $\det M \neq 0$:
\begin{itemize}
 \item $\{M\vec{e}_1, M\vec{e}_2, M\vec{e}_3\} = \{\vec{C}_1(M), \vec{C}_2(M), \vec{C}_3(M)\}$ are linearly independent
 \item $\im M = \R^3$
 \item $\rank M = 3$
\end{itemize}
\begin{remark}
  General $3 \times 3$ determinants can be expanded in terms of $2 \times 2$ determinants:
  \[
    \det M = \begin{vmatrix}
    M_{1 1} & M_{1 2} & M_{1 3} \\
    M_{2 1} & M_{2 2} & M_{2 3} \\
    M_{3 1} & M_{3 2} & M_{3 3} \\
    \end{vmatrix} =
    M_{1 1} \begin{vmatrix}
    M_{2 2} & M_{2 3} \\
    M_{3 2} & M_{3 3} \\
    \end{vmatrix}
    - M_{1 2}\begin{vmatrix}
    M_{2 1} & M_{2 3} \\
    M_{3 1} & M_{3 3} \\
    \end{vmatrix}
    + M_{1 3}\begin{vmatrix}
    M_{2 1} & M_{2 2} \\
    M_{3 1} & M_{3 2} \\
    \end{vmatrix}
  \]
  This will be explored further when computing determinants of $n \times n$ matrices.
\end{remark}
\subsection{Permutations}
The goal of this section is to rigorously define a general Levi-Civita Epsilon first encountered in \cref{generalLCE}.
\begin{definition}[Permutation]
  A \textit{permutation} of a set $S$ is a bijection $\rho: S \to S$.
\end{definition}
\begin{remark}[Notation]
  \begin{itemize}
    \item $S_n$ is the set of all permutations of the elements $1, \ldots, n$.
      This has $n!$ elements
    \item We usually use the ``two-line'' notation to describe permutations.
      For example, for a $\rho \in S_n$, we would write:
      \[
        \rho = \begin{pmatrix}
        1 & 2 & \cdots & n \\
        \rho(1) & \rho(2) & \cdots & \rho(n) \\
        \end{pmatrix}
      \]
  \end{itemize}
\end{remark}
\begin{definition}[Fixed Point]
  A \textit{fixed point} of $\rho$ is a $k$ such that $\rho(k) = k$.
  By convention, we usually omit fixed points from permutations.
\end{definition}
\begin{definition}[Disjoint]
  Two permutations are \textit{disjoint} if all elements fixed by one permutation are not fixed by the other and vice versa.

  That is, all elements are fixed by exactly one of the two permutations.
\end{definition}
\begin{example}[Simplifying Permutations]
  Consider the following permutation, we can simplify it into multiple smaller permutations:
  \begin{align*}
    \begin{pmatrix}
    1 & 2 & 3 & 4 & 5 & 6 \\
    5 & 6 & 3 & 1 & 4 & 2 \\
    \end{pmatrix} &=
    \begin{pmatrix}
    1 & 2 & 4 & 5 & 6 \\
    5 & 6 & 1 & 4 & 2 \\
    \end{pmatrix} \text{ as 4 is fixed}\\
    &= \begin{pmatrix}
    1 & 4 & 5 \\
    5 & 1 & 4 \\
    \end{pmatrix}
    \begin{pmatrix}
    2 & 6 \\
    6 & 2 \\
    \end{pmatrix} \text{ split into disjoint permutations}\\
    &= \cycle{5 4 1}\cycle{6 2} \text{ as both just cycle elements}
  \end{align*}
  The permutations denoted using a single line are \textit{cycles}, and represent permutations where the elements are just cycled.
\end{example}
\subsubsection{Conversion From Two Line to Cycle Notation}
\begin{enumerate}
  \item Pick an arbitrary element $x$.
  \item Repeatedly apply the permutation to $x$ until an element is repeated.
    This is the first cycle.
  \item Repeat \textbf{i} and \textbf{ii} on a new element not yet written until all elements have appeared.
\end{enumerate}
\begin{example}
  For example, with $\begin{pmatrix}1 & 4 & 5 \\ 5 & 1 & 4 \\\end{pmatrix}$, $\rho(1) = 5, \rho(\rho(1)) = 4, \rho(\rho(\rho(1))) = 1$ so this is written as $\cycle{5 4 1}$.
\end{example}
\begin{remark}[Warning]
  Disjoint permutations commute, but in general non-disjoint permutations do \textbf{not commute}.
\end{remark}
\begin{definition}[Two-Cycle]
  A \textit{2-cycle} or \textit{transposition} is a cycle which only contains two numbers.
  That is:
  \[
    \begin{pmatrix}
    i & j \\
    j & i \\
    \end{pmatrix} \text{ or }
    \cycle{i j}
  \]
\end{definition}
\begin{proposition}
  Any $q$-cycle can be written as a product of 2-cycles.
\end{proposition}
\begin{example}
  For example:
  \[
    \cycle{1 2 {\cdots} n} = \cycle{1 2}\cycle{2 3}\cdots\cycle{{n - 1} n}
  \]
\end{example}
\begin{definition}[Permutation Sign]
  The \textit{sign} of a permutation $\varepsilon(\rho)$ is $(-1)^{r}$ where $r$ is the number of two cycles of the permutation $\rho$ when written as a product of 2-cycles.

  If $\varepsilon(\rho) = +1$ then it is an \textit{even permutation}.
  Otherwise it is an \textit{odd permutation}.
\end{definition}
\begin{remark}
  \[
    \varepsilon(\rho \sigma) = \varepsilon(\rho)\varepsilon(\sigma) \text{ and } \varepsilon(\rho^{-1}) = \varepsilon(\rho)
  \]
\end{remark}
\begin{definition}[Levi-Civitia Epsilon]
Define $\levi_{a_1 a_2 \ldots a_n}$ (with $n$ indices) as:
\[
  \levi_{a_1 a_2 \ldots a_n} = \begin{cases}
  1 & (a_1, a_2, \ldots, a_n) \text{ is an even permutation of }(1, 2, \ldots, n) \\
  -1 & (a_1, a_2, \ldots, a_n) \text{ is an odd permutation of }(1, 2, \ldots, n) \\
  0 & \text{ otherwise (any two indices are equal)}
  \end{cases}
\]
\end{definition}
\begin{remark}[Note]
$\levi_{a_1 a_2 \ldots a_n}$ is \textit{totally antisymmetric}.
That is, if we swap any of the two indices then we get a change in sign.
\end{remark}
\subsection{Alternating Forms}
\begin{definition}[Alternating Form]
  For vector $\vec{v}_1, \ldots, \vec{v}_n \in \R^{n} \text{ or } \C^{n}$, the \textit{rank $n$ alternating form} is defined by:
  \begin{align*}
    [\vec{v}_1, \ldots, \vec{v}_n] &= \levi_{j_1 \ldots j_n} (\vec{v}_1)_{j_1} (\vec{v}_2)_{j_2} \cdots (\vec{v}_n)_{j_n} \\
                                   &= \sum_{\rho \in S_n}  \varepsilon(\rho) (\vec{v}_1)_{\rho(1)} (\vec{v}_2)_{\rho(2)} \cdots (\vec{v}_n)_{\rho(n)}
  \end{align*}
\end{definition}
\begin{remark}
  In $\R^2$, this is the scalar cross product and in $\R^3$ this is the scalar triple product.
\end{remark}
\subsubsection{Properties of Alternating Forms}
\begin{enumerate}
  \item \textbf{Multilinear -} Let $\vec{v}_p = \alpha \vec{u} + \beta \vec{w}$, then:
    \begin{align*}
    [\vec{v}_1, \ldots, \vec{v}_{p - 1}, \alpha \vec{u} + \beta \vec{w}, \vec{v}_{p + 1}, \ldots, \vec{v}_n] &= \alpha [\vec{v}_1, \ldots, \vec{v}_{p - 1}, \vec{u}, \vec{v}_{p + 1}, \ldots, \vec{v}_n]\\ &\quad+ \beta [\vec{v}_1, \ldots, \vec{v}_{p - 1}, \vec{w}, \vec{v}_{p + 1}, \ldots, \vec{v}_n]
    \end{align*}
  \item \textbf{Totally Antisymmetric -} Permuting the vectors introduces the sign of the permutation:
    \[
      [\vec{v}_{\rho(1)}, \ldots, \vec{v}_{\rho(n)}] = \varepsilon(\rho)[\vec{v}_1, \ldots, \vec{v}_n]
    \]
    This is because exchanging $\vec{v}_p$ and $\vec{v}_q$ for any $p \neq q$ changes the sign of the alternating form and $\varepsilon(\rho)$ depends on how many transpositions are needed to achieve the permutation.
  \item \textbf{Action on the Canonical Basis -} There is only one permutation of $1, \ldots, n$ where $(\vec{e}_1)_{\rho(1)} \cdots (\vec{e}_n)_{\rho(n)}$ is non-zero, namely the identity permutation.
    This has sign $+1$, so we have:
    \[
      [\vec{e}_1, \ldots, \vec{e}_n] = 1
    \]
\end{enumerate}
The above three properties axiomatically determine alternating forms but there are also further properties that follow from these:
\begin{enumerate}
  \setcounter{enumi}{3}
  \item If $\vec{v}_p = \vec{v}_q$ for $p \neq q$, then:
    \[
      [\vec{v}_1, \ldots, \vec{v}_p, \ldots, \vec{v}_q, \ldots, \vec{v}_n] = 0
    \]
    This follows from \textbf{ii} as $\varepsilon(\rho) = 0$ if two indices mapped to the same one.
  \item If the vectors are linearly dependant then we can write $\vec{v}_p = \sum_{i \neq p} \lambda_i \vec{v}_i$, therefore:
    \[
      [\vec{v}_1, \ldots, \vec{v}_p, \ldots, \vec{v}_n] = 0
    \]
    This follows from \textbf{i} and \textbf{iv}.
    We can use multilinearity to split up and pull out the scalars and then we will have repeated vectors in each term.
\end{enumerate}
\begin{proposition}
  \label{alternatingFormIndependent}
  \[
    [\vec{v}_1, \ldots, \vec{v}_n] \neq 0 \iff \{\vec{v}_1, \ldots, \vec{v}_n\} \text{ is linearly independent}
  \]
\end{proposition}
\begin{proof}
  \begin{proofdirection}{Assume $[\vec{v}_1, \ldots, \vec{v}_n] \neq 0$}
    Suppose, for contradiction, that $\{\vec{v}_1, \ldots, \vec{v}_n\}$ is linearly dependant.
    Then some $\vec{v}_p$ is a linear combination of the other vectors from the set, that is:
    \[
      \vec{v}_p = \sum_{i \neq p} \lambda_i \vec{v}_i
    \]
    so by property \textbf{v}, $[\vec{v}_1, \ldots, \vec{v}_p, \ldots, \vec{v}_n] = 0$.
    Which contradicts $[\vec{v}_1, \ldots, \vec{v}_n] \neq 0$, so $\{\vec{v}_1, \ldots, \vec{v}_n\}$ is linearly independent.
  \end{proofdirection}
  \begin{proofdirection}{Assume $\{\vec{v}_1, \ldots, \vec{v}_n\}$ is linearly independent}
    If $\{\vec{v}_1, \ldots, \vec{v}_n\}$  are linearly independent then they span $\R^{n}$ or $\C^{n}$.
    In particular, we can express the standard basis as:
    \[
      \vec{e}_j = u_{i j}\vec{v}_i
    \]
    for some coefficients $u_{i j}$.
    Hence,
    \begin{align*}
      [\vec{e}_1, \ldots, \vec{e}_n] &= u_{i_1 1} u_{i_2 2} \ldots u_{i_n n}[\vec{v}_{i_1}, \ldots, \vec{v}_{i_n}] \text{ by property \textbf{i}}\\
                                     &= u_{i_1 1} u_{i_2 2} \ldots u_{i_n n} \varepsilon_{i_1 \ldots i_n }[\vec{v}_{1}, \ldots, \vec{v}_{n}] \text{ by property \textbf{ii}}
    \end{align*}
    From property \textbf{iii}, $[\vec{e}_1, \ldots, \vec{e}_n] = 1$ so $[\vec{v}_{1}, \ldots, \vec{v}_{n}] \neq 0$.
  \end{proofdirection}
\end{proof}
\subsection{Determinants in \texorpdfstring{$\R^{n}$ and $\C^{n}$}{n Dimensions}}
\begin{definition}[Determinant]
  Consider an $n \times n$  matrix $M$ with columns given by:
  \[
    \vec{C}_i = M\vec{e}_i = M_{j i}\vec{e}_j
  \]
  Then, the \textit{determinant} of the matrix $M$, $\det M \in \R \text{ or } \C$, has the following equivalent definitions:
  \begin{align*}
    \det M &= [\vec{C}_1, \ldots, \vec{C}_n] \\
           &= [M\vec{e}_1, \ldots, M\vec{e}_n] \\
           &= \levi_{j_1 \ldots j_n} M_{j_1 1} \cdots M_{j_n n} \\
           &= \sum_{\rho \in S_n} \varepsilon(\rho) M_{\rho(1) 1} \cdots M_{\rho(n) n}
  \end{align*}
\end{definition}
\begin{remark}[Notation]
We also use vertical bars to denote the determinant of an $n \times n$  matrix
\[
  \det M =
  \begin{vmatrix}
  M_{1 1} & \cdots & M_{1 n} \\
  \vdots & \ddots & \vdots \\
  M_{n 1} & \cdots & M_{n n} \\
  \end{vmatrix}
\]
\end{remark}
\subsubsection{Properties of the Determinant}
\begin{enumerate}
  \item It is a multilinear function of the columns of the matrix.
    In particular,
    \[
      \det (\lambda M)  = [\lambda \vec{C}_1, \ldots, \lambda \vec{C}_n] = \lambda^{n} [\vec{C}_1, \ldots, \vec{C}_n] = \lambda^{n}\det M
    \]
    for any matrix of size $n \times n$.
  \item It is a totally antisymmetric function of the columns of the matrix.
    That is, if you interchange two columns of the matrix, then the sign of the determinant changes.
  \item $\det I = [\vec{e}_1, \ldots, \vec{e}_n] = 1$ for the identity matrix of any size.
  \item If any two columns of a matrix $M$ are identical, then $\det M = 0$.
    This follows from property \textbf{iv} of the alternating form.
  \item If two columns are a linearly dependant, then $\det M = 0$.
    This follows immediately from property \textbf{v} of the alternating form but can also be shown by the following proof.
\end{enumerate}
\begin{proof}[\textbf{v}]
  Suppose now that two columns are linearly dependant, then $\vec{C}_i(M) + \lambda \vec{C}_j(M) = \vec{0}$ for $i \neq j$.
  Define a matrix $N$ with entries given by:
  \[
    N_{r s} = \begin{cases}
    M_{r s} &  s \neq i \\
    M_{r s} + \lambda M_{r j} & s = i
    \end{cases}
  \]
  That is, $N$ is $M$ but with every entry in $i$-th column as zero.

  We can write $\det N$ as:
  \begin{align*}
    \det N &= [\vec{C}_1(N), \ldots, \vec{C}_i(N), \ldots \vec{C}_n(N)] \\
           &= [\vec{C}_1(M), \ldots, \vec{C}_i(M) + \lambda \vec{C}_j(M), \ldots, \vec{C}_n(M)] \\
           &= [\vec{C}_1(M), \ldots, \vec{C}_n(M)] + \lambda [\vec{C}_1(M), \ldots, \vec{C}_j(M), \ldots, \vec{C}_n(M)] \\
           &= \det M + \lambda \det L
  \end{align*}
  where $L$ is a matrix where columns $i$ and $j$ are identical.
  Therefore by property \textbf{iv}, $\det L = 0$ so $\det N = \det M$.
  By construction, the $i$-th column of $N$ is all zeroes, thus $\det N = 0$ and so $\det M = 0$.
\end{proof}
\begin{enumerate}
  \setcounter{enumi}{5}
  \item $\det M \neq 0 \iff$ its columns are linearly independent.
    This follows immediately from \cref{alternatingFormIndependent}.
\end{enumerate}
\begin{remark}
  As a consequence of the proof for \textbf{v}, under a column operation:
  \[
    \vec{C}_i \mapsto \vec{C}_i + \lambda \vec{C}_j \quad i \neq j
  \]
  $\det M$ is unchanged.
\end{remark}
\begin{enumerate}
  \setcounter{enumi}{6}
  \item $\det M = \det(M^{\trans})$
\end{enumerate}
\begin{proof}[\textbf{vii}]
  Take a single term of the alternating form sum, that is:
  \[
    \varepsilon(\rho)M_{\rho(1) 1} \cdots M_{\rho(n) n}
  \]
  for some $\rho \in S_n$.
  Take any $\sigma \in S_n$, we then have:
  \[
    M_{\rho(1) 1} \cdots M_{\rho(n) n} = M_{\rho(\sigma(1)) \sigma(1)} \cdots M_{\rho(\sigma(n)) \sigma(n)}
  \]
  as introducing the $\sigma$ only permutes the order in which the elements are multiplied so the value is unchanged.

  Now if we take $\rho = \sigma^{-1}$, $\varepsilon(\rho) = \varepsilon(\sigma^{-1}) = \varepsilon(\sigma)$ and thus:
  \begin{align*}
    \det M &= \sum_{\rho \in S_n} \varepsilon(\rho) M_{\rho(1) 1} \cdots M_{\rho(n) n} \\
           &= \sum_{\sigma \in S_n} \varepsilon(\sigma) M_{1 \sigma(1)} \cdots M_{n \sigma(n)} \\
           &= \det(M^{\trans})
  \end{align*}
  as, by definition, $(M^{\trans})_{i j} = M_{j i}$.
\end{proof}
\begin{remark}
  Since the columns of $M^{\trans}$ are the rows of $M$, all of the properties above for columns hold equally for rows.

  So under a row operation:
  \[
    \vec{R}_i \mapsto \vec{R}_i +\lambda \vec{R}_j \quad i \neq j
  \]
  $\det M$ is unchanged.
\end{remark}
\begin{enumerate}
  \setcounter{enumi}{7}
  \item $\det (M^{\dag}) = \overline{\det M}$.
    By definition $(M^{\dag})_{i j} = \overline{M_{j i}}$ so we can write:
    \[
      \overline{\det (M^{T})} = \overline{\sum_{\rho \in S_n} \varepsilon(\rho) M_{1 \rho(1)} \cdots M_{n \rho(n)}} = \sum_{\rho \in S_n} \varepsilon(\rho) \overline{M_{1 \rho(1)}} \cdots \overline{M_{n \rho(n)}} = \det (M^{\dag})
    \]
    From \textbf{vii}, $\det M^{\trans} = \det M$, thus $\det M^{\dag} = \overline{\det M}$
  \item If $M$ and $N$ are both $n \times n$ matrices, then $\det(MN) = \det(M)\det(N)$.

    In particular, if $M$ is invertible, $M^{-1} M = I$ so:
    \[
      (\det M^{-1})(\det M) = 1 \implies \det M^{-1} = \frac{1}{\det M}
    \]
\end{enumerate}
\begin{proof}[\textbf{ix}]
  By definition:
  \[
    \det MN = \sum_{\sigma} \varepsilon(\sigma) (MN)_{\sigma(1) 1} \cdots (MN)_{\sigma(n) n}
  \]
  We can write $(MN)_{i j} = \sum_{k = 1}^{n} M_{i k}N_{k j}$ so:
  \begin{align*}
    \det MN &= \sum_{\sigma} \varepsilon(\sigma) \sum_{k_1, \ldots, k_n = 1}^n M_{\sigma(1) k_1}N_{k_1 1} \cdots M_{\sigma(n) k_n}N_{k_n n} \\
            &= \sum_{k_1, \ldots, k_n = 1}^{n} N_{k_1 1} \cdots N_{k_n n} \underbrace{\sum_{\sigma} \varepsilon(\sigma)M_{\sigma(1) k_1} \cdots M_{\sigma(n) k_n}}_{=S}
  \end{align*}
  If $k_i = k_j$ for some $i \neq j$, then $S$ is the determinant of a matrix with two equal columns so $S = 0$.
  So the only terms in the outer sum that are non-zero are those where $k_1, \ldots, k_n$ must some permutation $\rho$ of $1, \ldots, n$.
  Therefore:
  \[
    \det MN = \sum_{\rho} N_{\rho(1) 1} \cdots N_{\rho(n) n} \sum_{\sigma} \varepsilon(\sigma) M_{\sigma(1) \rho(1)} \cdots M_{\sigma(n) \rho(n)}
  \]
  Similarly to in the proof of \textbf{vii}, we can write:
  \begin{align*}
    M_{\sigma(1) \rho(1)} \cdots M_{\sigma(n) \rho(n)} &= \varepsilon(\rho^{-1}) M_{\sigma(1) \rho(\rho^{-1}(1))} \cdots M_{\sigma(n) \rho(\rho^{-1}(n))} \\
                                                       &= \varepsilon(\rho) M_{\sigma(1) 1} \cdots M_{\sigma(n) n}
  \end{align*}
  Therefore:
  \begin{align*}
    \det MN &= \sum_{\rho} N_{\rho(1) 1} \cdots N_{\rho(n) n} \varepsilon(\rho) \sum_{\sigma} \varepsilon(\sigma) M_{\sigma(1) 1} \cdots M_{\sigma(n) n} \\
            &= \det M \sum_{\rho} \varepsilon(\rho) N_{\rho(1) 1} \cdots N_{\rho(n) n} \\
            &= (\det M)(\det N)
  \end{align*}
\end{proof}
\begin{enumerate}
  \setcounter{enumi}{9}
  \item If $M$ is orthogonal, then $\det M = \pm 1$.

    By definition $M^{\trans} M = I$ so $\det(M^{\trans} M) = \det I = 1$.
    By \textbf{ix} and \textbf{vii}, $\det M^{\trans}M = \det M \det M^{\trans} = (\det M)^2$.
    Therefore $(\det M)^2 = 1$, so $\det M = \pm 1$.
  \item If $M$ is unitary, then $|\det M| = 1$.

    From \textbf{viii}, $\det M^{\dag} = \overline{\det M}$ and since $M M^{\dag} = I$:
    \[
      \det(M M^{\dag}) = \det I = \det M \det M^{\dag} = (\det M)(\overline{\det M}) = |\det M|^2 = 1
    \]
    so $|\det M| =  1$.
\end{enumerate}
\subsection{Minors and Cofactors}
\begin{definition}[Minor]
  For an $n \times n$ matrix $M$, consider the $(n - 1) \times (n - 1)$ matrix in which row $i$ and column $j$ of $M$ have been removed.
  The determinant of this matrix, denoted by $M^{i j}$, is called a \textit{minor} of $M$.
\end{definition}
\begin{definition}[Cofactor]
  For an $n \times n$ matrix $M$, consider its columns and rows:
  \[
    \vec{C}_j = \sum_{i} M_{i j}\vec{e}_i,\ \vec{R}_i = \sum_{j} M_{i j}\vec{e}_j
  \]
  Then, the determinant of $M$ can be written as:
  \begin{align*}
    \det M &= [\vec{C}_1, \ldots, \vec{C}_n] \\
           &= \sum_{i} M_{i j} \Delta_{i j} \text{ for any fixed $j$} \\
           &= \sum_{j} M_{i j} \Delta_{i j} \text{ for any fixed $i$}
  \end{align*}
  where the $\Delta_{i j}$ are called \textit{cofactors.}
  \begin{align*}
    \Delta_{i j} &= [\vec{C}_1, \ldots, \vec{C}_{j - 1}, \vec{e}_i, \vec{C}_{j + 1}, \ldots, \vec{C}_n] \\
                 &= [\vec{R}_1, \ldots, \vec{R}_{i - 1}, \vec{e}_j, \vec{R}_{i + 1}, \ldots, \vec{R}_n] \\
                 &= \left(\begin{array}{c c c}
                    A & \multicolumn{1}{|c}{\scriptstyle{\substack{0 \\ \vdots}}} & \multicolumn{1}{|c}{B} \\ \cline{1-1} \cline{3-3}
                    \scriptstyle{0\ \cdots\ 0} & \scriptstyle{1} & \scriptstyle{0\ \cdots\ 0} \\ \cline{1-1} \cline{3-3}
                    C & \multicolumn{1}{|c}{\scriptstyle{\substack{\vdots \\ 0}}} & \multicolumn{1}{|c}{D}
                    \end{array}\right)
  \end{align*}
  where the zeroes are in column $j$ and row $i$.
\end{definition}
The cofactor $\Delta_{i j}$ is the determinant of the matrix obtained from $M$ by replacing the entry $M_{i j}$ by $1$ and all other entries in row $i$ and column $j$ with $0$.
\begin{remark}[Relation between $\Delta_{i j}$ and $M^{i j}$]
  By reordering rows and columns, noting that swapping a row or column introduces a $-1$, we have:
  \begin{align*}
    \Delta_{i j} &=
    \det \left(\begin{array}{c c c}
    A & \multicolumn{1}{|c}{\scriptstyle{\substack{0 \\ \vdots}}} & \multicolumn{1}{|c}{B} \\ \cline{1-1} \cline{3-3}
    \scriptstyle{0\ \cdots\ 0} & \scriptstyle{1} & \scriptstyle{0\ \cdots\ 0} \\ \cline{1-1} \cline{3-3}
    C & \multicolumn{1}{|c}{\scriptstyle{\substack{\vdots \\ 0}}} & \multicolumn{1}{|c}{D}
    \end{array}\right)\\
    &=(-1)^{n - i}(-1)^{n - j} \det \left(\begin{array}{c c c}
    A & \multicolumn{1}{|c}{B} & \multicolumn{1}{|c}{0} \\ \cline{1-2}
    C & \multicolumn{1}{|c}{D} & \multicolumn{1}{|c}{\vdots} \\ \cline{1-2}
    0 & \cdots & 1
    \end{array}\right)
  \end{align*}
  The determinant of this matrix is the same as the determinant of the top left block matrices as if we have a permutation of $n - 1$ elements and introduce a new element $n$ that is always mapped to itself the sign of the permutation is unchanged.
  That is:
  \begin{align*}
    \det \left(\begin{array}{c c c}
    A & \multicolumn{1}{|c}{B} & \multicolumn{1}{|c}{0} \\ \cline{1-2}
    C & \multicolumn{1}{|c}{D} & \multicolumn{1}{|c}{\vdots} \\ \cline{1-2}
    0 & \cdots & 1
    \end{array}\right)
    &= \sum_{i_1, \ldots, i_n = 1}^{n} \levi_{i_1 \ldots i_n} M_{i_1 1} \ldots M_{i_n n} \\
    &= \sum_{i_1, \ldots, i_{n-1} = 1}^{n} \levi_{i_1 \ldots i_{n-1} n} M_{i_1 1} \ldots M_{i_{n-1} n-1} M_{n n} \\
    &= \sum_{i_1, \ldots, i_{n-1} = 1}^{n} \levi_{i_1 \ldots i_{n - 1}} M_{i_1 1} \ldots M_{i_{n-1} n-1} \\
    &=\det \left(\begin{array}{c|c}
      A & B \\ \hline
      C & D
    \end{array}\right)
  \end{align*}
  Therefore:
  \begin{align*}
    \Delta_{i j}&=(-1)^{i + j} \det \left(\begin{array}{c|c}
      A & B \\ \hline
      C & D
    \end{array}\right) \\
                &= (-1)^{i + j}M^{i j}
  \end{align*}
\end{remark}
Hence we have the following expansion about any column $j$:
\[
  \det M = \sum_{i} M_{i j}\Delta_{i j} = \sum_{i} (-1)^{i + j}M_{i j}M^{i j}
\]
or about any row $i$:
\[
  \det M = \sum_{j} M_{i j}\Delta_{i j} = \sum_{j} (-1)^{i + j}M_{i j}M^{i j}
\]
\begin{theorem}[Laplace Expansion Formula]
  Consider an $n \times n$ matrix $M$.
  Then for any fixed $j$:
  \[
    \det M = \sum_{i} M_{i j}\Delta_{i j}
  \]
\end{theorem}
\begin{remark}[Notation]
  For brevity, we will write an over-line over an element in a list to denote the list without that particular element
  For example:
  \[
    1, \ldots, \overline{4}, \ldots 7 \equiv 1, 2, 3, 5, 6, 7
  \]
\end{remark}
\begin{proof}
  \begin{align*}
    \det M &= \sum_{i_1, \ldots, i_n = 1}^{n} \varepsilon_{i_1 \ldots i_n} M_{i_1 1} \cdots M_{i_n n} \\
           &= \sum_{i_j = 1}^{n} M_{i_j j} \sum_{i_1, \ldots, \overline{i_j}, \ldots, i_n = 1}^{n} \varepsilon_{i_1 \ldots i_n} M_{i_1 1} \cdots \overline{M_{i_j j}} \cdots M_{i_n n}
  \end{align*}
  Consider $\sigma \in S_n$, the permutation that moves $i_j$ to the $j$-th position, and the shifts the elements $j, \cdots, i_j - 1$ to the right into the gap left by $i_j$.
  If $i_j > j$, then $\sigma$ is written as:
  \[
    \sigma = \begin{pmatrix}
    1 & \cdots & j & j + 1 & j + 2 & \cdots & i_j -1 & i_j & i_j + 1 & \cdots & n \\
    1 & \cdots & i_j & j & j + 1 & \cdots & i_j - 2 & i_j - 1 & i_j + 1 & \cdots & n \\
    \end{pmatrix}
  \]
  If $i_j < j$, then we construct $\sigma$ analogously apart from that the elements $i_j + 1, \ldots, j$ are shifted to the left.
  To achieve the permutation $\sigma$ we need to perform $|j - i_j|$ transpositions, thus $\varepsilon(\sigma) = (-1)^{j - i_j}$.

  Now consider the permutation $\rho \in S_{n - 1}$:
  \[
    \rho =\begin{pmatrix}
    1 & \cdots & \cdots & \overline{i_j} & \cdots & n \\
    i_1 & \cdots & \overline{i_j} & \cdots & \cdots & i_n \\
    \end{pmatrix}
  \]
  so $\rho$ sends $1, 2, \ldots, \overline{i_j}, \ldots, n$ (1 to $n$ excluding $i_j$) to $i_1, \ldots, \overline{i_j}, \ldots, i_n$ (some permutation of 1 to $n$ excluding $i_j$), thus $\varepsilon(\rho) = \levi_{i_1 \ldots \overline{i_j} \ldots i_n}$.
  Since $i_j$ is not included in $\rho$, it is unchanged under $\rho$.

  Note that $\rho \sigma$ reorders $(1, \ldots, n)$ to $(i_1, \ldots, i_n)$ so $\varepsilon(\rho\sigma) = \levi_{i_1 \ldots i_n}$.
  Therefore:
  \[
    \varepsilon(\rho \sigma) = \levi_{i_1 \ldots i_n} = \varepsilon(\rho) \varepsilon(\sigma) = (-1)^{j - i_j} \levi_{i_1 \ldots \overline{i_j} \ldots i_n}
  \]
  Hence, we can write:
  \[
    \det M = \sum_{i_j = 1}^{n} M_{i_j j} \sum_{i_1, \ldots, \overline{i_j}, \ldots, i_n = 1}^n (-1)^{j - i_j} \levi_{i_1 \ldots \overline{i_j} \ldots i_n} M_{i_1 1} \ldots \overline{M_{i_j j}} \ldots M_{i_n n} \\
  \]
  Note that:
  \[
    \sum_{i_1, \ldots, \overline{i_j}, \ldots, i_n = 1}^n \levi_{i_1 \ldots \overline{i_j} \ldots i_n} M_{i_1 1} \ldots \overline{M_{i_j j}} \ldots M_{i_n n} = M^{i_i j}
  \]
  Therefore:
  \begin{align*}
    \det M &= \sum_{i_j = 1}^{n} M_{i_j j}(-1)^{j - i_j} M^{i_j j} \\
           &= \sum_{i_j = 1}^{n}  M_{i_j j} \Delta_{i_j j} \text{ (as $(-1)^{j - i_j}M^{i_j j} = (-1)^{i_j + j}M^{i_j j} = \Delta_{i_j j}$)}\\
           &= \sum_{i = 1}^{n} M_{i j} \Delta_{i j}
  \end{align*}
\end{proof}
\begin{remark}
  Since $\det M = \det M^{\trans}$, the above holds for an expansion about any row $i$.
\end{remark}
Reasoning as a above, if
\[
  \vec{C}_k = \sum_{i} M_{i k}\vec{e}_i
\]
then:
\[
  [\vec{C}_1, \ldots, \vec{C}_{j - 1}, \vec{C}_k, \vec{C}_{j + 1}, \ldots, \vec{C}_n] = \sum_{i} M_{i k} \Delta_{i j} =
  \begin{cases}
  \det M & \text{ if }j = k \\
  0 & \text{ if } j \neq k
  \end{cases}
\]
as if two columns are identical $\det M = 0$.
Hence:
\begin{align*}
  \sum_{i} M_{i k}\Delta_{i j} &= (\det M)\delta_{j k} \\
  \sum_{j} M_{l j}\Delta_{i j} &= (\det M)\delta_{l i}
\end{align*}
\begin{definition}[Adjugate]
  The \textit{adjugate} of a matrix $M$ is defined to be:
  \[
    \widetilde{M} = \adj M = \Delta^{\trans}
  \]
  where $\Delta$ is the matrix of cofactors $\Delta_{i j}$.
\end{definition}
From the expression above, using summation convention, note that:
\begin{align*}
  \Delta_{i j}M_{i k} &= (\Delta^{\trans})_{j i}M_{i k} \\
                      &= (\Delta^{\trans} M)_{j k} \\
                      &= (\widetilde{M} M)_{j k}
\end{align*}
Since $\Delta_{i j}M_{i k} = (\det M)\delta_{j k}$, $\widetilde{M} M = (\det M)I$.

Similarly:
\begin{align*}
  M_{l j}\Delta_{i j} &= M_{l j}(\Delta^{\trans})_{j i} \\
                      &= (M \Delta^{\trans})_{l i} \\
                      &= (M \widetilde{M})_{l i}
\end{align*}
Since $M_{l j}\Delta_{i j} = (\det M)\delta_{l i}$, $M \widetilde{M} = (\det M)I$.

Therefore:
\[
  \widetilde{M} M = M \widetilde{M} = (\det M)I
\]
and if $\det M \neq 0$, then $M^{-1}$ exists:
\[
  M^{-1} = \frac{1}{\det M}\widetilde{M} = \frac{1}{\det M}(\adj M)
\]
This is the inverse of $M$.
\begin{example}
  \label{determinantExample}
  Consider the matrix $M$:
  \[
    M = \begin{pmatrix}
    1 & x & 1 \\
    1 & 1 & x \\
    x & 1 & 1 \\
    \end{pmatrix}
    \quad x \in \R
  \]
  We want to compute its determinant.
  Recall that row and column operations do not change the value of the determinant and scaling a column/row by a factor multiplies the determinant by that factor.
  This will allow us to greatly simplify the matrix before needing to carry out the Laplace expansion.
  \begin{align*}
    \det M &= \det \begin{pmatrix}
    1 & x & 1 \\
    1 & 1 & x \\
    x & 1 & 1 \\
    \end{pmatrix}\\
    &= \det \begin{pmatrix}
    0 & x & 1 \\
    1-x & 1 & x \\
    x-1 & 1 & 1 \\
    \end{pmatrix} \quad\text{ ($\vec{C}_1 \mapsto \vec{C}_1 - \vec{C}_3$)}\\
    &= \det \begin{pmatrix}
    0 & x & 1 \\
    2-2x & 0 & x - 1 \\
    x-1 & 1 & 1 \\
    \end{pmatrix} \quad\text{ ($\vec{R}_2 \mapsto \vec{R}_2 - \vec{R}_3$)}\\
    &= \det \begin{pmatrix}
    0 & x & 1 \\
    2-2x & 0 & x - 1 \\
    x-1 & 1-x & 0 \\
    \end{pmatrix} \quad\text{ ($\vec{R}_3 \mapsto \vec{R}_3 - \vec{R}_1$)}\\
    &= (x - 1)^2\det \begin{pmatrix}
    0 & x & 1 \\
    -2 & 0 & 1 \\
    1 & -1 & 0 \\
    \end{pmatrix} \quad\text{ (scaling $\vec{R}_2$ and $\vec{R}_3$ by $1/(x - 1)$)}\\
    &= (x-1)^2 \det \begin{pmatrix}
    0 & x + 2 & 0 \\
    -2 & 0 & 1 \\
    1 & -1 & 0 \\
    \end{pmatrix} \quad\text{ ($\vec{R}_1 \mapsto \vec{R}_1 - \vec{R}_2 - 2\vec{R}_3$)}\\
    &= (x-1)^2(x + 2) \det \begin{pmatrix}
    0 & 1 & 0 \\
    -2 & 0 & 1 \\
    1 & -1 & 0 \\
    \end{pmatrix} \quad\text{ (scaling $\vec{R}_1$ by $1/(x + 2)$)}\\
    &= (x - 1)^2(x + 2)\left[
    0 \begin{vmatrix}
    0 & 1 \\
    -1 & 0 \\
    \end{vmatrix}
    -1 \begin{vmatrix}
    -2 & 1 \\
    1 & 0 \\
    \end{vmatrix}
    +0 \begin{vmatrix}
    -2 & 0 \\
    1 & -1 \\
    \end{vmatrix}\right] \\
    &= (x - 1)^2(x + 2)
  \end{align*}
\end{example}
\subsection{Determinants of Block Matrices}
\label{blockDeterminants}
\nonexaminable
For square matrices $A, B, C, D$, we have the following:
\[
  \det \left(\begin{array}{c|c}
    A & B \\ \hline
    O & D
  \end{array}\right) =
  \det \left(\begin{array}{c|c}
    A & O \\ \hline
    C & D
  \end{array}\right) =
  \det(A)\det(D)
\]
In particular:
\[
  \det \left(\begin{array}{c|c}
    A & O \\ \hline
    O & I
  \end{array}\right) =
  \det \left(\begin{array}{c|c}
    I & O \\ \hline
    O & A
  \end{array}\right) =
  \det A
\]
Furthermore, if $A$ is invertible:
\[
  \det \left(\begin{array}{c|c}
    A & B \\ \hline
    C & D
  \end{array}\right) =
  \det(A)\det(D - CA^{-1}B)
\]
or if $D$ is invertible:
\[
  \det \left(\begin{array}{c|c}
    A & B \\ \hline
    C & D
  \end{array}\right) =
  \det(D)\det(A - BD^{-1}C)
\]
\section{Systems of Linear Equations}
\subsection{System of Two Equations}
Consider the system of equations given by:
\begin{align}
  A_{1 1}x_1 + A_{1 2}x_2 &= b_1 \tag{\textasteriskcentered}\label{system1}\\
  A_{2 1}x_1 + A_{2 2}x_2 &= b_2 \tag{$\dagger$}\label{system2}
\end{align}
We can write this as:
\[
  A\vec{x} = \vec{b}
\]
Multiply \cref{system1} by $A_{2 2}$ and subtract \cref{system2} multiplied by $A_{1 2}$:
\[
  (A_{1 1}A_{2 2} - A_{1 2}A_{2 1})x_1 = A_{2 2}b_1 - A_{1 2}b_2
\]
Similarly multiply \cref{system2} by $A_{1 1}$ and subtract \cref{system1} multiplied by $A_{2 1}$:
\[
  (A_{1 1}A_{2 2} - A_{1 2}A_{2 1})x_2 = A_{1 1}b_2 - A_{2 1}b_1
\]
Take the matrix:
\[
  A = \begin{pmatrix}
  A_{1 1} & A_{1 2} \\
  A_{2 1} & A_{2 2} \\
  \end{pmatrix},\
  \det A = A_{1 1}A_{2 2} - A_{1 2}A_{2 1}
\]
Therefore:
\begin{align*}
  (\det A)x_1 &= A_{2 2}b_1 - A_{1 2}b_2 \\
  (\det A)x_2 &= A_{1 1}b_2 - A_{2 1}b_2
\end{align*}
Provided $\det A \neq 0$, we can then write:
\begin{align*}
  \begin{pmatrix}
  x_1 \\
  x_2 \\
  \end{pmatrix}
  &= \frac{1}{\det A}
  \begin{pmatrix}
  A_{2 2} & -A_{1 2} \\
  -A_{2 1} & A_{1 1} \\
  \end{pmatrix}
  \begin{pmatrix}
  b_1 \\
  b_2 \\
  \end{pmatrix} \\
  &= \frac{\adj A}{\det A}
  \begin{pmatrix}
  b_1 \\
  b_2 \\
  \end{pmatrix} \\
  &= A^{-1}
  \begin{pmatrix}
  b_1 \\
  b_2 \\
  \end{pmatrix}
\end{align*}
So in summary, given $A\vec{x} = \vec{b}$, if $A^{-1}$ exists, we can write:
\[
  \vec{x} = A^{-1} \vec{b}
\]
\subsection{General Case}
Consider a system of $n$ linear equations with $n$ unknowns $x_i$ written in vector-matrix form as:
\[
  A\vec{x} = \vec{b}
\]
where $\vec{x}, \vec{b} \in \R^{n}$ and $A$ is an $n \times n$ matrix.

A solution for $A\vec{x} = \vec{b}$ exists if and only if we can find $A\vec{x}_0 = \vec{b}$ for some $\vec{x}_0$, equivalently, $\vec{b} \in \im A$.

If there is a such a solution we have:
\begin{align*}
  &A\vec{x} = \vec{b} \\
  \iff& A\vec{x} - \vec{b} = \vec{0} \\
  \iff& A\vec{x} - A\vec{x}_0 = \vec{0} \\
  \iff& A(\vec{x} - \vec{x}_0) = \vec{0}
\end{align*}
So $\vec{x}$ is also solution if and only if:
\[
  \vec{u} = \vec{x} - \vec{x}_0
\]
satisfies the \textit{homogeneous equation} $A\vec{u} = \vec{0}$.
\begin{proofcases}
  \begin{case}{$\det A \neq 0$}
    Recall that the image of a matrix is the span of its columns and the determinant is zero if and only if its columns are linearly independent.
    Therefore:
    \[
      \det A \neq 0 \iff \im A = \R^{n} \iff \ker A = \{\vec{0}\}
    \]
    Since $\im A = \R^{n}$, $b \in \im A$ so there must be a particular solution $\vec{x}_0$.
    Furthermore, $A\vec{u} = 0 \iff \vec{u} = \vec{0}$ so there is a unique solution, $\vec{x}_0 = A^{-1} \vec{b}$, to the original system.
  \end{case}
  \begin{case}{$\det A = 0$}
    \[
      \det A = 0 \iff \im A \neq \R^{n} \iff \rank A < n \iff \nullity A > 0
    \]
    Since $\nullity A > 0$, $\ker A$ cannot be a point so must contain an infinite number of elements and therefore there must be infinitely many possibilities for $\vec{u}$.

    If $\{\vec{u}_1, \ldots, \vec{u}_k\}$ is a basis for $\ker A$, then the general solution for $A \vec{u} = \vec{0}$ is:
    \[
      \vec{u} = \sum_{i = 1}^{k} \lambda_i \vec{u}_i
    \]
    where $k = \nullity A$.

    Now if $b \in \im A$, we have a particular solution $\vec{x}_0$, so there is infinitely many solutions given by $\vec{x} = \vec{x}_0 + \vec{u}$.
    Otherwise, if $b \notin \im A$, there is no solutions.
  \end{case}
\end{proofcases}
\subsubsection{Summary}
\begin{enumerate}
  \item $\det A \neq 0$ so $A^{-1}$ exists and there is a unique solution given by:
    \[
      \vec{x} = A^{-1}\vec{b}
    \]
  \item $\det A = 0$ and $\vec{b} \centernot\in \im A$, so there is no solution.
  \item $\det A = 0$ and $\vec{b} \in \im A$, there are infinitely many solutions given by:
    \[
      \vec{x} = \vec{x}_0 + \vec{u}
    \]
    where $\vec{x}_0$ is a particular solution and $\vec{u} \in \ker A$.
\end{enumerate}
\begin{example}
  Solve $A\vec{x} = \vec{b}$ with:
  \[
    A = \begin{pmatrix}
    1 & x & 1 \\
    1 & 1 & x \\
    x & 1 & 1 \\
    \end{pmatrix},\
    \vec{b} = \begin{pmatrix}
    1 \\
    y \\
    1 \\
    \end{pmatrix}
  \]
  We saw before in \cref{determinantExample} that $\det A = (x - 1)^2(x + 2)$.
  \begin{proofcases}
    \begin{case}{$x \neq 1$ and $x \neq -2$}
      Assume that $x \neq 1$ and $x \neq -2$ so $\det A \neq 0$ and thus $A^{-1}$ exists.
      We can construct $A^{-1}$ from the matrix of cofactors.
      \[
        A^{-1} = \frac{\Delta^{\trans}}{\det A}
      \]
      where:
      \[
        \Delta = \begin{pmatrix}
        1 - x & x^2 - 1 & 1 - x \\
        1 - x & 1 - x & x^2 - 1 \\
        x^2 - 1 & 1 - x & 1 - x \\
        \end{pmatrix}
      \]
      So:
      \[
        \adj A = \Delta^{\trans} = \begin{pmatrix}
        1-x & 1-x & x^2-1 \\
        x^2-1 & 1-x & 1-x \\
        1-x & x^2 -1 & 1-x \\
        \end{pmatrix}
      \]
      Note that $x^2 - 1 = (x + 1)(x - 1)$.
      So the solution is:
      \begin{align*}
        \vec{x} &= A^{-1} \vec{b} \\
                &= \frac{1}{(1 - x)(x + 2)}
                \begin{pmatrix}
                1 & 1 & -x-1 \\
                -x-1 & 1 & 1 \\
                1 & -x-1 & 1 \\
                \end{pmatrix}
                \begin{pmatrix}
                1 \\
                y \\
                1 \\
                \end{pmatrix}\\
                &= \frac{1}{(1- x)(x+2)}
                \begin{pmatrix}
                y - x \\
                -x + y \\
                2 - xy - y \\
                \end{pmatrix}
      \end{align*}
      So the solution is a point, as expected.
    \end{case}
    \begin{case}{$x = 1$}
      So $\det A = 0$ and
      \[
        A = \begin{pmatrix}
        1 & 1 & 1 \\
        1 & 1 & 1 \\
        1 & 1 & 1 \\
        \end{pmatrix}
      \]
      We can then find:
      \[
        \im A = \Span\left\{\begin{pmatrix}
        1 \\
        1 \\
        1 \\
        \end{pmatrix}\right\},\
        \ker A = \Span\left\{\begin{pmatrix}
        1 \\
        -1 \\
        0 \\
        \end{pmatrix},
        \begin{pmatrix}
        1 \\
        0 \\
        -1 \\
        \end{pmatrix}\right\}
      \]
      So:
      \[
        \vec{b} = \begin{pmatrix}
        1 \\
        y \\
        1 \\
        \end{pmatrix} \in \im A
        \iff y = 1
      \]
      If $y \neq 1$, then we have no solutions.

      Otherwise, if $y  = 1$, a particular solution of $A\vec{x} = \vec{b}$ is:
      \[
        \vec{x}_0 = \begin{pmatrix}
        1 \\
        0 \\
        0 \\
        \end{pmatrix}
      \]
      So the general solution is $\vec{x} = \vec{x}_0 + \vec{u}$ where $\vec{u} \in \ker A$:
      \[
        \vec{x} =
        \begin{pmatrix}
        1 \\
        0 \\
        0 \\
        \end{pmatrix} +
        \lambda \begin{pmatrix}
        1 \\
        -1 \\
        0 \\
        \end{pmatrix} +
        \mu \begin{pmatrix}
        1 \\
        0 \\
        -1 \\
        \end{pmatrix}=
        \begin{pmatrix}
        1 + \lambda + \mu \\
        -\lambda \\
        -\mu \\
        \end{pmatrix}
      \]
      Which describes a plane.
    \end{case}
    \begin{case}{$x = -2$}
      So $\det A = 0$ and
      \[
        A = \begin{pmatrix}
        1 & -2 & 1 \\
        1 & 1 & -2 \\
        -2 & 1 & 1 \\
        \end{pmatrix}
      \]
      Since $\det A = 0$ the columns are linearly dependant and we see that any other pair of two columns is linearly independent so:
      \[
        \im A = \Span\left\{\begin{pmatrix}
        1 \\
        1 \\
        -2 \\
        \end{pmatrix},
        \begin{pmatrix}
        1 \\
        -2 \\
        1 \\
        \end{pmatrix}\right\}
      \]
      We expect $\nullity A = 1$ and can find that $(1, 1, 1)$ is orthogonal to all rows so:
      \[
        \ker A = \Span\left\{\begin{pmatrix}
        1 \\
        1 \\
        1 \\
        \end{pmatrix}\right\}
      \]
      Checking when $b \in \im A$:
      \[
        \vec{b} = \begin{pmatrix}
        1 \\
        y \\
        1 \\
        \end{pmatrix} \in \im A \iff
        \begin{pmatrix}
        1 \\
        y \\
        1 \\
        \end{pmatrix}=
        \begin{pmatrix}
        \alpha + \beta \\
        \alpha - 2\beta \\
        -2\alpha + \beta \\
        \end{pmatrix}
        \iff y = -2
      \]
      If $y \neq -2$, then we have no solutions.

      Otherwise, if $y = -2$, a particular solution of $A\vec{x} = \vec{b}$ is:
      \[
        \vec{x}_0 = \begin{pmatrix}
        0 \\
        0 \\
        1 \\
        \end{pmatrix}
      \]
      So the general solution is $\vec{x} = \vec{x}_0 + \vec{u}$ where $u \in \ker A$:
      \[
        \vec{x} = \begin{pmatrix}
        0 \\
        0 \\
        1 \\
        \end{pmatrix} +
        \lambda \begin{pmatrix}
        1 \\
        1 \\
        1 \\
        \end{pmatrix}=
        \begin{pmatrix}
        \lambda \\
        \lambda \\
        1 + \lambda \\
        \end{pmatrix}
      \]
      Which describes a line.
    \end{case}
  \end{proofcases}
\end{example}
\subsection{Geometric Interpretation of the Homogeneous Case}
Consider the problem $A \vec{u} = \vec{0}$.
Then if $\vec{R}_1, \vec{R}_2, \vec{R}_3$ are the rows of $A$,
\[
  A \vec{u} = \vec{0} \iff \begin{cases}
  \vec{R}_1 \cdot \vec{u} = 0 \\
  \vec{R}_2 \cdot \vec{u} = 0 \\
  \vec{R}_2 \cdot \vec{u} = 0
  \end{cases}
\]
Each of these equations describes a plane that passes through $\vec{0}$ with normal $\vec{R}_i$.
Then, the solution of $A \vec{u} = 0$, which is $\ker A$, is the intersection of the three planes.
\begin{proofcases}
  \begin{case}{$\rank A  = 3$}
    $\rank A = 3 \iff \nullity = 0$ so $\ker A = \vec\{\vec{0}\}$.
    The normals are linearly independent and the planes only all intersect at $\vec{0}$.
  \end{case}
  \begin{case}{$\rank A = 2$}
    $\rank A = 2 \iff \nullity = 1$ so $\ker A$ is a line.
    So the planes all intersect along a line, and the three normals span a plane.
  \end{case}
  \begin{case}{$\rank A  = 1$}
    $\rank A = 1 \iff \nullity = 2$ so $\ker A$ is a plane. So the three planes coincide.
    In this case, all the normals are parallel.
  \end{case}
\end{proofcases}
\subsection{Geometric Interpretation of the General Case}
Consider the problem $A\vec{x} = \vec{b}$.
Then:
\[
  A \vec{x} = \vec{b} \iff \begin{cases}
  \vec{R}_1 \cdot \vec{x} = b_1 \\
  \vec{R}_2 \cdot \vec{x} = b_2 \\
  \vec{R}_2 \cdot \vec{x} = b_3
  \end{cases}
\]
These are three equations of planes with $\vec{R}_i$ but not necessarily passing through $\vec{0}$.
\begin{proofcases}
  \begin{case}{$\rank A = 3$}
    $\rank A = 3 \iff \det A \neq 0$.
    The normals are all linearly independent, and the planes intersect at a single point.
    There is a unique solution for any $\vec{b}$.
  \end{case}
  \begin{case}{$\rank A < 3$}
    $\rank A < 3 \iff \det A = 0$.
    The existence of solutions depends on whether $\vec{b} \in \im A$.
    \begin{proofcases}
      \begin{case}{$\rank A = 2$}
        Planes may intersect in a line, as in the homogeneous case, or there is no solution.

        \textbf{Examples of no solutions:}
        \begin{center}
        \begin{tikzpicture}
          \draw[very thick] (0,0) -- (2, 2);
          \draw[very thick] (-0.1,0.3) -- (3, 0.3);
          \draw[very thick] (2.9, 0) -- (1, 2);
          \node at (1.4, -0.5) {Planes form a prism};
          \begin{scope}[xshift=5cm]
            \draw[very thick] (0,0) -- (2, 2);
            \draw[very thick] (1,0) -- (3, 2);
            \draw[very thick] (2.5, 0) -- (0.7, 2);

            \node at (1.4, -0.5) {Two planes parallel};
          \end{scope}
        \end{tikzpicture}
        \end{center}
      \end{case}
      \begin{case}{$\rank A = 1$}
        Planes may coincide as in the homogeneous case, or there is no solution.

        \textbf{Examples of no solutions:}
        \begin{center}
        \begin{tikzpicture}
          \draw[very thick] (0,0) -- (2, 2);
          \draw[very thick] (1,0) -- (3, 2);
          \draw[very thick] (2.5, 0) -- (0.7, 2);

          \node at (1.4, -0.5) {Two planes parallel};
          \begin{scope}[xshift=5cm]
            \draw[very thick] (0,0) -- (2, 2);
            \draw[very thick] (1,0) -- (3, 2);
            \draw[very thick] (2,0) -- (4, 2);

            \node at (1.7, -0.5) {All planes parallel};
          \end{scope}
        \end{tikzpicture}
        \end{center}
      \end{case}
    \end{proofcases}
  \end{case}
\end{proofcases}
\subsection{Gaussian Elimination}
Consider a system of $m$ equations in $n$ unknowns:
\begin{align*}
  A_{1 1}x_1 + A_{1 2}x_2 + \cdots + A_{1 n}x_n &= b_1 \\
  A_{2 1}x_1 + A_{2 2}x_2 + \cdots + A_{2 n}x_n &= b_2 \\
  \vdots \hspace{11.5em}&\;\;\vdots \\
  A_{m 1}x_1 + A_{m 2}x_2 + \cdots + A_{m n}x_n &= b_m
\end{align*}
Without loss in generality, assume $A_{1 1} \neq 0$ as at this stage we can reorder the equations as desired.
\begin{remark}[Notation]
  We will use a superscript $i$ to denote a coefficient at step $i$ of the process.
  For example $A^{(i)}_{1 2}$ and $b^{(i)}_1$.
\end{remark}
\subsubsection{Steps for Gaussian Elimination}
\begin{enumerate}
  \item We subtract multiples of the first equation from all other equations until the coefficient of $x_1$ in all other equations is 0.
    \begin{align*}
      A^{(1)}_{1 1}x_1 + A^{(1)}_{1 2}x_2 + A^{(1)}_{1 3} + \cdots + A^{(1)}_{1 n}x_n &= b^{(1)}_1 \\
      A^{(1)}_{2 2}x_2 + A^{(1)}_{2 3}x_3 + \cdots + A^{(1)}_{2 n}x_n &= b^{(1)}_2 \\
      A^{(1)}_{3 2}x_2 + A^{(1)}_{3 3}x_3 + \cdots + A^{(1)}_{3 n}x_n &= b^{(1)}_3 \\
      \vdots \hspace{11.5em}&\;\;\vdots \\
      A^{(1)}_{m 2}x_2 + A^{(1)}_{m 3}x_3 + \cdots + A^{(1)}_{m n}x_n &= b^{(1)}_m
    \end{align*}
  \item We then subtract multiples of the second equation from all later equations it until the coefficient of $x_2$ in all later equations is 0.
    \begin{align*}
      A^{(1)}_{1 1}x_1 + A^{(1)}_{1 2}x_2 + A^{(1)}_{1 3} + \cdots + A^{(1)}_{1 n}x_n &= b^{(1)}_1 \\
      A^{(2)}_{2 2}x_2 + A^{(2)}_{2 3}x_3 + \cdots + A^{(2)}_{2 n}x_n &= b^{(2)}_2 \\
      A^{(2)}_{3 3}x_3 + \cdots + A^{(2)}_{3 n}x_n &= b^{(2)}_3 \\
      \vdots \hspace{7.5em}&\;\;\vdots \\
      A^{(2)}_{m 3}x_3 + \cdots + A^{(2)}_{m n}x_n &= b^{(2)}_m
    \end{align*}
  \item Repeat this process for each later equation until there are either no more equations or the left hand side of all further equations is 0.
    The transformed system of equations is then as follows:
    \begin{align*}
      A^{(1)}_{1 1}x_1 + A^{(1)}_{1 2}x_2 + A^{(1)}_{1 3} + \cdots + A^{(1)}_{1 n}x_n &= b^{(1)}_1 \\
      A^{(2)}_{2 2}x_2 + A^{(2)}_{2 3}x_3 + \cdots + A^{(2)}_{2 n}x_n &= b^{(2)}_2 \\
      A^{(3)}_{3 3}x_3 + \cdots + A^{(3)}_{3 n}x_n &= b^{(3)}_3 \\
      \vdots \hspace{7.5em}&\;\;\vdots \\
      A^{(r)}_{r r}x_r + \cdots + A^{(r)}_{r n}x_n &= b^{(r)}_r \\
      0 &= b^{(r)}_{r + 1} \\
      &\;\;\vdots \\
      0 &= b^{(r)}_{m}
    \end{align*}
    where $A^{(i)}_{i i} \neq 0$ for all $i$.
\end{enumerate}
\subsubsection{Outcomes}
\begin{enumerate}
  \item If $r = n \leq m$  and $b^{(i)}_i = 0$ for $i = r + 1, \ldots, m$, then there is a unique solution.

    Since $r = n$, the last equation is simply $A^{(n)}_{n n}x_n = b^{(n)}_n$.
    $A^{(n)}_{n n} \neq 0$ so $x_n$ can be determined.
    We can then substitute this value for $x_n$ back into the previous equation to find $x_{n - 1}$.
    This process can then be reacted to find all other $x_i$.
  \item If $r < m$ and $b^{(r)}_i \neq 0$ for some $i = r + 1, \ldots, m$, then there is no solutions.

    The equations require that $b^{(r)}_i = 0$ for all $i = r + 1, \ldots, m$ so this is inconsistent, thus there is no solution.

    If instead $b^{(r)}_i = 0$ for all $i = r + 1 \ldots m$, then we solve as in \textbf{i}.
  \item If $r = m < n$, then there are infinitely many solutions.

    If we choose values for $x_{r + 1}, \ldots, x_n$, we can solve for $x_1, \ldots, x_r$.
    However, since we could pick any values for $x_{r + 1}, \ldots, x_n$, there are infinitely many solutions.
\end{enumerate}
\subsubsection{Matrix Approach}
We can rewrite this system as $A\vec{x} = \vec{b}$ where $A$ is a $m \times n$ matrix.
This can be re-expressed using row and column operations as above to obtain $M\vec{x} = \vec{d}$ with $M$:
\[
  M = \left(\begin{array}{c|c}
  \begin{matrix}
  M_{1 1} & \cdots & M_{1 r} \\
  \scriptstyle{\substack{0 \\\vdots}} & \ddots & \vdots \\
  \scriptstyle{0} & \cdots\;\scriptstyle{0} & M_{r r} \\
  \end{matrix} &\begin{matrix}
  M_{1 (r+1)} & \cdots & M_{1 n} \\
  \vdots & \ddots & \vdots \\
  M_{r (r+1)} & \cdots & M_{r n} \\
  \end{matrix} \\ \hline
  O & O
\end{array}\right)
\]
this is called an \textit{echelon form}.
The upper left block is \textit{upper triangular}, that is, below the diagonal there are only zeros.
The diagonal of the upper left block is also non zero as $M_{i i} \neq 0$ for $i = 1, \ldots, r$.
The lower two blocks are all zeros.

In the matrix approach:
\[
  r = \rank M = \rank A
\]
Furthermore, if $n = m$, then $\det A = \pm \det M$.
If $n = m = r$, that is M is just the upper triangular block, then $\det M = M_{1 1} \cdots M_{r r} \neq 0$ so both $A$ and $M$ are invertible.

\subsubsection{Examples of Solving via Gaussian Elimination}
\begin{example}[Unique Solution]
  Consider the system of equations:
  \begin{align*}
    x_1 - x_2 + x_3 &= 8 \\
    2x_1 + 3x_2 - x_3 &= -2 \\
    3x_1 - 2x_2 - 9x_3 &= 9 \\
  \end{align*}
  \begin{enumerate}
    \item \textbf{Step 1 -} $\vec{R}_2 \mapsto \vec{R}_2 - 2\vec{R}_1$ and $\vec{R}_3 \mapsto \vec{R}_3 - 3\vec{R}_1$
    \begin{align*}
      x_1 - x_2 + x_3 &= 8 \\
      5x_2 - 3x_3 &= -18 \\
      x_2 - 12x_3 &= -15 \\
    \end{align*}
    \item \textbf{Step 2 -} $\vec{R}_3 \mapsto \vec{R}_3 - \frac{1}{5}\vec{R}_2$
    \begin{align*}
      x_1 - x_2 + x_3 &= 8 \\
      5x_2 - 3x_3 &= -18 \\
      -\frac{57}{3}x_3 &= -\frac{57}{3} \\
    \end{align*}
  \end{enumerate}
  Last non-zero equation has unknown $x_3$ so $r = 3$.
  Since $r = m = n = 3$, we should have a unique solution which we can obtain via back substitution:
  \begin{align*}
    \frac{-57}{3}x_3 = -\frac{57}{3} &\implies x_3 = 1 \\
    5x_2 - 3(1) = -18 &\implies x_2 = -3 \\
    x_1 - (-3)+ 1 = 8 &\implies x_1 = 4
  \end{align*}
\end{example}
\begin{example}[Overdetermined System]
  Consider the same system of equations but with an additional equation:
  \begin{align*}
    x_1 - x_2 + x_3 &= 8 \\
    2x_1 + 3x_2 - x_3 &= -2 \\
    3x_1 - 2x_2 - 9x_3 &= 9 \\
    - 5x_2 + x_3 &= b
  \end{align*}
  \begin{enumerate}
    \item \textbf{Step 1 -} Identical to before as 4th equation does not include $x_1$.
    \item \textbf{Step 2 -} Additionally apply $\vec{R}_4 \mapsto \vec{R}_4 + \vec{R}_2$ to yield:
      \[
        -2x_3 = b - 18
      \]
    \item \textbf{Step 3 -} Apply $\vec{R}_4 \mapsto \vec{R}_4 - \frac{6}{57}\vec{R}_3$ to yield the transformed system:
      \begin{align*}
        x_1 - x_2 + x_3 &= 8 \\
        5x_2 - 3x_3 &= -18 \\
        -\frac{57}{3}x_3 &= -\frac{57}{3} \\
        0 &= b - 16
      \end{align*}
  \end{enumerate}
  Again $r = 3$, but now $r = 3 < m = 4$.
  If $b = 16$, then the equations are consistent so the solution is as before.
  Otherwise, there is no solution.
\end{example}
\begin{example}[Underdetermined System]
  Consider the same system of equations but with only two equations
  \begin{align*}
    x_1 - x_2 + x_3 &= 8 \\
    2x_1 + 3x_2 - x_3 &= -2
  \end{align*}
  The transformed system is:
  \begin{align*}
    x_1 - x_2 + x_3 &= 8 \\
    5x_2 - 3x_3 &= -18
  \end{align*}
  So the system is \textit{underdetermined}, $r = 2 = m < n = 3$.
  We can pick any value for $x_3$, say $t$ and then solve for $x_1, x_2$:
  \[
    x_1 = \frac{2}{5}(11 - t),\ x_2 = \frac{3}{5}(t - 6)
  \]
\end{example}
\end{document}
