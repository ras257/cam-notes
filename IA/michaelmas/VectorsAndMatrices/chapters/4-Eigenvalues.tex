\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Eigenvalues and Eigenvectors}
\section{Introduction}
\begin{theorem}[Fundemental Theorem of Algebra]
  \label{FTA}
  Let $p(z)$ be a polynomial of degree $m \geq 1$:
  \[
    p(z) = \sum_{j = 0}^{m}  c_jz^{j}
  \]
  where $c_j \in \C$, and $c_m \neq 0$.
  Then $p(z) = 0$ has precisely $m$ roots (not necessarily distinct) counted with multiplicity in $\C$.
\end{theorem}
\begin{definition}[Multiplicity]
  The root $z = w$ has \textit{multiplicity} $k$ if $(z - w)^{k}$ is a factor of $p(z)$ but $(z - w)^{k + 1}$ is not.
\end{definition}
\begin{definition}[Eigenvector and Eigenvalue]
  Let $T: V \to V$ be a linear map.
  Then $v \in V$ with $\vec{v} \neq 0$ is an \textit{eigenvector} of $T$ if:
  \[
    T(\vec{v}) = \lambda \vec{v}
  \]
  for a scalar $\lambda$ called the \textit{eigenvalue}.
\end{definition}
If $V = \R^{n} \text{ or } \C^{n}$, and $T$ is given in terms of an $n \times n$ matrix $A$, then:
\[
  A\vec{v} = \lambda \vec{v} \iff (A - \lambda I)\vec{v} = 0
\]
For a given $\lambda$, this holds for some vector $\vec{v} \neq 0$ if and only if $\det (A - \lambda I) = 0$.
This is because we require the kernel of $A - \lambda I$ to be nontrivial.

The equation obtained from this is called the \textit{characteristic equation}.
\begin{definition}[Characteristic Polynomial]
  For a matrix $A$, the \textit{characteristic polynomial} of degree $n$, $\chi_A$, is defined as:
  \[
    \chi_A(t) = \det (A - t I)
  \]
\end{definition}
From the definition of the determinant,
\begin{align*}
  \chi_A(t) &= \det (A - tI) \\
            &= \levi_{j_1 \ldots j_n}(A_{j_1 1} - t\delta_{j_1 1}) \cdots (A_{j_n n} - t\delta_{j_n n}) \\
            &= c_0 + c_1 t + \cdots + c_n t^{n}
\end{align*}
for some constants $c_0, \ldots, c_n$.

From this we can conclude the following:
\begin{enumerate}
  \item $\chi_A(t)$ has degree $n$, and thus has $n$ roots.
    Hence, an $n \times n$ matrix has $n$ eigenvalues accounting for multiplicity.
  \item If $A$ is real, then all $c_i \in \R$, and thus eigenvalues are either real or come in conjugate pairs.
  \item \label{eigenvalueTrDet}
    $c_n = (-1)^{n}$ as we multiply $-t$ by itself $n$ times.
    Moreover:
    \[
      c_{n - 1} = (-1)^{n - 1}(A_{1 1} + \ldots + A_{n n}) = (-1)^{n - 1}\tr A
    \]
    We also know that $-\frac{c_{n - 1}}{c_n}$ is the sum of the roots, hence:
    \[
      t_1 + \cdots + t_n = -\frac{c_{n - 1}}{(-1)^n} = -\frac{(-1)^{n - 1}}{(-1)^{n}} = \tr A
    \]
    so the trace of a matrix is the sum of its eigenvalues.

    Finally,
    \[
      c_0 = \chi_A(0) = \det A
    \]
    We also know that $(-1)^{n} \frac{c_0}{c_n} = c_0$ is the product of the roots, hence:
    \[
      \det A = t_1 \cdots t_n
    \]
    so the determinant of a matrix is the product of its eigenvalues.
\end{enumerate}
\begin{example}
  \begin{enumerate}
    \item $V = \C^{2}$, $A = \begin{pmatrix}
        0 & -1 \\
        1 & 0 \\
        \end{pmatrix}$

      \textbf{Characteristic Polynomial}
      \[
        \chi_A(t) = \det (A - tI) = \det \begin{pmatrix}
        -t & -1 \\
        1 & -t \\
        \end{pmatrix} = t^2  + 1
      \]
      \textbf{Eigenvalues -} $\lambda = \pm i$.\par
      \textbf{Eigenvectors -}
      To get the eigenvector for $\lambda = i$:
      \[
        (A - iI)\vec{v} =
        \begin{pmatrix}
        -i & -1 \\
        1 & -i \\
        \end{pmatrix}
        \begin{pmatrix}
        x \\
        y \\
        \end{pmatrix} = \vec{0} \implies
        -ix - y = 0
      \]
      So the associated eigenvector is:
      \[
        \vec{v} = \alpha \begin{pmatrix}
        1 \\
        -i \\
        \end{pmatrix}
      \]
      for all $\alpha \in \C$.
      Similarly, for $\lambda = -i$, the associated eigenvector is  $\vec{v} = \alpha\begin{pmatrix}
      1 \\
      i \\
      \end{pmatrix}$ for all $\alpha \in \C$.
    \item $V = \R^2$, $A = \begin{pmatrix}
      1 & 1 \\
      0 & 1 \\
      \end{pmatrix}$

      \textbf{Characteristic Polynomial -} $\chi_A(t) = (t - 1)^2$.\par
      \textbf{Eigenvalues -} $\lambda  = 1$ with multiplicity 2.\par
      \textbf{Eigenvectors -} To get the eigenvector for $\lambda = 1$:
      \[
        (A - 1I)\vec{v} = \vec{0}  \iff \begin{pmatrix}
        0 & 1 \\
        0 & 0 \\
        \end{pmatrix}
        \begin{pmatrix}
        x \\
        y \\
        \end{pmatrix} = \vec{0} \implies y = 0
      \]
      So the associated eigenvector is $\vec{v} = \alpha \begin{pmatrix}
      1 \\
      0 \\
      \end{pmatrix}$ for all $\alpha \in \R$.
  \end{enumerate}
\end{example}
\section{Eigenspaces and Multiplicity}
\begin{definition}[Eigenspace]
  For an eigenvalue $\lambda$ of a matrix $A$, we define its \textit{eigenspace} by:
  \[
    E_\lambda = \{\vec{v} : A \vec{v} = \lambda \vec{v}\} = \ker (A - \lambda I)
  \]
  It is the collection of eigenvectors associated with a particular eigenvalue.
\end{definition}
\begin{definition}[Algebraic Multiplicity]
  The \textit{algebraic multiplicity}, denoted $M(\lambda)$ or $M_\lambda$, of an eigenvalue $\lambda$ is the multiplicity of $\lambda$ in the characteristic polynomial $\chi_A(\lambda)$.
\end{definition}
By the fundamental theorem of algebra (\cref{FTA}), if $A$ is a $n \times n$ matrix, then the sum of all the algebraic multiplicities must be $n$. That is:
\[
  \sum_{\lambda} M(\lambda) = n
\]
\begin{definition}[Geomtric Multiplicity]
  The \textit{geometric multiplicity}, denoted $m(\lambda)$ or $m_\lambda$, of an eigenvalue $\lambda$ is the dimension of its eigenspace:
  \[
    m(\lambda) = \dim (E_\lambda)
  \]
\end{definition}
Equivalently, $m(\lambda)$ is the maximum number of linearly independent eigenvectors associated with the eigenvalue $\lambda$.
\begin{proposition}
  \label{multiplicityRelation}
  If $A$ is an $n \times n$ matrix with an eigenvalue $\lambda$, then $M_\lambda \geq m_\lambda$
\end{proposition}
Note that this means if an eigenvalue as $m_\lambda = 1 \implies M_\lambda = 1$ since $M_\lambda > 0$.
\begin{definition}
  The \textit{defect}, denoted $\Delta_\lambda$ of an eigenvalue $\lambda$ is defined as:
  \[
    \Delta_\lambda = M_\lambda - m_\lambda
  \]
\end{definition}
By \cref{multiplicityRelation}, we have $\Delta_\lambda \geq 0$.
\begin{example}
  \begin{enumerate}
    \item Consider the matrix:\[
      A = \begin{pmatrix}
      4 & 1 & 0 \\
      0 & 4 & 1 \\
      0 & 0 & 4 \\
      \end{pmatrix}
    \]
    \textbf{Characteristic Polynomial -} $\chi_A(t) = (4 - t)^3$.\par
    \textbf{Eigenvalues -} $A$ only has one eigenvalue $\lambda = 4$ with $M_\lambda = 3$.\par
    \textbf{Eigenvectors}
    \[
      (A - 4\lambda)\vec{v} = \begin{pmatrix}
      0 & 1 & 0 \\
      0 & 0 & 1 \\
      0 & 0 & 0 \\
      \end{pmatrix}
      \begin{pmatrix}
      x \\
      y \\
      z \\
      \end{pmatrix} = \vec{0} \implies
      y = z = 0
    \]
    So the associated eigenvector is $\vec{v} = \alpha \begin{pmatrix}
    1 \\
    0 \\
    0 \\
    \end{pmatrix}$, thus $m_\lambda = 1$ as the eigenspace is:
    \[
      E_\lambda = \Span\left\{\begin{pmatrix}
      1 \\
      0 \\
      0 \\
      \end{pmatrix}\right\} \implies \dim (E_\lambda) = 1
    \]
    Therefore the defect is $\Delta_{\lambda} = 3 - 1 = 2$.
  \item Consider a reflection in the plane through $\vec{0}$ with normal $\vec{n}$.
    We do not need to use the matrix representation of this to compute the eigenvalues.
    Instead, we can use geometric intuition.

    Since $\vec{n}$ is just sent to $-\vec{n}$, that is, $H\vec{n} = -\vec{n}$, we have an eigenvector $\vec{n}$ with eigenvalue $\lambda = -1$.
    So $E_{-1} = \Span\{\vec{n}\}$

    Moreover, any vector $\vec{u}$ on the plane is unchanged, that is, $H \vec{u} = \vec{u}$ for all $\vec{u} \perp \vec{n}$.
    So we also have eigenvalue $\lambda = 1$ with $E_1 = \{\vec{x} : \vec{x} \cdot \vec{n} = \vec{0}\}$.

    Since $m_{-1} = 1$ and $m_1 = 2$ and $m_1 + m_2 \leq 3$, we must have all eigenvalues.
    Furthermore, since $M_\lambda \geq m_\lambda$, we must have $M_1 = 2$ and $M_{1} = 1$ so the geometric and algebraic multiplicities coincide for each eigenvalue.
  \item Consider a rotation $\Rot(\theta) \in \R^2$,
    \[
      \Rot(\theta) = \begin{pmatrix}
      \cos \theta & -\sin \theta \\
      \sin \theta & \cos \theta \\
      \end{pmatrix}
    \]
    \textbf{Characteristic Polynomial -} $\chi_{\Rot(\theta)}(t) = t^2 - 2t \cos \theta + 1$.\par
    \textbf{Eigenvalues -} It does not have real eigenvalues but does have eigenvalues in $\C$, $\lambda = e^{\pm i\theta}$.
    Note that this is a conjugate pair, as expected.\par
    \textbf{Eigenvectors -} The associated eigenvectors are:
    \[
      \vec{v} = \alpha \begin{pmatrix}
      1 \\
      \mp i \\
      \end{pmatrix}
    \]
    Since these both have $m_\lambda = 1$, they must also both have $M_\lambda = 1$.
  \item Consider a rotation by and $\theta$ about $\vec{n}$, $R(\theta, \vec{n})$.
    Note that:
    \[
      R(\theta, \vec{n})\vec{n} = \vec{n}
    \]
    So we have an eigenvalue $\lambda = 1$ with eigenspace $E_\lambda = \Span\{\vec{n}\}$.

    If we consider a basis given by $\vec{n}$ and two orthogonal vectors lying in the plane, $\vec{n}$ is unchanged under $R$ whereas the two vectors are rotated in the plane, similarity to as in \textbf{iii}.

    From \textbf{iii}, we know the rotation in the plane has eigenvalues $e^{\pm i \theta}$.
    Note that there are no other real eigenvalues unless $\theta = n\pi$, in which case, all vectors on the plane are eigenvectors with eigenvalue $\lambda = -1$ and algebraic multiplicity $M_{-1} = 2$ as both $e^{+i\pi}$ and $e^{-i\pi}$ coincide.

    So we have found all three complex eigenvalues.
  \item Consider the matrix
    \[
      A = \begin{pmatrix}
      -3 & -1 & 1 \\
      -1 & -3 & 1 \\
      -2 & -2 & 0 \\
      \end{pmatrix}
    \]
    \textbf{Characteristic Polynomial -} $\chi_A(t) = -(t + 2)^3$.\par
    \textbf{Eigenvalues -} $A$ only has eigenvalue $\lambda = -2$ with $M_\lambda = 3$.\par
    \textbf{Eigenvectors -} For the eigenvectors:
    \[
      (A + 2I) \vec{v} =
      \begin{pmatrix}
      -1 & -1 & 1 \\
      -1 & -1 & 1 \\
      -2 & -2 & 2 \\
      \end{pmatrix}
      \begin{pmatrix}
      x \\
      y \\
      z \\
      \end{pmatrix} = \vec{0}
    \]
    By eliminating $z$, we see that the general solution for this is:
    \[
      \begin{pmatrix}
      x \\
      y \\
      x + y \\
      \end{pmatrix} =
      x \begin{pmatrix}
      1 \\
      0 \\
      1 \\
      \end{pmatrix} + y\begin{pmatrix}
      0 \\
      1 \\
      1 \\
      \end{pmatrix}
    \]
    Therefore:
    \[
      E_\lambda = \Span\left\{\begin{pmatrix}
      1 \\
      0 \\
      1 \\
      \end{pmatrix},
      \begin{pmatrix}
      0 \\
      1 \\
      1 \\
      \end{pmatrix}\right\} \implies m_\lambda = 2
    \]
    So the eigenvalues do not form a basis of $\C^{3}$.
    The defect is $\Delta_\lambda = 3 - 2 = 1$.
  \end{enumerate}
\end{example}
\section{Diagonalisation and Similarity}
\begin{proposition}[Diagonalisble]
  \label{diagonalProp}
  If $A$ is a $n \times n$ matrix acting on $V = \R^{n}$ or $\C^{n}$, then the following statements are equivalent:
  \begin{itemize}
    \item There exists a basis of eigenvectors for $V$, $\{\vec{v}_1, \ldots, \vec{v}_n\}$ with $A \vec{v}_i = \lambda_i \vec{v}_i$.

      \textit{(Summation convention not used)}
    \item There exits an $n \times n$ invertible matrix $P$ with:
      \[
        P^{-1} A P = D = \begin{pmatrix}
        \lambda_1 &  &  \\
         & \ddots &  \\
         &  & \lambda_n \\
        \end{pmatrix}
      \]
      where $D$ is a \textit{diagonal matrix}, that is, all non-diagonal entries are $0$.
  \end{itemize}
  If either of these conditions is satisfied, $A$ is \textit{diagonalisable}.
\end{proposition}
\subsection{Linearly Independent Eigenvectors}
\begin{theorem}
  \label{linearlyIndependantEigenvectors}
  If an $n \times n$ matrix $A$ has distinct eigenvalues $\lambda_1, \ldots, \lambda_r$ with corresponding eigenvectors $\vec{v}_1, \ldots, \vec{v}_r$ then, $\{\vec{v}_1, \ldots, \vec{v}_r\}$ are linearly independent.
\end{theorem}
\begin{proof}
  Suppose, for contradiction, that $\{\vec{v}_1, \ldots, \vec{v}_r\}$ are linearly dependant.
  Then:
  \[
    \sum_{j = 1}^{r} \alpha_j \vec{v}_j = \vec{0}
  \]
  where not all $\alpha_j = 0$.

  There will be at least one such linear combination but we can take the minimal $p$ for which $\exists \alpha_1, \ldots, \alpha_p \neq 0$ with:
  \[
    \sum_{j = 1}^{p} \alpha_j \vec{v}_j = \vec{0}
  \]
  We are able to assume WLOG that this minimal linear combination occurs with the first $p$ vectors as we can just relabel the vectors.

  If we then apply the matrix $(A - \lambda_1 I)$, then we remove the first term of the sum, that is:
  \[
    (A - \lambda_1 I) \sum_{j = 1}^{p} \alpha_j \vec{v}_j = \sum_{j = 2}^{p} \alpha_j(\lambda_j - \lambda_1)\vec{v}_j
  \]
  Since $(A - \lambda_1 I)\vec{v}_1 = \vec{0}$.

  The above is a linear combination with $p - 1$ non-zero coefficients which contradicts the fact that $p$ was minimal.
  Thus $\{\vec{v}_1, \ldots, \vec{v}_r\}$ is linearly independent.
\end{proof}
\begin{remark}[Note]
  The above theorem and proof only hold when each eigenvalue has geometric multiplicity one, that is, each eigenvalue has exactly one associated eigenvector.
  However this does generalise to the following:

  Let $B_\lambda$ be a basis for the eigenspace $E_\lambda$ associated with $\lambda$.
  If $\lambda_1, \ldots, \lambda_r$ are distinct, then $B_{\lambda_1} \cup \cdots \cup B_{\lambda_r}$ is a linearly independent set.
\end{remark}
We can now prove \cref{diagonalProp}
\begin{proof}[\Cref{diagonalProp}]
  For any matrix $P$:
  \begin{itemize}
    \item $AP$ has columns $A\vec{C}_i(P)$.
    \item $PD$ has columns $\lambda_i\vec{C}_i(P)$.
  \end{itemize}
  where $D$ is a diagonal matrix as in the proposition.
  So $AP = PD \iff A \vec{C}_i(P) = \lambda_i \vec{C}_i(P)\ \forall i = 1, \ldots, n$.

  \begin{proofdirection}{Suppose there exists a basis of eigenvectors for $V$}
    Given a basis of eigenvectors for $V$, the above relation allows us to determine such a $P$ as $\vec{C}_i(P)$ satisfies $A\vec{C}_i(P) = \lambda \vec{C}_i$ so we can take $\vec{C}_i(P) = \vec{v}_i$.

    Since we know what the eigenvectors form a basis for $V$, they are linearly independent and therefore the columns of $P$ are linearly independent so $\det P \neq 0$.
    Thus $P$ is invertible so we have constructed a $P$ such that $P^{-1} A P = D$.
  \end{proofdirection}
  \begin{proofdirection}{Suppose there exists such an invertible $P$}
    Since $P$ is invertible, $P^{-1} A P = D \iff AP = PD$.
    By the relation $A\vec{C}_i(P) = \lambda_i \vec{C}_i(P)$, we see that the columns of $P$ are eigenvectors.
    The columns of $P$ span $V$ so we have found a basis of eigenvectors for $V$.
  \end{proofdirection}
\end{proof}
\subsection{Criteria For Diagonalisability}
\subsubsection{A sufficient but not necessary condition}
\begin{proposition}
  If an $n \times n$ matrix has $n$ distinct eigenvalues, then it is diagonalisable.
\end{proposition}
\begin{proof}
If an $n \times n$ matrix has $n$ distinct eigenvalues, then by \cref{linearlyIndependantEigenvectors}, we have $n$ eigenvectors which are linearly independent.
This provides a basis of eigenvectors for $\R^{n}$ or $\C^{n}$.
So from \cref{diagonalProp}, $A$ is diagonalisable.
\end{proof}
Note that we require that all $n$ eigenvalues are distinct as otherwise we could have an eigenvalue with $m_\lambda < M_\lambda$ so would have less than $n$ linearly independent eigenvectors so would be unable to form a basis.
\subsubsection{A necessary and sufficient condition}
\begin{proposition}
  $A$ is diagonalisable if and only if for all eigenvalues $\lambda$, $M_\lambda = m_\lambda$
\end{proposition}
\begin{proof}
  \begin{proofdirection}{Suppose $A$ is diagonalisable}
    Then by \cref{diagonalProp}, there exists a basis of $n$ eigenvectors for $V$.

    Suppose we have $m_{\lambda_i} < M_{\lambda_i}$ for some $\lambda_i$.
    Then since $m_\lambda \leq M_\lambda$ for all eigenvalues, $\sum_{\lambda} m_\lambda < \sum_{\lambda} M_\lambda = n$.

    However, this implies that $\sum_{\lambda} m_\lambda < n$ which contradicts the fact the eigenvectors form a basis for $V$.
    So we must have $m_\lambda = M_\lambda$ for all eigenvalues.
  \end{proofdirection}
  \begin{proofdirection}{Suppose $M_\lambda = m_\lambda$ for all $\lambda$}
    If $\lambda_i$ with $i = 1, \ldots, r$ are all the distinct eigenvalues of a matrix, then $B_{\lambda_1} \cup \cdots \cup B_{\lambda_r}$ is a linearly independent set, and its number of elements is:
    \[
      \sum_{i} m_i = \sum_{i} M_i = n
    \]
    so it must form a basis.
  \end{proofdirection}
\end{proof}
\subsection{Similarity}
\begin{definition}
  We say that two $n \times n$ matrices $A$ and $B$ are \textit{similar} if there exists an $n \times n$ invertible matrix $P$ such that:
  \[
    B = P^{-1}AP
  \]
\end{definition}
\begin{remark}
  Similar matrices represent the same linear map with respect to different bases.
\end{remark}
\begin{proposition}
  If $A$ and $B$ are similar, then:
  \begin{enumerate}
    \item $\tr A= \tr B$
    \item $\det A = \det B$
    \item $\chi_A(t) = \chi_B(t)$
  \end{enumerate}
\end{proposition}
\begin{proof}
  \begin{enumerate}
    \item Using $\det AB = \det A \det B$ and $\det P^{-1} = (\det P)^{-1}$, we see that:
      \[
        \det B = \det (P^{-1} A P) = \det (P^{-1}) \det (A) \det (P) = \det A
      \]
    \item Using $\tr(AB) = \tr(BA)$, we see that:
      \[
        \tr B = \tr(P^{-1} A P) = \tr(APP^{-1}) = \tr A
      \]
    \item Take an eigenvector $\vec{v}$ of $B$ with eigenvalue $\lambda$.
      \[
        (P^{-1}AP)\vec{v} = \lambda \vec{v} \iff A(P \vec{v}) = \lambda (P \vec{v})
      \]
      So $P \vec{v}$ is an eigenvector of $A$ with eigenvalue $\lambda$.
      Therefore the roots of $\chi_A$ and $\chi_B$ are the same.
      Since $A$ and $B$ are both $n \times n$ matrices, the leading coefficient of both $\chi_A$ and $\chi_B$ is $(-1)^{n}$.
      Thus $\chi_A(t) = \chi_B(t)$.
  \end{enumerate}
\end{proof}
\begin{remark}
  For the particular case of $B = D$ where $D$ is a diagonal matrix, this means that $A$ is diagonalisable.
  Utilising the fact that the determinant of a diagonal matrix is the product of the diagonal entries, we then have:
  \begin{enumerate}
    \item $\tr A = \sum_{i} \lambda_i$
    \item $\det A = \prod_{i} \lambda_i$
    \item $\chi_A(t) = (\lambda_1 - t) \cdots(\lambda_n - t)$
  \end{enumerate}
  These results are consistent with \cref{eigenvalueTrDet}.
\end{remark}
\subsection{Hermitian and Symmetric Matrices}
\begin{remark}[Recap] See \cref{transposeAndConjugate,complexInnerProduct} for further details.\par
  \textbf{Hermitian -} $A$ is Hermitian if and only if:
  \[
    A^{\dag} = \overline{(A^{\trans})} = A \iff A_{i j} = \overline{A_{j i}}
  \]
  \textbf{Symmetric -} $A$ is symmetric if and only if:
  \[
    A^{\trans} = A \iff A_{i j} = A_{j i}
  \]
  \textbf{Complex Inner Product -} For $\vec{v}, \vec{w} \in \C^{n}$:
  \[
    \vec{v}^{\dag}\vec{w} = \sum_{i} \overline{v_i}w_i
  \]
  \textbf{Real Inner Product - } For $\vec{v}, \vec{w} \in \R^{n}$, this reduces to:
  \[
    \vec{v}^{\trans}\vec{w} = \sum_{i} v_i w_i
  \]
\end{remark}
\begin{remark}
  If $A$ is hermitian, then:
  \[
    (A \vec{v})^{\dag}\vec{w} = \vec{v}^{\dag}(A\vec{w})
  \]
\end{remark}
\begin{theorem}
  If $A$ is a $n \times n$ Hermitian matrix, then:
  \begin{enumerate}
    \item Every eigenvalue of $A$ is real.
    \item Eigenvectors $\vec{v}, \vec{w}$ with distinct eigenvalues, $\lambda, \mu$ are orthogonal, that is, $\vec{v}^{\dag} \vec{w} = 0$
    \item If $A$ is symmetric, then for any eigenvalues $\lambda$, we can construct a real eigenvector $\vec{v}$ so that \textbf{ii} reduces to  $\vec{v}^{\trans} \vec{w} = 0$
  \end{enumerate}
\end{theorem}
\begin{proof}
  \begin{enumerate}
    \item Consider an eigenvector $\vec{v}$ with eigenvalue $\lambda$, then:
    \begin{align*}
      \vec{v}^{\dag}(A \vec{v}) &= (A \vec{v})^{\dag}\vec{v} \\
      \vec{v}^{\dag}(\lambda \vec{v}) &=(\lambda \vec{v})^{\dag} \vec{v} \\
      \lambda \vec{v}^{\dag}\vec{v} &= \overline{\lambda}\vec{v}^{\dag}\vec{v}
    \end{align*}
    Since $\vec{v} \neq \vec{0}$, $\lambda = \overline{\lambda} \implies \lambda \in \R$.
  \item Consider eigenvectors $\vec{v}, \vec{w}$ with eigenvalues $\lambda, \mu$ respectively:
    \begin{align*}
      \vec{v}^{\dag}(A \vec{w}) &= (A \vec{v})^{\dag} \vec{w} \\
      \mu \vec{v}^{\dag}\vec{w} &= \overline{\lambda} \vec{v}^{\dag} \vec{w} \\
      \mu \vec{v}^{\dag}\vec{w} &= \lambda \vec{v}^{\dag} \vec{w}
    \end{align*}
    Since $\lambda \neq \mu$ we must have $\vec{v}^{\dag}\vec{w} = 0$, so $\vec{v}$ and $\vec{w}$ are orthogonal.
  \item Since $A$ is Hermitian and symmetric, $A$ must be real.
    From \textbf{i}, all eigenvalues must be real also.

    Consider an eigenvector $\vec{v} \in \C^{n}$ with eigenvalue $\lambda \in \R$.
    Let $\vec{v} = \vec{u} + i \vec{u}'$ with $\vec{u}, \vec{u}' \in \R^{n}$.
    Then:
    \begin{align*}
      A \vec{v} = \lambda \vec{v} \\
      A \vec{u} + i(A \vec{u}') = \lambda \vec{u} + i(\lambda \vec{u})
    \end{align*}
    Since $\lambda \in \R$, we must have:
    \[
      A \vec{u} = \lambda \vec{u} \text{ and } A \vec{u}' = \lambda \vec{u}'
    \]
    Furthermore, since $\vec{v} \neq \vec{0}$, at least one of $\vec{u}, \vec{u}' \neq \vec{0}$ so there is at least one real eigenvector.
  \end{enumerate}
\end{proof}
\end{document}
