\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Eigenvalues and Eigenvectors}
\section{Introduction}
\begin{theorem}[Fundemental Theorem of Algebra]
  \label{FTA}
  Let $p(z)$ be a polynomial of degree $m \geq 1$:
  \[
    p(z) = \sum_{j = 0}^{m}  c_jz^{j}
  \]
  where $c_j \in \C$, and $c_m \neq 0$.
  Then $p(z) = 0$ has precisely $m$ roots (not necessarily distinct) counted with multiplicity in $\C$.
\end{theorem}
\begin{definition}[Multiplicity]
  The root $z = w$ has \textit{multiplicity} $k$ if $(z - w)^{k}$ is a factor of $p(z)$ but $(z - w)^{k + 1}$ is not.
\end{definition}
\begin{definition}[Eigenvector and Eigenvalue]
  Let $T: V \to V$ be a linear map.
  Then $v \in V$ with $\vec{v} \neq 0$ is an \textit{eigenvector} of $T$ if:
  \[
    T(\vec{v}) = \lambda \vec{v}
  \]
  for a scalar $\lambda$ called the \textit{eigenvalue}.
\end{definition}
If $V = \R^{n} \text{ or } \C^{n}$, and $T$ is given in terms of an $n \times n$ matrix $A$, then:
\[
  A\vec{v} = \lambda \vec{v} \iff (A - \lambda I)\vec{v} = 0
\]
For a given $\lambda$, this holds for some vector $\vec{v} \neq 0$ if and only if $\det (A - \lambda I) = 0$.
This is because we require the kernel of $A - \lambda I$ to be nontrivial.

The equation obtained from this is called the \textit{characteristic equation}.
\begin{definition}[Characteristic Polynomial]
  For a matrix $A$, the \textit{characteristic polynomial} of degree $n$, $\chi_A$, is defined as:
  \[
    \chi_A(t) = \det (A - t I)
  \]
\end{definition}
From the definition of the determinant,
\begin{align*}
  \chi_A(t) &= \det (A - tI) \\
            &= \levi_{j_1 \ldots j_n}(A_{j_1 1} - t\delta_{j_1 1}) \cdots (A_{j_n n} - t\delta_{j_n n}) \\
            &= c_0 + c_1 t + \cdots + c_n t^{n}
\end{align*}
for some constants $c_0, \ldots, c_n$.

From this we can conclude the following:
\label{eigenvalueTrDet}
\begin{enumerate}
  \item $\chi_A(t)$ has degree $n$, and thus has $n$ roots.
    Hence, an $n \times n$ matrix has $n$ eigenvalues accounting for multiplicity.
  \item If $A$ is real, then all $c_i \in \R$, and thus eigenvalues are either real or come in conjugate pairs.
  \item
    $c_n = (-1)^{n}$ as we multiply $-t$ by itself $n$ times.
    Moreover:
    \[
      c_{n - 1} = (-1)^{n - 1}(A_{1 1} + \ldots + A_{n n}) = (-1)^{n - 1}\tr A
    \]
    We also know that $-\frac{c_{n - 1}}{c_n}$ is the sum of the roots, hence:
    \[
      t_1 + \cdots + t_n = -\frac{c_{n - 1}}{(-1)^n} = -\frac{(-1)^{n - 1}}{(-1)^{n}} = \tr A
    \]
    so the trace of a matrix is the sum of its eigenvalues.

    Finally,
    \[
      c_0 = \chi_A(0) = \det A
    \]
    We also know that $(-1)^{n} \frac{c_0}{c_n} = c_0$ is the product of the roots, hence:
    \[
      \det A = t_1 \cdots t_n
    \]
    so the determinant of a matrix is the product of its eigenvalues.
\end{enumerate}
\begin{example}
  \begin{enumerate}
    \item $V = \C^{2}$, $A = \begin{pmatrix}
        0 & -1 \\
        1 & 0 \\
        \end{pmatrix}$

      \textbf{Characteristic Polynomial}
      \[
        \chi_A(t) = \det (A - tI) = \det \begin{pmatrix}
        -t & -1 \\
        1 & -t \\
        \end{pmatrix} = t^2  + 1
      \]
      \textbf{Eigenvalues -} $\lambda = \pm i$.\par
      \textbf{Eigenvectors -}
      To get the eigenvector for $\lambda = i$:
      \[
        (A - iI)\vec{v} =
        \begin{pmatrix}
        -i & -1 \\
        1 & -i \\
        \end{pmatrix}
        \begin{pmatrix}
        x \\
        y \\
        \end{pmatrix} = \vec{0} \implies
        -ix - y = 0
      \]
      So the associated eigenvector is:
      \[
        \vec{v} = \alpha \begin{pmatrix}
        1 \\
        -i \\
        \end{pmatrix}
      \]
      for all $\alpha \in \C$.
      Similarly, for $\lambda = -i$, the associated eigenvector is  $\vec{v} = \alpha\begin{pmatrix}
      1 \\
      i \\
      \end{pmatrix}$ for all $\alpha \in \C$.
    \item $V = \R^2$, $A = \begin{pmatrix}
      1 & 1 \\
      0 & 1 \\
      \end{pmatrix}$

      \textbf{Characteristic Polynomial -} $\chi_A(t) = (t - 1)^2$.\par
      \textbf{Eigenvalues -} $\lambda  = 1$ with multiplicity 2.\par
      \textbf{Eigenvectors -} To get the eigenvector for $\lambda = 1$:
      \[
        (A - 1I)\vec{v} = \vec{0}  \iff \begin{pmatrix}
        0 & 1 \\
        0 & 0 \\
        \end{pmatrix}
        \begin{pmatrix}
        x \\
        y \\
        \end{pmatrix} = \vec{0} \implies y = 0
      \]
      So the associated eigenvector is $\vec{v} = \alpha \begin{pmatrix}
      1 \\
      0 \\
      \end{pmatrix}$ for all $\alpha \in \R$.
  \end{enumerate}
\end{example}
\section{Eigenspaces and Multiplicity}
\begin{definition}[Eigenspace]
  For an eigenvalue $\lambda$ of a matrix $A$, we define its \textit{eigenspace} by:
  \[
    E_\lambda = \{\vec{v} : A \vec{v} = \lambda \vec{v}\} = \ker (A - \lambda I)
  \]
  It is the collection of eigenvectors associated with a particular eigenvalue.
\end{definition}
\begin{definition}[Algebraic Multiplicity]
  The \textit{algebraic multiplicity}, denoted $M(\lambda)$ or $M_\lambda$, of an eigenvalue $\lambda$ is the multiplicity of $\lambda$ in the characteristic polynomial $\chi_A(\lambda)$.
\end{definition}
By the fundamental theorem of algebra (\cref{FTA}), if $A$ is a $n \times n$ matrix, then the sum of all the algebraic multiplicities must be $n$. That is:
\[
  \sum_{\lambda} M(\lambda) = n
\]
\begin{definition}[Geomtric Multiplicity]
  The \textit{geometric multiplicity}, denoted $m(\lambda)$ or $m_\lambda$, of an eigenvalue $\lambda$ is the dimension of its eigenspace:
  \[
    m(\lambda) = \dim (E_\lambda)
  \]
\end{definition}
Equivalently, $m(\lambda)$ is the maximum number of linearly independent eigenvectors associated with the eigenvalue $\lambda$.
\begin{proposition}
  \label{multiplicityRelation}
  If $A$ is an $n \times n$ matrix with an eigenvalue $\lambda$, then $M_\lambda \geq m_\lambda$
\end{proposition}
\begin{proof}
  See \cref{multiplicityProof}.
\end{proof}
Note that this means if an eigenvalue as $m_\lambda = 1 \implies M_\lambda = 1$ since $M_\lambda > 0$.
\begin{definition}
  The \textit{defect}, denoted $\Delta_\lambda$ of an eigenvalue $\lambda$ is defined as:
  \[
    \Delta_\lambda = M_\lambda - m_\lambda
  \]
\end{definition}
By \cref{multiplicityRelation}, we have $\Delta_\lambda \geq 0$.
\begin{example}
  \begin{enumerate}
    \item Consider the matrix:\[
      A = \begin{pmatrix}
      4 & 1 & 0 \\
      0 & 4 & 1 \\
      0 & 0 & 4 \\
      \end{pmatrix}
    \]
    \textbf{Characteristic Polynomial -} $\chi_A(t) = (4 - t)^3$.\par
    \textbf{Eigenvalues -} $A$ only has one eigenvalue $\lambda = 4$ with $M_\lambda = 3$.\par
    \textbf{Eigenvectors}
    \[
      (A - 4\lambda)\vec{v} = \begin{pmatrix}
      0 & 1 & 0 \\
      0 & 0 & 1 \\
      0 & 0 & 0 \\
      \end{pmatrix}
      \begin{pmatrix}
      x \\
      y \\
      z \\
      \end{pmatrix} = \vec{0} \implies
      y = z = 0
    \]
    So the associated eigenvector is $\vec{v} = \alpha \begin{pmatrix}
    1 \\
    0 \\
    0 \\
    \end{pmatrix}$, thus $m_\lambda = 1$ as the eigenspace is:
    \[
      E_\lambda = \Span\left\{\begin{pmatrix}
      1 \\
      0 \\
      0 \\
      \end{pmatrix}\right\} \implies \dim (E_\lambda) = 1
    \]
    Therefore the defect is $\Delta_{\lambda} = 3 - 1 = 2$.
  \item Consider a reflection in the plane through $\vec{0}$ with normal $\vec{n}$.
    We do not need to use the matrix representation of this to compute the eigenvalues.
    Instead, we can use geometric intuition.

    Since $\vec{n}$ is just sent to $-\vec{n}$, that is, $H\vec{n} = -\vec{n}$, we have an eigenvector $\vec{n}$ with eigenvalue $\lambda = -1$.
    So $E_{-1} = \Span\{\vec{n}\}$

    Moreover, any vector $\vec{u}$ on the plane is unchanged, that is, $H \vec{u} = \vec{u}$ for all $\vec{u} \perp \vec{n}$.
    So we also have eigenvalue $\lambda = 1$ with $E_1 = \{\vec{x} : \vec{x} \cdot \vec{n} = \vec{0}\}$.

    Since $m_{-1} = 1$ and $m_1 = 2$ and $m_1 + m_2 \leq 3$, we must have all eigenvalues.
    Furthermore, since $M_\lambda \geq m_\lambda$, we must have $M_1 = 2$ and $M_{1} = 1$ so the geometric and algebraic multiplicities coincide for each eigenvalue.
  \item Consider a rotation $\Rot(\theta) \in \R^2$,
    \[
      \Rot(\theta) = \begin{pmatrix}
      \cos \theta & -\sin \theta \\
      \sin \theta & \cos \theta \\
      \end{pmatrix}
    \]
    \textbf{Characteristic Polynomial -} $\chi_{\Rot(\theta)}(t) = t^2 - 2t \cos \theta + 1$.\par
    \textbf{Eigenvalues -} It does not have real eigenvalues but does have eigenvalues in $\C$, $\lambda = e^{\pm i\theta}$.
    Note that this is a conjugate pair, as expected.\par
    \textbf{Eigenvectors -} The associated eigenvectors are:
    \[
      \vec{v} = \alpha \begin{pmatrix}
      1 \\
      \mp i \\
      \end{pmatrix}
    \]
    Since these both have $m_\lambda = 1$, they must also both have $M_\lambda = 1$.
  \item Consider a rotation by and $\theta$ about $\vec{n}$, $R(\theta, \vec{n})$.
    Note that:
    \[
      R(\theta, \vec{n})\vec{n} = \vec{n}
    \]
    So we have an eigenvalue $\lambda = 1$ with eigenspace $E_\lambda = \Span\{\vec{n}\}$.

    If we consider a basis given by $\vec{n}$ and two orthogonal vectors lying in the plane, $\vec{n}$ is unchanged under $R$ whereas the two vectors are rotated in the plane, similarity to as in \textbf{iii}.

    From \textbf{iii}, we know the rotation in the plane has eigenvalues $e^{\pm i \theta}$.
    Note that there are no other real eigenvalues unless $\theta = n\pi$, in which case, all vectors on the plane are eigenvectors with eigenvalue $\lambda = -1$ and algebraic multiplicity $M_{-1} = 2$ as both $e^{+i\pi}$ and $e^{-i\pi}$ coincide.

    So we have found all three complex eigenvalues.
  \item Consider the matrix
    \[
      A = \begin{pmatrix}
      -3 & -1 & 1 \\
      -1 & -3 & 1 \\
      -2 & -2 & 0 \\
      \end{pmatrix}
    \]
    \textbf{Characteristic Polynomial -} $\chi_A(t) = -(t + 2)^3$.\par
    \textbf{Eigenvalues -} $A$ only has eigenvalue $\lambda = -2$ with $M_\lambda = 3$.\par
    \textbf{Eigenvectors -} For the eigenvectors:
    \[
      (A + 2I) \vec{v} =
      \begin{pmatrix}
      -1 & -1 & 1 \\
      -1 & -1 & 1 \\
      -2 & -2 & 2 \\
      \end{pmatrix}
      \begin{pmatrix}
      x \\
      y \\
      z \\
      \end{pmatrix} = \vec{0}
    \]
    By eliminating $z$, we see that the general solution for this is:
    \[
      \begin{pmatrix}
      x \\
      y \\
      x + y \\
      \end{pmatrix} =
      x \begin{pmatrix}
      1 \\
      0 \\
      1 \\
      \end{pmatrix} + y\begin{pmatrix}
      0 \\
      1 \\
      1 \\
      \end{pmatrix}
    \]
    Therefore:
    \[
      E_\lambda = \Span\left\{\begin{pmatrix}
      1 \\
      0 \\
      1 \\
      \end{pmatrix},
      \begin{pmatrix}
      0 \\
      1 \\
      1 \\
      \end{pmatrix}\right\} \implies m_\lambda = 2
    \]
    So the eigenvalues do not form a basis of $\C^{3}$.
    The defect is $\Delta_\lambda = 3 - 2 = 1$.
  \end{enumerate}
\end{example}
\section{Diagonalisation and Similarity}
\begin{proposition}[Diagonalisble]
  \label{diagonalProp}
  If $A$ is a $n \times n$ matrix acting on $V = \R^{n}$ or $\C^{n}$, then the following statements are equivalent:
  \begin{itemize}
    \item There exists a basis of eigenvectors for $V$, $\{\vec{v}_1, \ldots, \vec{v}_n\}$ with $A \vec{v}_i = \lambda_i \vec{v}_i$.

      \textit{(Summation convention not used)}
    \item There exits an $n \times n$ invertible matrix $P$ with:
      \[
        P^{-1} A P = D = \begin{pmatrix}
        \lambda_1 &  &  \\
         & \ddots &  \\
         &  & \lambda_n \\
        \end{pmatrix}
      \]
      where $D$ is a \textit{diagonal matrix}, that is, all non-diagonal entries are $0$.
  \end{itemize}
  If either of these conditions is satisfied, $A$ is \textit{diagonalisable}.
\end{proposition}
\subsection{Linearly Independent Eigenvectors}
\begin{theorem}
  \label{linearlyIndependantEigenvectors}
  If an $n \times n$ matrix $A$ has distinct eigenvalues $\lambda_1, \ldots, \lambda_r$ with corresponding eigenvectors $\vec{v}_1, \ldots, \vec{v}_r$ then, $\{\vec{v}_1, \ldots, \vec{v}_r\}$ are linearly independent.
\end{theorem}
\begin{proof}
  Suppose, for contradiction, that $\{\vec{v}_1, \ldots, \vec{v}_r\}$ are linearly dependant.
  Then:
  \[
    \sum_{j = 1}^{r} \alpha_j \vec{v}_j = \vec{0}
  \]
  where not all $\alpha_j = 0$.

  There will be at least one such linear combination but we can take the minimal $p$ for which $\exists \alpha_1, \ldots, \alpha_p \neq 0$ with:
  \[
    \sum_{j = 1}^{p} \alpha_j \vec{v}_j = \vec{0}
  \]
  We are able to assume WLOG that this minimal linear combination occurs with the first $p$ vectors as we can just relabel the vectors.

  If we then apply the matrix $(A - \lambda_1 I)$, then we remove the first term of the sum, that is:
  \[
    (A - \lambda_1 I) \sum_{j = 1}^{p} \alpha_j \vec{v}_j = \sum_{j = 2}^{p} \alpha_j(\lambda_j - \lambda_1)\vec{v}_j
  \]
  Since $(A - \lambda_1 I)\vec{v}_1 = \vec{0}$.

  The above is a linear combination with $p - 1$ non-zero coefficients which contradicts the fact that $p$ was minimal.
  Thus $\{\vec{v}_1, \ldots, \vec{v}_r\}$ is linearly independent.
\end{proof}
\begin{remark}[Note]
  The above theorem and proof only hold when each eigenvalue has geometric multiplicity one, that is, each eigenvalue has exactly one associated eigenvector.
  However this does generalise to the following:

  Let $B_\lambda$ be a basis for the eigenspace $E_\lambda$ associated with $\lambda$.
  If $\lambda_1, \ldots, \lambda_r$ are distinct, then $B_{\lambda_1} \cup \cdots \cup B_{\lambda_r}$ is a linearly independent set.

  See \cref{gramSchmidt} for a construction of this in the case of Hermitian matrices.
\end{remark}
We can now prove \cref{diagonalProp}
\begin{proof}[\Cref{diagonalProp}]
  For any matrix $P$:
  \begin{itemize}
    \item $AP$ has columns $A\vec{C}_i(P)$.
    \item $PD$ has columns $\lambda_i\vec{C}_i(P)$.
  \end{itemize}
  where $D$ is a diagonal matrix as in the proposition.
  So $AP = PD \iff A \vec{C}_i(P) = \lambda_i \vec{C}_i(P)\ \forall i = 1, \ldots, n$.

  \begin{proofdirection}{Suppose there exists a basis of eigenvectors for $V$}
    Given a basis of eigenvectors for $V$, the above relation allows us to determine such a $P$ as $\vec{C}_i(P)$ satisfies $A\vec{C}_i(P) = \lambda \vec{C}_i$ so we can take $\vec{C}_i(P) = \vec{v}_i$.

    Since we know what the eigenvectors form a basis for $V$, they are linearly independent and therefore the columns of $P$ are linearly independent so $\det P \neq 0$.
    Thus $P$ is invertible so we have constructed a $P$ such that $P^{-1} A P = D$.
  \end{proofdirection}
  \begin{proofdirection}{Suppose there exists such an invertible $P$}
    Since $P$ is invertible, $P^{-1} A P = D \iff AP = PD$.
    By the relation $A\vec{C}_i(P) = \lambda_i \vec{C}_i(P)$, we see that the columns of $P$ are eigenvectors.
    The columns of $P$ span $V$ so we have found a basis of eigenvectors for $V$.
  \end{proofdirection}
\end{proof}
\begin{remark}
  We see that the columns of $P$ are the eigenvectors of $A$.
  Since we need $P$ to be invertible, they must form a basis for $V$, which agrees with the equivalence of the two statements.
\end{remark}
\subsection{Criteria For Diagonalisability}
\subsubsection{A sufficient but not necessary condition}
\begin{proposition}
  If an $n \times n$ matrix has $n$ distinct eigenvalues, then it is diagonalisable.
\end{proposition}
\begin{proof}
If an $n \times n$ matrix has $n$ distinct eigenvalues, then by \cref{linearlyIndependantEigenvectors}, we have $n$ eigenvectors which are linearly independent.
This provides a basis of eigenvectors for $\R^{n}$ or $\C^{n}$.
So from \cref{diagonalProp}, $A$ is diagonalisable.
\end{proof}
Note that we require that all $n$ eigenvalues are distinct as otherwise we could have an eigenvalue with $m_\lambda < M_\lambda$ so would have less than $n$ linearly independent eigenvectors so would be unable to form a basis.
\subsubsection{A necessary and sufficient condition}
\begin{proposition}
  $A$ is diagonalisable if and only if for all eigenvalues $\lambda$, $M_\lambda = m_\lambda$
\end{proposition}
\begin{proof}
  \begin{proofdirection}{Suppose $A$ is diagonalisable}
    Then by \cref{diagonalProp}, there exists a basis of $n$ eigenvectors for $V$.

    Suppose we have $m_{\lambda_i} < M_{\lambda_i}$ for some $\lambda_i$.
    Then since $m_\lambda \leq M_\lambda$ for all eigenvalues, $\sum_{\lambda} m_\lambda < \sum_{\lambda} M_\lambda = n$.

    However, this implies that $\sum_{\lambda} m_\lambda < n$ which contradicts the fact the eigenvectors form a basis for $V$.
    So we must have $m_\lambda = M_\lambda$ for all eigenvalues.
  \end{proofdirection}
  \begin{proofdirection}{Suppose $M_\lambda = m_\lambda$ for all $\lambda$}
    If $\lambda_i$ with $i = 1, \ldots, r$ are all the distinct eigenvalues of a matrix, then $B_{\lambda_1} \cup \cdots \cup B_{\lambda_r}$ is a linearly independent set, and its number of elements is:
    \[
      \sum_{i} m_i = \sum_{i} M_i = n
    \]
    so it must form a basis.
  \end{proofdirection}
\end{proof}
\subsection{Similarity}
\begin{definition}
  We say that two $n \times n$ matrices $A$ and $B$ are \textit{similar} if there exists an $n \times n$ invertible matrix $P$ such that:
  \[
    B = P^{-1}AP
  \]
\end{definition}
\begin{remark}
  Similar matrices represent the same linear map with respect to different bases.
  See \cref{COBSpecialCases}.
\end{remark}
\begin{proposition}
  \label{similarProperties}
  If $A$ and $B$ are similar, then:
  \begin{enumerate}
    \item $\tr A= \tr B$
    \item $\det A = \det B$
    \item $\chi_A(t) = \chi_B(t)$
  \end{enumerate}
\end{proposition}
\begin{proof}
  \begin{enumerate}
    \item Using $\det AB = \det A \det B$ and $\det P^{-1} = (\det P)^{-1}$, we see that:
      \[
        \det B = \det (P^{-1} A P) = \det (P^{-1}) \det (A) \det (P) = \det A
      \]
    \item Using $\tr(AB) = \tr(BA)$, we see that:
      \[
        \tr B = \tr(P^{-1} A P) = \tr(APP^{-1}) = \tr A
      \]
    \item Again, using $\det AB = \det A \det B$ and $\det P^{-1} = (\det P)^{-1}$, we see that:
      \begin{align*}
        \chi_B(t) &= \det(B - tI) \\
                  &= \det(P^{-1} A P - tI) \\
                  &= \det(P^{-1})\det(AP - tP) \\
                  &= \det(P^{-1})\det(A - tI)\det(P) \\
                  &= \det(A - tI) \\
                  &= \chi_A(t)
      \end{align*}
      So $\chi_A(t) = \chi_B(t)$, as required.
  \end{enumerate}
\end{proof}
\begin{remark}
  For the particular case of $B = D$ where $D$ is a diagonal matrix, this means that $A$ is diagonalisable.
  Utilising the fact that the determinant of a diagonal matrix is the product of the diagonal entries, we then have:
  \begin{enumerate}
    \item $\tr A = \sum_{i} \lambda_i$
    \item $\det A = \prod_{i} \lambda_i$
    \item $\chi_A(t) = (\lambda_1 - t) \cdots(\lambda_n - t)$
  \end{enumerate}
  These results are consistent with \cref{eigenvalueTrDet}.
\end{remark}
We can use these relations between similar matrices to provide a proof for \cref{multiplicityRelation}.
\begin{proof}
  \nonexaminable
  \label{multiplicityProof}
  Consider a basis for $E_\lambda$, $\{\vec{v}_1, \ldots, \vec{v}_r\}$ with $r = \dim E_\lambda = m_\lambda$.
  Using \cref{addBasis}, we can extend this to a basis for $V = \R^{n}$ or $\C^{n}$ by adding the vectors $\{\vec{w}_{r + 1}, \ldots, \vec{w}_n\}$.

  Define the $n \times n$ matrix P with columns such that:
  \begin{align*}
    \vec{C}_i(P) &= \vec{v}_i \text{ for } i = 1, \ldots, r \\
    \vec{C}_j(P) &= \vec{w}_i \text{ for } j = r + 1, \ldots, n \\
  \end{align*}
  Then, since $\vec{v}_i \in E_\lambda$ for $i = 1, \ldots, r$ we have:
  \[
    A \vec{C}_i = A \vec{v}_i = \lambda \vec{v}_i
  \]
  and for $\vec{C}_j$:
  \[
    A \vec{C}_j = A \vec{w}_j = \sum_{i} B_{i j}\vec{v}_i + \sum_{k} B_{k j}\vec{w}_k
  \]
  where $B$ is an $n \times n$ matrix with entries:
  \[
    B_{i j} = \begin{cases}
    \lambda \delta_{i j} & i, j = 1, \ldots, r \\
    0 & i = r + 1, \ldots, n \text{ and }j = 1, \ldots, r  \\
    \text{unknown} & i = 1, \ldots, n \text{ and }j = r + 1, \ldots, n
    \end{cases}
  \]
  We can rewrite with as $AP = PB$ and then, as $P$ is invertible since its columns form a basis for $V$, we have:
  \[
    P^{-1}AP = B =
      \left(\begin{array}{c|c}
      \lambda I & ? \\ \hline
      O & \hat{B}
      \end{array}\right)
  \]
  Where $?$ and $\hat{B}$ denote the unknown elements.
  From \cref{blockDeterminants}:
  \[
    \det(B - tI) = \det(\lambda I - tI)\det(\hat{B} - tI) = (\lambda - t)^{r}\det(\hat{B} - tI)
  \]
  From \cref{similarProperties}$, \chi_A(t) = \chi_B(t)$.
  Therefore:
  \[
    \chi_A(t)= (\lambda - t)^{r}\det(\hat{B} - tI)
  \]
  So the multiplicity of the root $\lambda$ must be at least $r$.
  Thus $M_\lambda \geq r = m_\lambda$.
\end{proof}
\subsection{Hermitian and Symmetric Matrices}
\begin{remark}[Recap] See \cref{transposeAndConjugate,complexInnerProduct} for further details.\par
  \textbf{Hermitian -} $A$ is Hermitian if and only if:
  \[
    A^{\dag} = \overline{(A^{\trans})} = A \iff A_{i j} = \overline{A_{j i}}
  \]
  \textbf{Symmetric -} $A$ is symmetric if and only if:
  \[
    A^{\trans} = A \iff A_{i j} = A_{j i}
  \]
  \textbf{Complex Inner Product -} For $\vec{v}, \vec{w} \in \C^{n}$:
  \[
    \vec{v}^{\dag}\vec{w} = \sum_{i} \overline{v_i}w_i
  \]
  \textbf{Real Inner Product - } For $\vec{v}, \vec{w} \in \R^{n}$, this reduces to:
  \[
    \vec{v}^{\trans}\vec{w} = \sum_{i} v_i w_i
  \]
\end{remark}
\begin{remark}
  If $A$ is hermitian, then:
  \[
    (A \vec{v})^{\dag}\vec{w} = \vec{v}^{\dag}(A\vec{w})
  \]
\end{remark}
\begin{theorem}
  \label{hermitianProperties}
  If $A$ is a $n \times n$ Hermitian matrix, then:
  \begin{enumerate}
    \item Every eigenvalue of $A$ is real.
    \item Eigenvectors $\vec{v}, \vec{w}$ with distinct eigenvalues, $\lambda, \mu$ are orthogonal, that is, $\vec{v}^{\dag} \vec{w} = 0$
    \item If $A$ is symmetric, then for any eigenvalues $\lambda$, we can construct a real eigenvector $\vec{v}$ so that \textbf{ii} reduces to  $\vec{v}^{\trans} \vec{w} = 0$
  \end{enumerate}
\end{theorem}
\begin{proof}
  \begin{enumerate}
    \item Consider an eigenvector $\vec{v}$ with eigenvalue $\lambda$, then:
    \begin{align*}
      \vec{v}^{\dag}(A \vec{v}) &= (A \vec{v})^{\dag}\vec{v} \\
      \vec{v}^{\dag}(\lambda \vec{v}) &=(\lambda \vec{v})^{\dag} \vec{v} \\
      \lambda \vec{v}^{\dag}\vec{v} &= \overline{\lambda}\vec{v}^{\dag}\vec{v}
    \end{align*}
    Since $\vec{v} \neq \vec{0}$, $\lambda = \overline{\lambda} \implies \lambda \in \R$.
  \item Consider eigenvectors $\vec{v}, \vec{w}$ with eigenvalues $\lambda, \mu$ respectively:
    \begin{align*}
      \vec{v}^{\dag}(A \vec{w}) &= (A \vec{v})^{\dag} \vec{w} \\
      \mu \vec{v}^{\dag}\vec{w} &= \overline{\lambda} \vec{v}^{\dag} \vec{w} \\
      \mu \vec{v}^{\dag}\vec{w} &= \lambda \vec{v}^{\dag} \vec{w}
    \end{align*}
    Since $\lambda \neq \mu$ we must have $\vec{v}^{\dag}\vec{w} = 0$, so $\vec{v}$ and $\vec{w}$ are orthogonal.
  \item Since $A$ is Hermitian and symmetric, $A$ must be real.
    From \textbf{i}, all eigenvalues must be real also.

    Consider an eigenvector $\vec{v} \in \C^{n}$ with eigenvalue $\lambda \in \R$.
    Let $\vec{v} = \vec{u} + i \vec{u}'$ with $\vec{u}, \vec{u}' \in \R^{n}$.
    Then:
    \begin{align*}
      A \vec{v} = \lambda \vec{v} \\
      A \vec{u} + i(A \vec{u}') = \lambda \vec{u} + i(\lambda \vec{u})
    \end{align*}
    Since $\lambda \in \R$, we must have:
    \[
      A \vec{u} = \lambda \vec{u} \text{ and } A \vec{u}' = \lambda \vec{u}'
    \]
    Furthermore, since $\vec{v} \neq \vec{0}$, at least one of $\vec{u}, \vec{u}' \neq \vec{0}$ so there is at least one real eigenvector.
  \end{enumerate}
\end{proof}
\subsection{Gram-Schmidt Orthogonalisation}
\label{gramSchmidt}
Given a linearly independent set of vectors in $\C^{n}$ $\{\vec{w}_1, \ldots, \vec{w}_r\}$.
Using the \textit{Gram-Schmidt Process}, we can construct a sequence of sets of the form:
\begin{center}
\begin{tabular}{c}
$\{\vec{w}_1, \vec{w}_2, \vec{w}_3, \ldots, \vec{w}_r\} $ \\
$\{\vec{u}_1, \vec{w}_2', \vec{w}_3', \ldots, \vec{w}_r'\} $ \\
$\{\vec{u}_1, \vec{u}_2, \vec{w}_3'', \ldots, \vec{w}_r''\} $ \\
$\vdots$ \\
$\{\vec{u}_1, \vec{u}_2, \vec{u}_3,\ldots, \vec{u}_r\} $ \\
\end{tabular}
\end{center}
where the final set, $\{\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_n\}$, is an orthonormal set.

At each step of the process:
\begin{itemize}
  \item The span of the set is unmodified.
  \item The set is linearly independent.
  \item All of the $\vec{u}_i$ are orthonormal to each other.
  \item All of the $\vec{u}_i$ are orthogonal all of the $\vec{w}_i$.
\end{itemize}
\subsubsection{Construction}
\begin{itemize}
  \item \textbf{Step 1 -} Define $\vec{u}_1$ and $\vec{w}_j'$ for $j = 2, \ldots, r$ as:
    \[
      \vec{u}_1 = \frac{\vec{w}_1}{|\vec{w}_1|},\ \vec{w}_j' = \vec{w}_j  - (\vec{u}^{\dag}_{1}\vec{w}_j)\vec{u}_1
    \]
    This guarantees that $|\vec{u}_1| = 1$. We also see that:
    \begin{align*}
      \vec{u}^{\dag}_{1}\vec{w}_j' &= \vec{u}^{\dag}_{1}(\vec{w}_j - (\vec{u}^{\dag}_{1}\vec{w}_j)\vec{u}_1) \\
                                   &= \vec{u}^{\dag}_{1}\vec{w}_j - (\vec{u}^{\dag}_{1}\vec{w}_j)(\underbrace{\vec{u}^{\dag}_{1}\vec{u}_1}_{|\vec{u}_1| = 1}) \\
                                   &= \vec{u}^{\dag}_{1}\vec{w}_j -\vec{u}^{\dag}_{1}\vec{w}_j = 0
    \end{align*}
    So $\vec{u}^{\dag}_{1}\vec{w}_j' = 0$ for $j = 2, \ldots, r$.
  \item \textbf{Step 2 -} Define $\vec{u}_2$ and $\vec{w}_j''$ for $j = 3, \ldots, r$ as:
    \[
      \vec{u}_2 = \frac{\vec{w}_2'}{|\vec{w}_2|},\ \vec{w}_j'' = \vec{w}_j' - (\vec{u}^{\dag}_{2}\vec{w}_j')\vec{u}_2
    \]
    This guarantees that:
    \begin{itemize}
      \item $|\vec{u}_2| = 1$
      \item $\vec{u}_1$ and $\vec{u}_2$ are orthogonal since:
        \[
          \vec{u}^{\dag}_{1}\vec{u}_2 = \frac{\vec{u}^{\dag}_{1}\vec{w}_2'}{|\vec{w}_2|} = 0
        \]
        where $\vec{u}^{\dag}_{1}\vec{w}_2' = 0$ by the construction from the previous step.
      \item $\vec{u}_1$ is orthogonal to $\vec{w}_j''$ for $j = 3, \ldots, r$ since:
        \[
          \vec{u}^{\dag}_{1}\vec{w}_j'' = \underbrace{\vec{u}^{\dag}_{1}\vec{w}_j}_{=0} -(\vec{u}^{\dag}_{2}\vec{w}_j')(\underbrace{\vec{u}^{\dag}_{1}\vec{u}_2}_{=0}) =0
        \]
      \item $\vec{u}_2$ is orthogonal to $\vec{w}_j''$ for $j = 3, \ldots, r$ because $\vec{u}^{\dag}_{2}\vec{w}_j'' = 0$ by the same argument as in Step 1.
    \end{itemize}
  \item \textbf{Step ...}\par
    Repeatedly apply the same construction for $r$ steps total.
\end{itemize}
Since the construction at each step preserves the properties listed earlier, we will end up with an orthonormal set of vectors.

If we have a Hermitian matrix $A$, we can use this process to obtain an orthonormal basis $B_\lambda$ for each eigenspace $E_\lambda$.
Since we established that all the eigenvectors associated with different eigenvalues of a hermitian matrix are orthogonal, if $\lambda_1, \ldots, \lambda_r$ are distinct eigenvalues of $A$, then $B_{\lambda_1} \cup \cdots \cup B_{\lambda_r}$ is an orthonormal set.
\subsection{Unitary and Orthogonal Diagonalisation}
\begin{theorem}
  If $A$ is a $n \times n$ hermitian matrix, then $A$ is diagonalisable.
\end{theorem}
More specifically, if $A$ is hermitian, then all of the following equivalent statements hold:
\begin{enumerate}
  \item There exists a basis for $\C^{n}$ of eigenvectors $\vec{u}_1, \ldots, \vec{u}_n \in \C^{n}$ with $A \vec{u}_i = \lambda_i \vec{u}_i$.
  \item There exists a $n \times n$ invertible matrix $P$ with $P^{-1}AP = D$ where:
    \begin{itemize}
      \item $D$ is a diagonal matrix with $\lambda_1, \ldots, \lambda_n$ on the diagonal
      \item $C_i(P) = \vec{u}_i$, i.e., the columns of $P$ are the eigenvectors.
    \end{itemize}
\end{enumerate}
\begin{remark}
  In addition, the eigenvectors, $\vec{u}_i$ can be chosen to be orthonormal, that is:
  \[
    \vec{u}^{\dag}_{i}\vec{u}_j = \delta_{i j}
  \]
  Equivalently, the matrix $P$ can be chosen to be unitary, so that $P^{-1} = P^{\dag}$ and thus $P^{\dag} A P = D$.

  When working with real matrices, for a $n \times n$ real \textbf{symmetric} matrix $A$ ($A^{\trans} = A$), the eigenvectors can be taken to be $\vec{u}_1, \ldots, \vec{u}_n \in \R^{n}$ (see \cref{hermitianProperties}) and can be chosen so that:
  \[
    \vec{u}^{\trans}_{i} \vec{u}_j = \vec{u}_i \cdot \vec{u}_j= \delta_{i j}
  \]
  Equivalently, $P$ can be chosen to be orthogonal, so that $P^{-1} = P^{\trans}$ and thus $P^{\trans} A P = D$.
\end{remark}
\section{Change of Basis}
Consider the real or complex vector spaces $V$ and $W$ with $\dim V = n$ and $\dim W = m$.
Let $T: V \to W$ be a linear map and let $\{\vec{e}_1, \ldots, \vec{e}_n\}$ be a basis of $V$ and $\{\vec{f}_1, \ldots, \vec{f}_n\}$ be a basis of $W$ such that $T$ is represented by an $m \times n$ matrix $A$ with respect to these bases.

From \cref{matrixDef}, this means that:
\[
  T(\vec{e}_i) = \sum_{j} \vec{f}_j A_{j i}
\]
Now let $\{\vec{e}_1', \ldots, \vec{e}_n'\}$ be a new basis of $V$ and $\{\vec{f}_1', \ldots, \vec{f}_n'\}$ be a new basis of $W$.
$T$ is represented with respect to these new basis by a $m \times n$ matrix $B$.
Namely:
\[
  T(\vec{e}_i') = \sum_{j} \vec{f}_j' B_{j i}
\]
Suppose that the bases are related by:
\[
  \vec{e}_i' = \sum_{k} \vec{e}_k P_{k i} \text{ and } \vec{f}_j' = \sum_{l} \vec{f}_l Q_{l j}
\]
where $P$ is a $n \times n$ matrix and $Q$ is a $m \times m$ matrix, both of which are invertible.
\begin{proposition}[Change of Basis Formula]
  If $A, B, P, Q$ are defined as above, then:
  \[
    B = Q^{-1} A P
  \]
  This defines the \textit{change of basis formula} for matrices of linear maps.
  $P$ and $Q$ are called the \textit{change of basis matrices}.
\end{proposition}
\begin{proof}
  Consider the image of an element from the new basis for $V$, $\vec{e}_i'$, under $T$.

  We first use the definition of $P$ to yield:
  \begin{align*}
    T(\vec{e}_i') &= T\left(\sum_{k} \vec{e}_k P_{k i} \right) \text{ (by definition of $P$)} \\
                  &= \sum_{k} T(\vec{e}_k)P_{k i} \text{ (by linearity)} \\
                  &= \sum_{k, j} \vec{f}_j A_{j k} P_{k i}  \text{ (by definition of $A$)}
  \end{align*}
  Secondly, we use the definition of $B$:
  \begin{align*}
    T(\vec{e}_i') &= \sum_{j} \vec{f}_j' B_{j i} \text{ (by definition of $B$)}\\
                  &= \sum_{j, l} \vec{f}_l Q_{l j}B_{j i} \text{ (by definition of $Q$)}\\
                  &= \sum_{k, j} \vec{f}_j Q_{j k}B_{k i} \text{ (Relabel $l \mapsto j$, $j \mapsto k$)}
  \end{align*}
  We can then compare the coefficients of $\vec{f}_j$ in each expression for $T(\vec{e}_i)$:
  \begin{align*}
    \sum_{k} A_{j k}P_{k i} &= \sum_{k} Q_{j k}B_{k i} \\
    (AP)_{j i} &= (QB)_{j i} \\
    \implies AP &= QB
  \end{align*}
  Since $P$ and $Q$ are invertible, the result follows.
\end{proof}
\begin{remark}
  \begin{enumerate}
    \item From the definition of $A$ which represents $T$ with respect to the original bases $\{\vec{e}_i\}$ and $\{\vec{f}_j\}$, we see that  that column $i$ of $A$ consists of the components of $T(\vec{e}_i)$ with respect to $\{\vec{f}_j\}$.

      Similarly, for the matrix $B$ which represents $T$ with respect to the new bases $\{\vec{e}_i'\}$ and $\{\vec{f}_j'\}$,  column $i$ of $B$ consists of the components of $T(\vec{e}_i')$ with respect to $\{\vec{f}_j'\}$.
    \item Column $i$ of $P$ consists of the components of $\vec{e}_i'$ with respect to the original basis $\{\vec{e}_i\}$.

      Similarly, column $i$ of $Q$ consists of the components of $\vec{f}_i'$ with respect to the original basis $\{\vec{f}_j\}$
    \item If instead of changing in the direction:
      \begin{align*}
        \{\vec{e}_i\} &\to \{\vec{e}_i'\} \\
        \{\vec{f}_j\} &\to \{\vec{f}_j'\}
      \end{align*}
      we instead change in the converse direction, then $P' = P^{-1}$ and $Q' = Q^{-1}$.
      The original bases can then be obtained with:
      \[
        \vec{e}_i = \sum_{k} \vec{e}_k' P_{k i}' \text{ and } \vec{f}_j = \sum_{l} \vec{f}_l' Q_{l j}'
      \]
  \end{enumerate}
\end{remark}
\begin{example}
  Consider $\dim V = n = 2$ and $\dim W = m = 3$.
  Suppose the image of the original basis is given by:
  \begin{align*}
    T(\vec{e}_1) &= \vec{f}_1  + 2\vec{f}_2 - \vec{f}_3 \\
    T(\vec{e}_2) &= -\vec{f}_1  + 2\vec{f}_2 + \vec{f}_3
  \end{align*}
  Then, from remark \textbf{i} above, $A$ is represented by:
  \[
    A = \begin{pmatrix}
    1 & -1 \\
    2 & 2 \\
    -1 & 1 \\
    \end{pmatrix}
  \]

  Now consider a new basis for $V$, $\{\vec{e}_1', \vec{e}_2'\}$ that is related to $\{\vec{e}_1, \vec{e}_2\}$ by:
  \[
    \vec{e}_1' = \vec{e}_1 - \vec{e}_2 \text{ and } \vec{e}_2' = \vec{e}_1 + \vec{e}_2
  \]
  so, by remark \textbf{ii} above, $P$ is:
  \[
    P = \begin{pmatrix}
    1 & 1 \\
    -1 & 1 \\
    \end{pmatrix}
  \]
  Now consider a new basis for $W$, $\{\vec{f}_1', \vec{f}_2', \vec{f}_3'\}$ that is related to $\{\vec{f}_1, \vec{f}_2, \vec{f}_3\}$ by:
  \begin{align*}
    \vec{f}_1' &= \vec{f}_1 - \vec{f}_3 \\
    \vec{f}_2' &= \vec{f}_2 \\
    \vec{f}_3' &= \vec{f}_1 + \vec{f}_3
  \end{align*}
  So, again by remark \textbf{ii}, $Q$ is:
  \[
    Q = \begin{pmatrix}
    1 & 0 & 1 \\
    0 & 1 & 0 \\
    -1 & 0 & 1 \\
    \end{pmatrix}
  \]
  From the change of basis formula, we then have:
  \[
    B = Q^{-1}AP = \begin{pmatrix}
    2 & 0 \\
    0 & 4 \\
    0 & 0 \\
    \end{pmatrix}
  \]
  Thus, by remark \textbf{i}, the images of the new basis vectors are:
  \[
    T(\vec{e}_1') = 2\vec{f}_1',\ T(\vec{e}_2') = 4\vec{f}_2'
  \]
  We can check our answer by ensuring that the images of the original basis match:
  \begin{align*}
    T(\vec{e}_1') &= T(\vec{e}_1) - T(\vec{e}_2) \\
                  &= \vec{f}_1 + 2\vec{f}_2 - \vec{f}_3 - (-\vec{f}_1 + 2\vec{f}_2 + \vec{f}_3)) \\
                  &= 2\vec{f}_1 -2 \vec{f}_3 \\
                  &= 2\vec{f}_1'
  \end{align*}
  and:
  \begin{align*}
    T(\vec{e}_2') &= T(\vec{e}_1) + T(\vec{e}_2) \\
                  &= \vec{f}_1 + 2\vec{f}_2 - \vec{f}_3 + (-\vec{f}_1 + 2\vec{f}_2 + \vec{f}_3)) \\
                  &= 4\vec{f}_2 \\
                  &= 4\vec{f}_2'
  \end{align*}
  which is consistent \tick
\end{example}
\begin{remark}[Special Cases]
  \label{COBSpecialCases}
  \begin{enumerate}
    \item If $V = W$ and both have the same bases, that is, $\vec{e}_i = \vec{f}_i$ and $\vec{e}_i' = \vec{f}_i'$, then $P = Q$ so the change of basis formula becomes:
      \[
        B = P^{-1} A P
      \]
      Therefore, matrices represent the same linear map $T: V \to V$ if and only if they are \textbf{similar}.
    \item Consider the case $V = W = \R^{n} \text{ or }\C^{n}$ both with the standard basis $\{\vec{e}_i\}$.

      If there exists a basis of eigenvectors of $T$, $\{\vec{v}_1, \ldots, \vec{v}_n\}$, then we can construct this to be the new basis $\{\vec{e}_i' = \vec{v}_i\}$ and define $B$ to be the matrix of $T$ with respect to this new basis.

      It then follows that:
      \[
        B = D
      \]
      where $D$ is a diagonal matrix with $\lambda_1, \ldots, \lambda_n$ on the diagonal where $T(\vec{v}_i) = \lambda_i \vec{v}_i$ and thus $D = P^{-1} A P$.
      By definition:
      \[
        \vec{v}_i  = \sum_{j} \vec{e}_j P_{j i}
      \]
      That is, the eigenvectors are the columns of $P$.
      Hence $P$ is both the change of basis matrix and also the matrix that diagonalises $A$.
  \end{enumerate}
\end{remark}
\end{document}
